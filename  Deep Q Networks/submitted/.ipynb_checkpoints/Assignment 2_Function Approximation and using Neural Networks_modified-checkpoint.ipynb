{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Function Approximation for Q Learning\n",
    "\n",
    "Name: Chuqiao Song\n",
    "\n",
    "ID: A53239614"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cartpole\n",
    "\n",
    "A cartpole problem is shown below.\n",
    "![pendulum2.png](pendulum2.png)\n",
    "\n",
    "The equation for the cartpole problem is nonlinear in nature, but it has been shown through robust control theory that a linear version of the equation of the form $\\dot{x} = Ax+Bu$ can be solved by a linear controller. Let us assume that we are interested in minimizing cart stray from the center, and pendulum falling. It turns out that typical techniques - open loop control, PID control, root locus, etc. is not suitable for stabilizing both the cart position (keep near center) or the pole angle (keep vertical). The solution to this question is a linear quadratic controller, but we won't be using the solution at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment for Function Approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create the CartPole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrate your understanding of the simulation\n",
    "For OpenAI's CartPole-v0 environment,\n",
    "- describe the reward system\n",
    "- describe the each state variable (observation space)\n",
    "- describe the action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Describe the reward system\n",
    "- Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "Describe the each state variable (observation space):\n",
    "- The observation space are four dimensions space.\n",
    "- obersvation[0] is the cart position \n",
    "- obersvation[1] is the cart velocity \n",
    "- obersvation[2] is the pole angle\n",
    "- obersvation[3] is the pole velocity at tip\n",
    "\n",
    "Describe the action space:\n",
    "- There are two actions which are 0 and 1. The 0 means push cart to left, and 1 means push cart to right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Deep Neural Network class that creates a dense network of a desired architecture\n",
    "In this problem we will create neural network that is our function that takes states to q-values: $q=f(x)$. While any function approximator could be used (i.e. Chebyshev functions, taylor series polynomials), neural networks offer a most general form of 1st-order smooth function (though comprising of trivial small activation functions means that complex functions require a significant amount of weights to identify). \n",
    "\n",
    "Create a class for a QNetwork that uses PyTorch to create a fully connected sequential neural network, of the following properties:\n",
    "- solver: Adam\n",
    "\n",
    "- input and hidden layer activation function: tanh\n",
    "\n",
    "- output activation function: linear\n",
    "\n",
    "- loss: MSE\n",
    "\n",
    "- learning_rate: 0.01\n",
    "\n",
    "- decay_rate:  5/num_episodes (epsilon)\n",
    "\n",
    "- hidden_state sizes: 64 (one layer)\n",
    "\n",
    "- state and action sizes: 4 states, 2 actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "# Define your network here\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.tanh(x)\n",
    "        Qs_actions = self.out(x) # Q value for one state, at different actions\n",
    "        return Qs_actions\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate, state_size, action_size, hidden_size, alpha_decay):\n",
    "        self.LR = learning_rate\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.model = Net(self.state_size, self.action_size, self.hidden_size)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.LR)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def learn(self, batch_Q_behavior, batch_Q_target):\n",
    "        loss = self.criterion(batch_Q_behavior, batch_Q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Replay class that includes all the functionality of a replay buffer\n",
    "The replay buffer should kept to some maximum size (10000), allow adding of samples and returning of samples at random from the buffer. Each sample (or experience) is formed as (state, action, reward, next_state, done). The replay buffer should also be able to generate a minibatch. The generate_minibatch method should take in DQN, targetDQN, selected batch_size, and return the states present in the minibatch and the target Q values for those states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay():\n",
    "    def __init__(self, max_size):\n",
    "        self.capacity = max_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def initialize(self, init_length, envir):\n",
    "        st = env.reset()\n",
    "        for _ in range(init_length):\n",
    "            a = random.randint(0, 1)\n",
    "            st1, r, done, info = env.step(a)\n",
    "            self.push((st, a, st1, r, done))\n",
    "            if done: st = env.reset()\n",
    "            else : st = st1\n",
    "            \n",
    "    def push(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def generate_minibatch(self, DQN, targetDQN, batch_size):\n",
    "        batch_memory = random.sample(self.memory, batch_size) #return a list\n",
    "        batch_memory = list(zip(*batch_memory))\n",
    "        \n",
    "        batch_st = Variable(FloatTensor(batch_memory[0]))\n",
    "        batch_a = Variable(torch.unsqueeze(LongTensor(batch_memory[1]),1))\n",
    "        batch_st1 = Variable(FloatTensor(batch_memory[2]))\n",
    "        batch_r = Variable(torch.unsqueeze(FloatTensor(batch_memory[3]),1))\n",
    "        \n",
    "        batch_done = FloatTensor(batch_memory[4])  ## Tensor (128,)\n",
    "        batch_Q_behavior = DQN.model(batch_st).gather(1, batch_a)\n",
    "        mask = 1 - batch_done   # if done is true, change the target to just reward\n",
    "        batch_Q_next = targetDQN.model(batch_st1).detach() #(128,1), when doing max it is (128,)\n",
    "        QQ_next = Variable((batch_Q_next.max(1)[0].data * mask).view(batch_size, 1))\n",
    "        batch_Q_target = batch_r + self.gamma*(QQ_next)\n",
    "        return batch_Q_behavior, batch_Q_target\n",
    "         \n",
    "    def __len__(self):            \n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that creates a minibatch from a buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Function Approximation\n",
    "Initialize DQN networks and Replay objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run gpu !\n"
     ]
    }
   ],
   "source": [
    "# Initialize DQN\n",
    "# Play around with your learning rate, alpha decay and hidden layer units \n",
    "# Two layers with a small number of units should be enough\n",
    "learning_rate = 0.01 \n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "hidden_size = 64\n",
    "alpha_decay = 0.1\n",
    "batch_size = 500\n",
    "\n",
    "DQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "targetDQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "# set targetDQN weights to DQN weights\n",
    "# for ex. targetDQN.model.weights = DQN.model.weights (syntax given here is for representation purpose only)\n",
    "targetDQN.model.load_state_dict(DQN.model.state_dict())\n",
    "replay = Replay(max_size=10000) ## Initialize Replay Buffer\n",
    "replay.initialize(init_length=1000, envir=env) ## Populate the initial experience buffer\n",
    "\n",
    "if use_cuda:\n",
    "    print('run gpu !')\n",
    "    targetDQN.model.cuda()\n",
    "    DQN.model.cuda()\n",
    "else: \n",
    "    print('gpu not activited !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function that solves the above environment using a deep Q network that uses a minibatch strategy.\n",
    "Use the following parameters (these had to be derived empirically - there is generally no trusted way of choosing the right parameter values - i.e. gamma, number of episodes, decay rate, min_epsilon). \n",
    "\n",
    "Generate a graph of the average return per episode every 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training\n"
     ]
    }
   ],
   "source": [
    "# Runtime parameters\n",
    "num_episodes = 2000            # max number of episodes to learn from\n",
    "gamma = 0.99                   # future reward discount\n",
    "max_steps = 500                # cut off simulation after this many steps\n",
    "# Exploration parameters\n",
    "min_epsilon = 0.01             # minimum exploration probability\n",
    "decay_rate = 5/num_episodes    # exponential decay rate for exploration prob\n",
    "returns = np.zeros(num_episodes)\n",
    "\n",
    "for ep in range(1, num_episodes):\n",
    "    print(ep, returns[ep-1], end = '\\r')\n",
    "    epsilon = min_epsilon + (1.0 - min_epsilon)*np.exp(-decay_rate*ep)\n",
    "    # --> start episode\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # generate the steps in each episode \n",
    "        # explore/exploit and get action using DQN\n",
    "        if random.random()<= epsilon:\n",
    "            action = random.randint(0,1)\n",
    "        else:\n",
    "            var_state = Variable(torch.unsqueeze(FloatTensor(state),0)) # here change the (4,) to (1,4) in variable \n",
    "            Qs_actions = DQN.model.forward(var_state) # shape of (1, 2) variable\n",
    "            cuda_tensor_action = torch.max(Qs_actions,1)[1].data\n",
    "            action = int(cuda_tensor_action.cpu().numpy())\n",
    "           \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        replay.push((state, action, new_state, reward, done))\n",
    "        \n",
    "    # perform action and record new_state, action, reward\n",
    "    # populate Replay experience buffer \n",
    "        if done:  break\n",
    "        else: state = new_state \n",
    "    # <-- end episode\n",
    "    returns[ep] = total_reward\n",
    "    \n",
    "    batch_Q_behavior, batch_Q_target = replay.generate_minibatch(DQN, targetDQN, batch_size) #outputs and targets\n",
    "    DQN.learn(batch_Q_behavior, batch_Q_target) \n",
    "    targetDQN.model.load_state_dict(DQN.model.state_dict())\n",
    "print('finished training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAIABJREFUeJzt3XmYHGW59/HvLxv7kkCIyJaAAUWFABEIAQxEZXmVTUU4iMjhiPiiEgEVxBcR3I6IHhfEE5RDkFVEhKOoBAhkgAmYnZ0EiCYhJMEoIAlZ7/ePp8bphJ6ZmqWmpnt+n+uqq6ueXurunqTvfurZFBGYmZltqE/ZAZiZWc/kBGFmZlU5QZiZWVVOEGZmVpUThJmZVeUEYWZmVTlBmJlZVU4QZmZWlROEmZlV1a/sADpj2223jaFDh5YdhplZTZk2bdrLETG4rcfVdIIYOnQoU6dOLTsMM7OaIukveR7nS0xmZlaVE4SZmVVVWIKQtJOkSZKelPSEpHOy8kGSJkqak90OzMol6UeS5kqaLWnfomIzM7O2FVmDWAOcFxF7AgcCZ0vaE7gAuDcihgP3ZscARwHDs+1M4KoCYzMzszYUliAiYlFETM/2XwOeAnYAjgUmZA+bAByX7R8LXBfJFGBrSdsXFZ+ZmbWuW9ogJA0F9gEeAYZExKLsrpeAIdn+DsD8iqctyMrMzKwEhScISZsDtwHjIuLVyvsiLWfXriXtJJ0paaqkqUuXLu3CSM16kcZG+Pa3021v1Nn3X/bzu0mh4yAk9Sclhxsi4jdZ8WJJ20fEouwS0pKsfCGwU8XTd8zK1hMR44HxACNHjvR6qWZ5vf46zJ0Lv/sdfP3rsGYN9OsHn/scvPOdsMkmrW8bb9x8O2UK3H8/jBkDo0a1P5bGxurPj4C1a2HVKli9Om1N+5W306bB1Kmw117w9ren95J3mzsXrrqq+f2fcQbstFPz+Zu2lo7nz4df/jLF2bcvnHoq7Lhj/ve+YEHz8wcMgBtvhOOOgz49r1OpilqTWpJIbQzLImJcRfnlwN8i4juSLgAGRcSXJP0f4LPA0cABwI8iYv/WzjFy5MjwQDmzCqtXwwsvwLPPNm9z5qTbBQuKOeemm6YvOql569On5eNVq2Dp0uYv3S22gHXrmr/8a5GU/7HVvnM32QSGD4c99kjb7rs372+1VdfFmZE0LSJGtvW4ImsQo4FTgcckzczKvgJ8B/iVpDOAvwAnZvfdRUoOc4HlwOkFxmZWrpZ+QUP6sly79s2/fCvLHnkE7r4bBg1Kx02J4Pnn0+OaDByYvmwOOyzd7r47rFgBn/lM+jIeMABuvTXVIFasSNsbbzTvV9smTkyxR6QvxhEjYL/91v+1vW7d+seVZbNmwZLswoEE73pX+gwGDID+/dPWtL/hbf/+8L//CzfdlF6vT59UAzjttFQbyLNNnw7HHtv8/u+6K52/6Uu+MrFVO25shLFjm59/773tq0VVPr9fPzjnnPQ3fOYZmDEDfvOb9f+GQ4Y0J4um5LF8efp7jx3bsRpcToXVILqDaxBWU15+Ge67D264IX3JNf3f23TTtN/05d/e/5MDBsA73tGcAIYPb97fZpvqz2ktQbWlK78gy3h+02sUcYmsK56/alVK9M888+bt5ZfXf+wmm3To/eetQThBmBVl+XJ48EG45560zZiRyjfaCFauTPsSHHRQ+g9e7ddu375vLvvjH+H229Mv6L594dJL4Stf6d731pO/YOvZsmVw0UXw3/+dfkj07QuXXQYXXtiul3GCMOtua9akxtN77km/6h56KP0a7N8fRo+G970vbatWwRFHlPsL2mpXF/z9e0IbhFn9amyESZNg113hb39LSWHSJHjllXT/iBHw+c+nhHDwwbDZZus//957O/4LeNSozj3fals3/v1dgzBrr/vuSzWANWuay3bZBd7//pQQDj8cBrc51b5ZaVyDMCvC4sXwiU80J4c+feALX4DLL29fV0ezGtDzRmaY9VTPPJOq8y+/nK799u2bGpw//GEnB6tLrkGY5fHgg3DMManBuaEh1SDcBmB1zgnCrC2/+lW6rDR0KPzhDzBsWCp3YrA650tM1jF/+hN861s9frKxTomA730PPvYxeM97UrfVpuRg1gu4BmHtM2sWjBuXLq9AGrh19dVpqoN6ug6/dm2aAuHKK+HEE2HChDRJnVkv4hqE5TN9Ohx/fOrf39jYnAzWrIHTT09TPVx6aZofptYtXw4nnJCSw/nnp3l/nBysF3KCsNZNnZoaZ/fbL9UaLrkE7rgjfWH27Ztuv/QleMtb0n27754ux3z/+7DwTbO193xLlqSJ7X73O/jJT1L31R44DbNZd/AlJqvukUdSjeCuu9KMoJddltYNaJp6uNpIzgUL4JZb0i/u885Lv77f+144+eTUFbSlieN6imeegaOPhkWL0oyaxx5bdkRmpfJIalvfww+nxPCnP6Uv9PPOg7PPhi23bN/rPPtsShQ33pj2+/WDI49MyeKYY+Cxx3pWN9GHHkpx9e2bZlo94ICyIzIrjCfrs/ZpaEiJ4Z57YNtt4YtfTGsGbLFF5143Is1ietNNaVu4MA0ua5rWeqONyp9s7te/ho9/HHbeOXVj3W238mIx6wZ5E4QvrvZ299+f5g469FCYPTt165w3L7UrdDY5QGrM3nffdC3/r3+FBx6AvfdOvYTWrUvTXk+a1PnzdEREais58cTUxtLY6ORgVsEJore68srUp/+ww+Cpp+AHP0hLVZ533ptnHu0qffqkRPRf/9XcK2jdurQy2rJlxZyzJQ8+mKbgPu+81D5yzz09v43ErJu5kbo3Ou+89MsZ0tQRN92U2gK6y6hRaUbUSZPgxRdh/Pi0+Pz113dPHL/9bUoK69altpFx49LKXGa2Htcgepvx45uTA6QvyTJGQ48alVZB+8lP0vk33TRd6rroorR4fRGWLUttKx/5SHrfkC4zTZ5czPnMapwTRG/yP/8Dn/50+nLeZJPUY2fAgO6tPVSz335pIN7pp6fpOw45JK3J21WWL4fvfCct7nPFFfCBDzSP4+gJ79+shyosQUi6RtISSY9XlN0iaWa2zZM0MysfKmlFxX0/KyquXuv66+GMM9KX4333pZ5Dl11Wfg+iJptvDr/4RRpH8fTTacT29dd37jXXrEnTgAwfntbsPfjgNFXIXXelz6AnvX+znigiCtmAQ4F9gcdbuP8K4OJsf2hLj2tt22+//cJyuPnmiD59Ig4/PGL58rKjadu8eRGjR0dAxMc/HvHKK+17/rp1EbfdFrHHHuk1Ro2KmDy5mFjNahAwNXJ8xxZWg4iIyUDVrimSBJwI3FTU+S1z221wyimpx86dd9ZGY+wuu6Tut1//ehpoN2IETJmS77kPPJBqBB/+cOo1dfvtaRDcIYcUGrJZPSqrDeIQYHFEVM7sNkzSDEkPSGrxf7OkMyVNlTR16dKlxUday+68E046KY0K/v3vi+u+WoR+/eDii1MD8rp16fLQN7+Zxk9UM2tWmiZjzJg05cfPf57GdRx3XH3NMmvWjXInCEmbduF5T2b92sMiYOeI2Ac4F7hRUtW5HSJifESMjIiRg70wfMvuuiv11tl337TfFYPeyjB6NMycCR/9KHz1qzB2LMyf33z/vHlw6qmwzz6pN9R3v5tmlD3jjJRkzKzD2kwQkg6S9CTwdHa8t6SfdvSEkvoBJwC3NJVFxMqI+Fu2Pw14Dti9o+fo9e6+O01X/e53pzmVmibYq1Vbb50uNV17bZpddu+9U+1i9OjUAP3rX6eR388/n7qx1sJlNLMakOcn1g+AI4A7ASJilqRDO3HO9wFPR8SCpgJJg4FlEbFW0q7AcKAL+zn2Ivfdl2YhffvbYeLE9OVaD6S0KNFBB6VJ9S67LJX37Qs33+yZV80KkOsSU0TM36CohQvBzSTdBDQCe0haIOmM7K6TeHPj9KHA7Kzb66+BsyKim+deqAOTJ8OHPpTmE5o4EQYNKjuirjd8eGp0r2xXePLJ8uIxq2N5ahDzJR0EhKT+wDnAU209KSJObqH8k1XKbgNuyxGLteThh1Mj7c47p7799dw+M3ZsGlC3apUHupkVKE+COAv4IbADsBC4Gzi7yKCsnR59FI46Ct761nSJaciQsiMq1qhR1RcsMrMu1WaCiIiXgVO6IRbriOnT4Ygj0hoO990H229fdkTdY9QoJwazgrWYICT9GGhxNaGI+HwhEVl+s2bB+9+feinddx/suGPZEZlZHWmtkXoqMA3YmDRlxpxsGwEMKD40a9X116cePX37puSwyy5lR2RmdabFGkRETACQ9Bng4IhYkx3/DGjonvCsqrvvToPDII0yXrw4zVRqZtaF8nRzHQhUjmrePCuzslx3XfP+6tWpsdbMrIvl6cX0HWCGpEmASGMWLikyKGtD04I6Xs/AzAqUpxfT/0j6A3AAqdH6yxHxUuGRWcueey7NPfTRj7qbp5kVJu9sZvuTZmCFlCT+t5hwrE2vvQYzZqSlOS+8sOxozKyO5Zms7zuk0dNPZtvnJX2r6MCsBQ8/nBqmvb6BmRUsTw3iaGBERKwDkDQBmAF8pcjArAUNDantwZeVzKxgedeDqJwStMbnjq5xDQ1pjYfNNy87EjOrc3lqEN/mzb2YLig0Kqtu5Up45BE421NhmVnx8vRiuknS/cB7siL3YirLn/+ckoTbH8ysG+RppB4NvBoRd5IGzH1Jkud1KENDNoD94IPLjcPMeoU8bRBXAcsl7U1aL/o54LrWn2KFaGiAPfdMM7eamRUsT4JYExEBHAtcGRFXAlsUG5a9ydq18NBDvrxkZt0mTyP1a5IuBD4OHCqpD9C/2LDsTWbPhldfhUM7sxy4mVl+eWoQHwNWAmdkjdM7ApcXGpW9WVP7g2sQZtZN2kwQEfFSRHw/Ihqy479GRJttEJKukbRE0uMVZZdIWihpZrYdXXHfhZLmSnpG0hEdfUN1a/LktObDTjuVHYmZ9RItJghJD2a3r0l6dcPbHK99LXBklfIfRMSIbLsrO8eewEnAO7Pn/FRS3/a+mboVkWoQrj2YWTdqbcGgg7PbDjVIR8RkSUNzPvxY4OaIWAm8IGkuaYLAxo6cu+7MmQNLlrj9wcy6Va6pNiTtK+nzkj4naZ9OnvOzkmZnl6CaFh7aAZhf8ZgFWZmB2x/MrBR5BspdDEwAtgG2Ba6V9NUOnu8qYDfSutaLgCva+wKSzpQ0VdLUpUuXdjCMGjN5MgweDHvsUXYkZtaL5Onmegqwd0S8Af+a/nsm8I32niwiFjftS7oa+F12uBCobH3dMSur9hrjgfEAI0eOjPbGUJOa2h+ksiMxs14kzyWmF4GNK443ooUv77ZI2r7i8HigqYfTncBJkjaSNAwYDjzakXPUnQUL4IUXfHnJzLpdnhrEK8ATkiaSVpN7P/CopB8BRMTnqz1J0k3AGGBbSQuArwFjJI3IXmce8OnsNZ6Q9CvSgkRrgLMjYm0n3lf9cPuDmZUkT4K4Pdua3J/nhSPi5CrFv2jl8d8EvpnntXuVhoa09sPee5cdiZn1Mi0mCElbRsSrETGhyn07R8Rfiw3NgJQgRo+GfnmXDzcz6xqttUHc37Qj6d4N7vttIdHY+pYtg8cf9+UlMytFawmissvMoFbus6I8+GC6dYIwsxK0liCihf1qx1aEhgYYMAD237/sSMysF2rtwvZ2ks4l1Raa9smOBxcemaUEsf/+sPHGbT/WzKyLtVaDuJq0MNDmFftNxz8vPrRe7vXXYdo0X14ys9K0Nlnf17szENvAlCmwZo0ThJmVJtdkfVaChgbo0wcOOqjsSMysl3KC6KkaGtLguK22KjsSM+ulWk0QkvpIOrG7grHMqlXQ2OjLS2ZWqlYTRESsA77UTbFYk+nTYcUKLxBkZqXKc4npHknnS9pJ0qCmrfDIerOmCfoOPrjcOMysV8szwc/HstuzK8oC2LXrwzEgLRC0++4wZEjZkZhZL9ZmgoiIYd0RiGXWrYOHHoITTig7EjPr5fIsObqppK9KGp8dD5f0weJD66WeeAL+/ne3P5hZ6fK0QfwPsApo6pC/kA4sN2o5eYEgM+sh8iSI3SLiu8BqgIhYjmdzLc7kybDDDjB0aNmRmFkvlydBrJK0CdkMrpJ2A1YWGlVvFZFqEIceCnIONrNy5enF9DXgj8BOkm4ARgOfLDKoXuuFF+DFF315ycx6hDy9mCZKmg4cSLq0dE5EvFx4ZL3R5Mnp1gnCzHqAvHMxvRcYCxwG5Pr2knSNpCWSHq8ou1zS05JmS7pd0tZZ+VBJKyTNzLaftfeN1IWGBhg0CPbcs+xIzMxydXP9KXAW8BjwOPBpSVfmeO1rgSM3KJsIvCsi9gKeBS6suO+5iBiRbWflCb7uNDSk0dN9PIeimZUvTxvE4cA7IqKpkXoC8ERbT4qIyZKGblB2d8XhFOAjuSOtdy+9BHPmwJlnlh2JmRmQ7xLTXGDniuOdsrLO+nfgDxXHwyTNkPSApBYvY0k6U9JUSVOXLl3aBWH0EB7/YGY9TJ4EsQXwlKT7JU0CngS2lHSnpDs7clJJFwFrgBuyokXAzhGxD3AucKOkLas9NyLGR8TIiBg5eHAdLY3d0ACbbgr77lt2JGZmQL5LTBd35QklfRL4IDC26bJVRKwkG1sREdMkPQfsDkztynP3aA0NMGoU9O9fdiRmZkC+bq4PdNXJJB1JWl/ivdmI7KbywcCyiFgraVdgOPB8V523x/vHP2DWLPja18qOxMzsX/LUIDpE0k3AGGBbSQtIA+4uBDYCJiqNFJ6S9Vg6FLhU0mpgHXBWRCwrKrYe5+GH0yhqT9BnZj1IYQkiIk6uUvyLFh57G3BbUbH0eA0N6dLSAQeUHYmZ2b+0q8O9pIGS9ioqmF6roQH22y81UpuZ9RB5BsrdL2nLbJnR6cDVkr5ffGi9xIoV8Oij7t5qZj1OnhrEVhHxKnACcF1EHAC8r9iwepFHH4XVq93+YGY9Tp4E0U/S9sCJwO8Kjqf3aWhIU3uPHl12JGZm68mTIC4F/gTMjYg/Z91Q5xQbVi8yeTK8610wcGDZkZiZrSfPOIhbgVsrjp8HPlxkUL3GmjXQ2AinnVZ2JGZmb9JmgsgGsX0KGFr5+Ij49+LC6iVmzoR//tMN1GbWI+UZB3EH0ADcA6wtNpxexhP0mVkPlidBbBoRXy48kt5o8mTYdVd461vLjsTM7E3yNFL/TtLRhUfS20TAgw+6e6uZ9Vh5EsQ5pCSxQtKrkl6T9GrRgdW9p5+Gl1/25SUz67FavcSkNKPeOyPir90UT+8xeXK6dYIwsx6q1RpEtl7D77splt6loQHe8hZ429vKjsTMrKo8l5imS3pP4ZH0NvfcA9tuC1OmlB2JmVlVeRLEAUCjpOckzZb0mKTZRQdW126/HRYvhieegLFj02A5M7MeJk831yMKj6K3ueWWdBsBq1bB/fen5UbNzHqQPAkiCo+it1m3Lt327QsDBsCYMaWGY2ZWTZ4E8XtSkhCwMTAMeAZ4Z4Fx1bd582DECDjxxJQcXHswsx4oz2R97648lrQv8H8Li6jerVgBM2bA+efDhReWHY2ZWYvateQoQERMJzVct0nSNZKWSHq8omyQpImS5mS3A7NySfqRpLlZY/i+7Y2tJkyfnmZxPfDAsiMxM2tVniVHz63Yzpd0I/Bizte/Fjhyg7ILgHsjYjhwb3YMcBQwPNvOBK7KeY7a0tRjyZeVzKyHy1OD2KJi24jUJnFsnhePiMnAsg2KjwUmZPsTgOMqyq+LZAqwdbaSXX1pbEwT9G23XdmRmJm1Kk8j9ZPZokH/IumjVCwi1E5DImJRtv8SMCTb3wGYX/G4BVnZIupFREoQhx9ediRmZm3KU4Oo1pLaJa2r2VQe7epGK+lMSVMlTV26dGlXhNF9/vpXWLTIl5fMrCa0WIOQdBRwNLCDpB9V3LUlsKYT51wsafuIWJRdQlqSlS8Edqp43I5Z2XoiYjwwHmDkyJG1NUbD7Q9mVkNaq0G8CEwF3gCmVWx30rnR1XcCTYswn0Zasa6p/BNZb6YDgVcqLkXVhylTYJNN4N3vbvuxZmYla7EGERGzgFlZr6V+wM4R8Ux7XlzSTcAYYFtJC4CvAd8BfiXpDOAvwInZw+8i1VjmAsuB09v3VmpAYyO85z3Qv3/ZkZiZtSlPI/WRwPeAAcAwSSOASyPimLaeGBEnt3DX2CqPDeDsHPHUpjfeSAPkzj237EjMzHLJ00h9CbA/8A+AiJhJmm7D2mPaNFi92u0PZlYz8iSI1RHxygZltdU43BO4gdrMakyeS0xPSPo3oK+k4cDngYeLDasOeYCcmdWYPDWIz5Fmbl0J3Ai8CowrMqi60zRAzrUHM6sheWZzXQ5clG0ASNoZ+GuBcdWX+fPTADlP0GdmNaTVGoSkUZI+Imm77HivrNvrQ90SXb1w+4OZ1aAWE4Sky4FrgA8Dv5f0DeBu4BHSjKuWV2NjGiC3115lR2Jmlltrl5j+D7BPRLyRrdkwH3hXRMzrlsjqiQfImVkNau0S0xsR8QZARPwdmOPk0AFNA+R8ecnMakxrNYhdJd1ZcTys8jjPSGrDA+TMrGa1liA2XBToiiIDqVtTpqRb92AysxrT2mR9D3RnIHWrsRGGDYMhQ9p+rJlZD5JnoJx1lAfImVkNc4Io0vz58OKLThBmVpNyJwhJmxYZSF3yADkzq2FtJghJB0l6Eng6O95b0k8Lj6weeICcmdWwPDWIH5CWGP0b/GuluUOLDKpuTJniAXJmVrNyXWKKiPkbFK0tIJb68sYbMH26u7eaWc3Ksx7EfEkHASGpP3AO8FSxYdWB6dM9QM7MalqeBHEW8ENgB2AhacK+Dq8dLWkP4JaKol2Bi4GtgU8BS7Pyr0TEXR09T+ncQG1mNS7PehAvA6d01Qkj4hlgBICkvqSkcztwOvCDiPheV52rVB4gZ2Y1rs0EIelHVYpfAaZGxB2dPP9Y4LmI+IukTr5UD9I0QG7MmLIjMTPrsDyN1BuTfvHPyba9gB2BMyT9VyfPfxJwU8XxZyXNlnRNNsV4bfIAOTOrA3kSxF7AYRHx44j4MfA+4O3A8cAHOnpiSQOAY4Bbs6KrgN1IyWgRLUwOKOlMSVMlTV26dGm1h5TPE/SZWR3IkyAGAptXHG8GDIqItcDKTpz7KGB6RCwGiIjFEbE2ItYBVwP7V3tSRIyPiJERMXLw4MGdOH2BmgbI7b132ZGYmXVYnl5M3wVmSrofEGmQ3LckbQbc04lzn0zF5SVJ20fEouzweODxTrx2uRobYeRID5Azs5qWpxfTLyTdRfMv+q9ExIvZ/hc7ctIsubwf+HRF8XcljQACmLfBfbWjaYDcF75QdiRmZp2SpwYB8AapXWBj4G2S3hYRkzt60oh4Hdhmg7JTO/p6PYoHyJlZncjTzfU/SKOndwRmAgcCjcDhxYZWozxAzszqRJ5G6nOA9wB/iYjDgH2AfxQaVS2bMsUD5MysLuRJEG9ExBsAkjaKiKeBPYoNq4Y1Nrp7q5nVhTxtEAskbQ38Fpgo6e/AX4oNq0bNnw8LF/rykpnVhTy9mI7Pdi+RNAnYCvhjoVHVKrc/mFkdaTVBZJPpPRERbweIiAe6Japa5QFyZlZHWm2DyEZLPyNp526Kp7Z5gJyZ1ZE8bRADgSckPQq83lQYEccUFlUt8gA5M6szeRLE/ys8inowY0YaIOceTGZWJ/I0Uj8gaRdgeETcI2lToG/xodUYN1CbWZ1pcxyEpE8Bvwb+OyvagdTl1So1NsLQofCWt5QdiZlZl8gzUO5sYDTwKkBEzAG2KzKomtTY6NqDmdWVPAliZUSsajqQ1I8046o18QA5M6tDeRLEA5K+Amwi6f2kFeD+t9iwaozbH8ysDuVJEBcAS4HHSGs03AV8tcigas6UKbDxxrDXXmVHYmbWZfJ0cz0OuC4iri46mJrVNEBuwICyIzEz6zJ5ahAfAp6V9EtJH8zaIKzJypVpgJwvL5lZnWkzQUTE6cDbSG0PJwPPSfp50YHVjOnTYdUqJwgzqzu5agMRsVrSH0i9lzYhXXb6jyIDqxluoDazOpVnoNxRkq4F5gAfBn4OeDRYEw+QM7M6lacG8QngFuDTEbGyq04saR7wGrAWWBMRIyUNys41FJgHnBgRf++qcxZiyhQ45JCyozAz63J52iBOjojfNiUHSQdLurKLzn9YRIyIiJHZ8QXAvRExHLg3O+65FixImyfoM7M6lKcXE5L2kXR59qv/MuDpguI5FpiQ7U8gtXX0XG5/MLM61uIlJkm7k3otnQy8TLr0o4g4rIvOHcDdkgL474gYDwyJiEXZ/S8BQ7roXMVobEwD5LyCnJnVodbaIJ4GGoAPRsRcAElduRrOwRGxUNJ2wERJ69VKIiKy5LEeSWcCZwLsvHPJC915gJyZ1bHWLjGdACwCJkm6WtJYQF114ohYmN0uAW4H9gcWS9oeILtdUuV54yNiZESMHDx4cFeF034eIGdmda7FBJE1TJ8EvB2YBIwDtpN0laQPdOakkjaTtEXTPvAB4HHgTuC07GGnAXd05jyF8gA5M6tzeVaUex24EbhR0kDgo8CXgbs7cd4hwO2SmmK4MSL+KOnPwK8knQH8BTixE+co1pQp6dY9mMysTrVrXqVsTML4bOuwiHgeeFPLbkT8DRjbmdfuNo2NsMsusP32ZUdiZlaIXN1crYr774ettmru6mpmVmecIDri1lth6VJ47DEYO9ZJwszqkhNEe61dC1/6UtqPSA3V999fakhmZkVwgmivb3wD5s1LYx/69k23Y8aUHZWZWZfz4j/tMXkyXHopnHoqfOYzqeYwZoy7uppZXXKCyGvZMjjlFNh1V7jySthiCycGM6trThB5RMAZZ8DixalBeostyo7IzKxwThB5XHUV/Pa3cMUVsN9+ZUdjZtYt3Ejdltmz4dxz4aijYNy4sqMxM+s2ThCtWb4cTjoJBg6Ea6+FPv64zKz38CWm1owbB08/DXffDdttV3Y0Zmbdyj+JW3LrrXD11fDlL8P73ld2NGZm3c4Jopp58+BTn4IDDkjjHszMeiEniA2tXg3/9m+pa+tNN0H//mWXHgG3AAAKgklEQVRHZGZWCrdBbOiSS9JYh5tvhmHDyo7GzKw0rkFUuu8++Pa306C4j32s7GjMzErlBNFk6VL4+Mdhjz3ghz8sOxozs9L5EhOk9obTT0/zLf3hD7DZZmVHZGZWOicISDWG3/8efvxj2PtNK6GamfVK3X6JSdJOkiZJelLSE5LOycovkbRQ0sxsO7pbApo+PS0AdMwxcPbZ3XJKM7NaUEYNYg1wXkRMl7QFME3SxOy+H0TE97otkn/+M02lsd12cM01IHXbqc3MerpuTxARsQhYlO2/JukpYIfujgOAz34Wnnsu9V7aZptSQjAz66lK7cUkaSiwD/BIVvRZSbMlXSNpYKEnv+QSmDABTjsN3vveQk9lZlaLSksQkjYHbgPGRcSrwFXAbsAIUg3jihaed6akqZKmLl26tGMnv/VW+PrX0/7NN6eBcWZmtp5SEoSk/qTkcENE/AYgIhZHxNqIWAdcDexf7bkRMT4iRkbEyMGDB3csgEcfbd5ftSqtLW1mZuspoxeTgF8AT0XE9yvKt6942PHA44UFccIJsMkm0LcvDBgAY8YUdiozs1pVRi+m0cCpwGOSZmZlXwFOljQCCGAe8OnCIhg1Cu69N9UcxoxJx2Zmtp4yejE9CFTrT3pXtwYyapQTg5lZKzwXk5mZVeUEYWZmVTlBmJlZVU4QZmZWlROEmZlV5QRhZmZVKSLKjqHDJC0F/lJ2HK3YFni57CBa4fg6x/F1juPrnM7Et0tEtDkVRU0niJ5O0tSIGFl2HC1xfJ3j+DrH8XVOd8TnS0xmZlaVE4SZmVXlBFGs8WUH0AbH1zmOr3McX+cUHp/bIMzMrCrXIMzMrConiA6StJOkSZKelPSEpHOy8kskLZQ0M9uOrnjOhZLmSnpG0hHdEOM8SY9lcUzNygZJmihpTnY7MCuXpB9l8c2WtG/Bse1R8RnNlPSqpHFlfn7ZUrdLJD1eUdbuz0vSadnj50g6reD4Lpf0dBbD7ZK2zsqHSlpR8Tn+rOI5+2X/LuZm76Ha7MpdFV+7/56SjszK5kq6oCtiayW+Wypim9e0BEFJn19L3ynl/RuMCG8d2IDtgX2z/S2AZ4E9gUuA86s8fk9gFrARMAx4DuhbcIzzgG03KPsucEG2fwHwn9n+0cAfSFOxHwg80o2fZV/gJWCXMj8/4FBgX+Dxjn5ewCDg+ex2YLY/sMD4PgD0y/b/syK+oZWP2+B1Hs1iVvYejiowvnb9PbPtOWBXYED2mD2Lim+D+68ALi7x82vpO6W0f4OuQXRQRCyKiOnZ/mvAU8AOrTzlWODmiFgZES8Ac2lhWdWCHQtMyPYnAMdVlF8XyRRga62/yl+RxgLPRURrgx4L//wiYjKwrMp52/N5HQFMjIhlEfF3YCJwZFHxRcTdEbEmO5wC7Njaa2QxbhkRUyJ9m1xX8Z66PL5WtPT33B+YGxHPR8Qq4ObssYXGl9UCTgRuau01Cv78WvpOKe3foBNEF5A0FNgHeCQr+mxW5bumqTpI+kPPr3jaAlpPKF0hgLslTZN0ZlY2JCIWZfsvAUNKjK/JSaz/H7OnfH7Q/s+rzM/x30m/KJsMkzRD0gOSDsnKdshi6s742vP3LOvzOwRYHBFzKspK+/w2+E4p7d+gE0QnSdocuA0YFxGvAlcBuwEjgEWkamtZDo6IfYGjgLMlHVp5Z/YLqNRubJIGAMcAt2ZFPenzW09P+LxaIukiYA1wQ1a0CNg5IvYBzgVulLRlCaH12L/nBk5m/R8ppX1+Vb5T/qW7/w06QXSCpP6kP+QNEfEbgIhYHBFrI2IdcDXNl0EWAjtVPH3HrKwwEbEwu10C3J7Fsrjp0lF2u6Ss+DJHAdMjYnEWa4/5/DLt/by6PU5JnwQ+CJySfYGQXbr5W7Y/jXRdf/cslsrLUIXG14G/ZxmfXz/gBOCWirhL+fyqfadQ4r9BJ4gOyq5Z/gJ4KiK+X1Feed3+eKCpx8SdwEmSNpI0DBhOauwqKr7NJG3RtE9qzHw8i6OpV8NpwB0V8X0i6xlxIPBKRbW2SOv9cuspn1+F9n5efwI+IGlgdjnlA1lZISQdCXwJOCYilleUD5bUN9vflfR5PZ/F+KqkA7N/w5+oeE9FxNfev+efgeGShmW1y5OyxxbpfcDTEfGvS0dlfH4tfadQ5r/Brmh9740bcDCpqjcbmJltRwO/BB7Lyu8Etq94zkWkXyLP0EU9H1qJb1dSD5BZwBPARVn5NsC9wBzgHmBQVi7gyiy+x4CR3fAZbgb8Ddiqoqy0z4+UqBYBq0nXbc/oyOdFaguYm22nFxzfXNL15qZ/gz/LHvvh7O8+E5gOfKjidUaSvqifA35CNmC2oPja/ffM/h89m913UZGfX1Z+LXDWBo8t4/Nr6TultH+DHkltZmZV+RKTmZlV5QRhZmZVOUGYmVlVThBmZlaVE4SZmVXlBGE1SVJIuqLi+HxJl3Tj+TeSdI/STJ8f2+C+ayW9oOaZQB9u47XeKunXXRDTJZLO7+zrmDXpV3YAZh20EjhB0rcj4uUSzr8PQESMaOH+L0ZEri/9iHgR+EhXBWbWVVyDsFq1hrTk4hc2vCP7Bf+RiuN/ZrdjsonX7pD0vKTvSDpF0qNK8/vvVuW1Bkn6bTbZ3BRJe0naDrgeeE9WQ3jT86rJfuH/UlKj0jz9n8rKhypbo0DSO7N4ZmbnHJ6Vnyvp8WwbV/GaF0l6VtKDwB4V5btJ+qPSRI0Nkt6elX80e41Zkibnidt6L9cgrJZdCcyW9N12PGdv4B2kaZ+fB34eEfsrLc7yOWDcBo//OjAjIo6TdDhpeuURkv6DtM7BB1s4z+WSvprtPxERp2T7e5Hm7t8MmCHp9xs87yzghxFxQzbVRF9J+wGnAweQRs8+IukB0g+8k0gT4fUjjfidlr3OeNLo4DmSDgB+ChwOXAwcERELlS0uZNYSJwirWRHxqqTrgM8DK3I+7c+RzTEl6Tng7qz8MeCwKo8/mDTtAhFxn6RtlG9Wz5YuMd0RESuAFZImkSavm1lxfyNwkaQdgd9kX/AHA7dHxOtZ3L8hTU/dJytfnpXfmd1uDhwE3Krmxc42ym4fAq6V9CugaTI4s6p8iclq3X+R5vzZrKJsDdm/bUl9SCuTNVlZsb+u4ngd3fODacO5bdY7jogbSdOfrwDuymot7dUH+EdEjKjY3pG9/lnAV0mzfU6TtE0HXt96CScIq2kRsQz4FSlJNJkH7JftHwP078QpGoBTILVhAC/HBnP0t9OxkjbOvpjHkGYv/Zds5tDnI+JHpFk798piOE7Spkoz8x6flU3OyjdRmrn3Q5BqVsALkj6avaYk7Z3t7xYRj0TExcBS1p8W2mw9vsRk9eAK4LMVx1cDd0iaBfwReL0Tr30JcI2k2cBymqddbktlGwQ0r4MwG5gEbAtcFhEvKq0e1uRE4FRJq0mrh30rIpZJupbm6c1/HhEzACTdQpqxdwnrJ5tTgKuyGPqTlu6clcU1nNSWcW9WZlaVZ3M16ybZOI1/RsT3yo7FLA9fYjIzs6pcgzAzs6pcgzAzs6qcIMzMrConCDMzq8oJwszMqnKCMDOzqpwgzMysqv8PkLkFoaOXHWAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot average returns\n",
    "returns_over_100_episodes = []\n",
    "x = []\n",
    "for i in range(0,int(num_episodes/100)):\n",
    "    returns_over_100_episodes.append(sum(returns[100*i:100*(i+1)-1])/100)\n",
    "    x.append((i+1)*100)\n",
    "plt.plot(x,returns_over_100_episodes,'.-r')\n",
    "plt.ylabel('Average Returns per Episode')\n",
    "plt.xlabel('Num of Episodes')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "# DEMO FINAL NETWORK\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = np.reshape(state, [1, state.size])\n",
    "total_reward = 0\n",
    "for i in range(0, max_steps):\n",
    "    #env.render() # here I comment the render for pyplot package issue\n",
    "    # Get action from Q-network\n",
    "    var_state = Variable(torch.unsqueeze(FloatTensor(state[0]),0))\n",
    "    # above change the (4,) to (1,4) in variable \n",
    "    \n",
    "    # Qs = output of DQN.model when state is passed in\n",
    "    Qs = DQN.model.forward(var_state)# shape of (1, 2) variable\n",
    "    action = int(torch.max(Qs,1)[1].data.cpu().numpy())\n",
    "    # torch.max(Qs,1) get max value and its index of Qs along 1 axis, and torch.max(Qs,1)[1] is to get maxindex\n",
    "    # Take action, get new state and reward\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if done: break\n",
    "    else: state = np.reshape(next_state, [1, state.size])\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "np.save(\"./outputs/returns.npy\", returns)\n",
    "# myreturns = np.load( \"./outputs/returns.npy\" )\n",
    "torch.save(DQN.model, './outputs/trainedDQN.pkl')\n",
    "# myDQN = torch.load('./outputs/trainedDQN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################Some Untils###################\n",
    "\n",
    "##### save the array of the returns\n",
    "# np.save(\"returns.npy\", returns)\n",
    "# myreturns = np.load( \"returns.npy\" )\n",
    "\n",
    "##### save the whole model\n",
    "# torch.save(DQN.model, './outputs/trainedDQN.pkl')\n",
    "# myDQN = torch.load('./outputs/trainedDQN.pkl')\n",
    "\n",
    "##### save the parameter of the model\n",
    "# torch.save(DQN.model.state_dict(), './outputs/DQN_params.pkl')\n",
    "# need construct a new network\n",
    "# myDQN.load_state_dict(torch.load('./outputs/DQN_params.pkl'))\n",
    "\n",
    "##### load the whole model\n",
    "# myDQN = torch.load('./outputs/DQN.pkl', map_location=lambda storage, loc: storage)\n",
    "# myDQN.cpu()\n",
    "\n",
    "\n",
    "\n",
    "###### running the epochs \n",
    "# for epoch in range(num_epochs):\n",
    "#     train(...)  # Train\n",
    "#     acc = eval(...)  # Evaluate after every epoch\n",
    "\n",
    "#     # Some stuff with acc(accuracy)\n",
    "#     ...\n",
    "\n",
    "#     # Get bool not ByteTensor\n",
    "#     is_best = bool(acc.numpy() > best_accuracy.numpy())\n",
    "#     # Get greater Tensor to keep track best acc\n",
    "#     best_accuracy = torch.FloatTensor(max(acc.numpy(), best_accuracy.numpy()))\n",
    "#     # Save checkpoint if is a new best\n",
    "#     save_checkpoint({\n",
    "#         'epoch': start_epoch + epoch + 1,\n",
    "#         'state_dict': model.state_dict(),\n",
    "#         'best_accuracy': best_accuracy\n",
    "#     }, is_best)\n",
    "\n",
    "\n",
    "####### def the checkpoint\n",
    "# def save_checkpoint(state, is_best, filename='/output/checkpoint.pth.tar'):\n",
    "#     \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "#     if is_best:\n",
    "#         print (\"=> Saving a new best\")\n",
    "#         torch.save(state, filename)  # save checkpoint\n",
    "#     else:\n",
    "#         print (\"=> Validation Accuracy did not improve\")\n",
    "\n",
    "\n",
    "\n",
    "######## resume the saved state\n",
    "# if cuda:\n",
    "#     checkpoint = torch.load(resume_weights)\n",
    "# else:\n",
    "#     # Load GPU model on CPU\n",
    "#     checkpoint = torch.load(resume_weights,\n",
    "#                             map_location=lambda storage,\n",
    "#                             loc: storage)\n",
    "# start_epoch = checkpoint['epoch']\n",
    "# best_accuracy = checkpoint['best_accuracy']\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
