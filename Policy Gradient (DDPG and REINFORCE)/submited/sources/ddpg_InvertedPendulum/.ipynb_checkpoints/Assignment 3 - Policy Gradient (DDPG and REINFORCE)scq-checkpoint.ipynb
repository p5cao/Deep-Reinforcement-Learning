{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Policy Gradients (DDPG and REINFORCE)\n",
    "\n",
    "Name: Chuqiao Song\n",
    "\n",
    "ID: A53239614"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "This exercise requires you to solve various continous control problems in OpenAI-Gym.  \n",
    "\n",
    "DDPG is policy gradient actor critic method for continous control which is off policy. It tackles the curse of dimensionality / loss of performance faced when discretizing a continous action domain. DDPG uses similiar \"tricks\" as DQN to improve the stability of training, including a replay buffer and target networks.\n",
    "\n",
    "Furthermore, you will implement REINFORCE for discrete and continous environments, and as a bonus compare the sample efficiency and performance with DQN and DDPG.\n",
    "\n",
    "\n",
    "### DDPG paper: https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:\n",
    "<img src=\"inverted_pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Pendulum-v0 environment:\n",
    "<img src=\"pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"half_cheetah.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment for Actor Critic\n",
    "- inline plotting\n",
    "- gym\n",
    "- directory for logging videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "#environment\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "logging_interval = 100\n",
    "animate_interval = logging_interval * 5\n",
    "logdir='./DDPG/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up gym environment\n",
    "The code below does the following for you:\n",
    "- Wrap environment, log videos, setup CUDA variables (if GPU is available)\n",
    "- Record action and observation space dimensions\n",
    "- Fix random seed for determinisitic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:31:40,975] Making new env: InvertedPendulum-v1\n",
      "[2018-05-14 15:31:40,984] Clearing 38 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "VISUALIZE = True\n",
    "SEED = 0\n",
    "MAX_PATH_LENGTH = 500\n",
    "NUM_EPISODES = 12000\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Environments to be tested on\n",
    "env_name = 'InvertedPendulum-v1'\n",
    "# env_name = 'Pendulum-v0'\n",
    "# env_name = 'HalfCheetah-v1' \n",
    "\n",
    "# wrap gym to save videos\n",
    "env = gym.make(env_name)\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "env._max_episode_steps = MAX_PATH_LENGTH\n",
    "\n",
    "# check observation and action space\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    print(\"This is a discrete action space, probably not the right algorithm to use\")\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate your understanding of the simulation:\n",
    "For the environments mentioned above ('Pendulum-v0', 'HalfCheetah-v2', 'InvertedPendulum-v2'),\n",
    "- describe the reward system\n",
    "- describe the each state variable (observation space)\n",
    "- describe the action space\n",
    "- when is the environment considered \"solved\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an action normalization class:\n",
    "To train across various environments, it is useful to normalize action inputs and outputs between [-1, 1]. This class should take in actions and implement forward and reverse functions to map actions between [-1, 1] and [action_space.low, action_space.high].\n",
    "\n",
    "Using the following gym wrapper, implement this class.\n",
    "- https://github.com/openai/gym/blob/78c416ef7bc829ce55b404b6604641ba0cf47d10/gym/core.py\n",
    "- i.e. we are overriding the outputs scale of actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAction(gym.ActionWrapper):\n",
    "    def action(self, act):\n",
    "        # [-1, 1] => [action_space.low, action_space.high]\n",
    "        #tanh outputs (-1,1) from tanh, need to be [action_space.low, action_space.high]\n",
    "        act = (act + 1)/2  #[-1, 1] => [0,1]\n",
    "        act = act * (self.action_space.high - self.action_space.low)\n",
    "        act = act + self.action_space.low\n",
    "        return act\n",
    "    \n",
    "    def reverse_action(self, act):\n",
    "        # [action_space.low, action_space.high] => [-1,1]\n",
    "        #reverse of that above\n",
    "        act = act - self.action_space.low\n",
    "        act = act / (self.action_space.high - self.action_space.low)\n",
    "        act = act * 2 - 1\n",
    "        return act\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a weight syncing function\n",
    "In contrast to DQN, DDPG uses soft weight sychronization. At each time step following training, the actor and critic target network weights are updated to track the rollout networks. \n",
    "- target_network.weights <= target_network.weights \\* (1 - tau) + source_network.weights \\* (tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightSync(target_model, source_model, tau = 0.001):\n",
    "    # soft update\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Replay class that includes all the functionality of a replay buffer\n",
    "DDPG is an off policy actor-critic method and an identical replay buffer to that used for the previous assignment is applicable here as well (do not include the generate_minibatch method in your Replay class this time). Like before, your constructor for Replay should create an initial buffer of size 1000 when you instantiate it.\n",
    "\n",
    "The replay buffer should kept to some maximum size (60000), allow adding of samples and returning of samples at random from the buffer. Each sample (or experience) is formed as (state, action, reward, next_state, done). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay():\n",
    "    def __init__(self):\n",
    "        self.capacity = 60000\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def initialize(self, init_length, envir):\n",
    "        st = envir.reset()\n",
    "        for _ in range(init_length):\n",
    "            a = envir.action_space.sample()\n",
    "            st1, r, done, info = envir.step(a)\n",
    "            # normalizing action \n",
    "            # [action_space.low, action_space.high] => [-1,1]\n",
    "            a = envir.reverse_action(a)\n",
    "            self.push((st, a, st1, r, done))\n",
    "            if done: st = envir.reset()\n",
    "            else : st = st1\n",
    "                \n",
    "    def push(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def generateMinibatch(self, batch_size):\n",
    "        batch_memory = random.sample(self.memory, batch_size) #return a list\n",
    "        batch_memory = list(zip(*batch_memory))\n",
    "        \n",
    "        batch_st = Variable(FloatTensor(batch_memory[0]))\n",
    "        batch_at = Variable(FloatTensor(batch_memory[1]))\n",
    "        batch_st1 = Variable(FloatTensor(batch_memory[2]))\n",
    "        batch_r = Variable(torch.unsqueeze(FloatTensor(batch_memory[3]),1))\n",
    "        batch_done = torch.unsqueeze(FloatTensor(batch_memory[4]),1)\n",
    "        \n",
    "        return batch_st, batch_at, batch_st1, batch_r, batch_done\n",
    "        \n",
    "    def __len__(self):            \n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write an Ornstein Uhlenbeck process class for exploration noise\n",
    "The proccess is described here:\n",
    "- https://en.wikipedia.org/wiki/Ornsteinâ€“Uhlenbeck_process\n",
    "- http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "\n",
    "You should implement:\n",
    "- a step / sample method\n",
    "- reset method\n",
    "\n",
    "Use theta = 0.15, mu = 0, sigma = 0.3, dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckProcess():\n",
    "    def __init__(self, mu=np.zeros(act_dim), sigma=0.3, theta=.15, dimension=1e-2, x0=None,num_steps=12000):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dimension\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def step(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Deep Neural Network class that creates a dense network of a desired architecture for actor and critic networks\n",
    "\n",
    "\n",
    "#### Actor\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: Tanh\n",
    "\n",
    "- hidden_state sizes: 400\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers\n",
    "\n",
    "- weight initialization: normal distribution with small variance. \n",
    "\n",
    "#### Critic\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: None\n",
    "\n",
    "- hidden_state sizes: 300, 300 + action size\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers prior to the action input\n",
    "\n",
    "- weight initialization: normal distribution with small variance.\n",
    "\n",
    "Good baselines can be found in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# actor model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 400 units per layer, tanh output to bound outputs between -1 and 1\n",
    "\n",
    "class actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 400)\n",
    "        self.bn1 = nn.BatchNorm1d(400) # batchnormalization\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        self.bn2 = nn.BatchNorm1d(400) # batchnormalization\n",
    "        self.fc3 = nn.Linear(400, output_size)\n",
    "        \n",
    "        # parameters initialization\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.normal_(self.fc1.bias)\n",
    "        nn.init.normal_(self.fc2.bias)\n",
    "        nn.init.normal_(self.fc3.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = self.bn1(x)  # turn off for inverted-pendulum -v1\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.bn2(x)  # turn off for inverted-pendulum -v1\n",
    "        \n",
    "        outputs = F.tanh(self.fc3(x))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# critic model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 300 units per layer, ouputs rewards therefore unbounded\n",
    "# Action not to be included until 2nd layer of critic (from paper). Make sure to formulate your critic.forward() accordingly\n",
    "\n",
    "class critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, output_size):\n",
    "        super(critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, 300)\n",
    "        self.bn1 = nn.BatchNorm1d(300) # batchnormalization\n",
    "        self.fc2 = nn.Linear(300 + action_size, 300)\n",
    "        self.fc3 = nn.Linear(300, output_size)\n",
    "\n",
    "        # parameters initialization\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.normal_(self.fc1.bias)\n",
    "        nn.init.normal_(self.fc2.bias)\n",
    "        nn.init.normal_(self.fc3.bias)\n",
    "    \n",
    "    def forward(self, states, actions):\n",
    "        x = F.relu(self.fc1(states))\n",
    "#         x = self.bn1(x)  # turn off for inverted-pendulum -v1\n",
    "        x = torch.cat((x, actions), 1) # actions only join at second layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        outputs = self.fc3(x)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG class to encapsulate definition, rollouts, and training\n",
    "\n",
    "- gamma = 0.99\n",
    "\n",
    "- actor_lr = 1e-4\n",
    "\n",
    "- critic_lr = 1e-3\n",
    "\n",
    "- critic l2 regularization = 1e-2\n",
    "\n",
    "- noise decay\n",
    "\n",
    "- noise class\n",
    "\n",
    "- batch_size = 128\n",
    "\n",
    "- optimizer: Adam\n",
    "\n",
    "- loss (critic): mse\n",
    "\n",
    "Furthermore, you can experiment with action versus parameter space noise. The standard implimentation works with action space noise, howeve parameter space noise has shown to produce excellent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, obs_dim, act_dim, critic_lr = 1e-3, actor_lr = 1e-4, gamma = GAMMA, batch_size = BATCH_SIZE):\n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        \n",
    "        # actor\n",
    "        self.actor = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # critic\n",
    "        self.critic = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr = critic_lr, weight_decay=1e-2)\n",
    "        \n",
    "        # critic loss\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        # noise\n",
    "        self.noise = OrnsteinUhlenbeckProcess(dimension = act_dim, num_steps = NUM_EPISODES)\n",
    "\n",
    "        # replay buffer \n",
    "        self.replayBuffer = Replay()\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        # sample from Replay\n",
    "        b_st, b_at, b_st1, b_r, b_d = self.replayBuffer.generateMinibatch(self.batch_size)\n",
    "\n",
    "        ## update critic (create target for Q function)       \n",
    "        # below is for target actor network\n",
    "        targetActorPredict_b_at1 = self.actor_target(b_st1)\n",
    "        \n",
    "        #below is for target critic network\n",
    "        mask = 1 - b_d   # if done is true, change the target to just reward\n",
    "        batch_Q_next = self.critic_target(b_st1, targetActorPredict_b_at1)\n",
    "        QQ_next = Variable((batch_Q_next.data * mask).view(self.batch_size, 1))\n",
    "        b_Q_critic_target = b_r + self.gamma*(QQ_next)\n",
    "        \n",
    "        \n",
    "        # below is for behavior critic network\n",
    "        b_Q_critic_behaviorQ = self.critic(b_st, b_at)\n",
    "        \n",
    "        ## critic optimizer and backprop step (feed in target and predicted values to self.critic_loss)\n",
    "        critic_loss = self.critic_loss(b_Q_critic_behaviorQ, b_Q_critic_target.detach())\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        \n",
    "        ## update actor (formulate the loss wrt which actor is updated)\n",
    "        # below is for behavior actor network\n",
    "        b_at_actor_behavior = self.actor(b_st)\n",
    "        # below is for behavior critic network\n",
    "        b_Q_critic_behaviorP = self.critic(b_st, b_at_actor_behavior)\n",
    "\n",
    "        ## actor optimizer and backprop step (loss_actor.backward())\n",
    "        loss_actor = -1. * b_Q_critic_behaviorP\n",
    "        loss_actor = loss_actor.mean()\n",
    "        \n",
    "        self.optimizer_actor.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        # sychronize target network with fast moving one\n",
    "        weightSync(self.critic_target, self.critic)\n",
    "        weightSync(self.actor_target, self.actor)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of your DDPG object\n",
    "- Print network architectures, confirm they are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor(\n",
      "  (fc1): Linear(in_features=4, out_features=400, bias=True)\n",
      "  (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (bn2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=400, out_features=1, bias=True)\n",
      ")\n",
      "critic(\n",
      "  (fc1): Linear(in_features=4, out_features=300, bias=True)\n",
      "  (bn1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=301, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ddpg = DDPG(obs_dim = obs_dim, act_dim = act_dim)\n",
    "print(ddpg.actor)\n",
    "print(ddpg.critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DDPG on different environments\n",
    "Early stopping conditions:\n",
    "- avg_val > 500 for \"InvertedPendulum\" \n",
    "- avg_val > -150 for \"Pendulum\" \n",
    "- avg_val > 1500 for \"HalfCheetah\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:25:59,024] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000000.mp4\n",
      "[2018-05-14 15:25:59,026] GLFW error: 65544, desc: b'X11: RandR gamma ramp support seems broken'\n",
      "[2018-05-14 15:25:59,086] GLFW error: 65544, desc: b'Linux: Failed to watch for joystick connections in /dev/input: No such file or directory'\n",
      "[2018-05-14 15:25:59,088] GLFW error: 65544, desc: b'Linux: Failed to open joystick device directory /dev/input: No such file or directory'\n",
      "[2018-05-14 15:25:59,576] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000100.mp4\n",
      "[2018-05-14 15:25:59,902] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000200.mp4\n",
      "[2018-05-14 15:26:00,314] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000300.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 0.1 for episode: 0\n",
      "Average value: 0.195 for episode: 1\n",
      "Average value: 0.28525 for episode: 2\n",
      "Average value: 0.3709875 for episode: 3\n",
      "Average value: 0.452438125 for episode: 4\n",
      "Average value: 0.52981621875 for episode: 5\n",
      "Average value: 0.6033254078124999 for episode: 6\n",
      "Average value: 0.6731591374218748 for episode: 7\n",
      "Average value: 0.7395011805507811 for episode: 8\n",
      "Average value: 0.802526121523242 for episode: 9\n",
      "Average value: 0.9123998154470798 for episode: 10\n",
      "Average value: 0.9667798246747258 for episode: 11\n",
      "Average value: 1.0684408334409894 for episode: 12\n",
      "Average value: 1.11501879176894 for episode: 13\n",
      "Average value: 1.159267852180493 for episode: 14\n",
      "Average value: 1.2513044595714686 for episode: 15\n",
      "Average value: 1.2887392365928951 for episode: 16\n",
      "Average value: 1.3243022747632505 for episode: 17\n",
      "Average value: 1.408087161025088 for episode: 18\n",
      "Average value: 1.4876828029738336 for episode: 19\n",
      "Average value: 1.563298662825142 for episode: 20\n",
      "Average value: 1.6351337296838846 for episode: 21\n",
      "Average value: 1.6533770431996904 for episode: 22\n",
      "Average value: 1.720708191039706 for episode: 23\n",
      "Average value: 1.7346727814877207 for episode: 24\n",
      "Average value: 1.7979391424133349 for episode: 25\n",
      "Average value: 1.858042185292668 for episode: 26\n",
      "Average value: 1.8651400760280348 for episode: 27\n",
      "Average value: 1.921883072226633 for episode: 28\n",
      "Average value: 1.9257889186153012 for episode: 29\n",
      "Average value: 1.9294994726845363 for episode: 30\n",
      "Average value: 1.9830244990503094 for episode: 31\n",
      "Average value: 2.033873274097794 for episode: 32\n",
      "Average value: 2.0821796103929042 for episode: 33\n",
      "Average value: 2.0780706298732587 for episode: 34\n",
      "Average value: 2.0741670983795957 for episode: 35\n",
      "Average value: 2.0704587434606156 for episode: 36\n",
      "Average value: 2.116935806287585 for episode: 37\n",
      "Average value: 2.1110890159732056 for episode: 38\n",
      "Average value: 2.1055345651745454 for episode: 39\n",
      "Average value: 2.100257836915818 for episode: 40\n",
      "Average value: 2.095244945070027 for episode: 41\n",
      "Average value: 2.090482697816525 for episode: 42\n",
      "Average value: 2.135958562925699 for episode: 43\n",
      "Average value: 2.179160634779414 for episode: 44\n",
      "Average value: 2.220202603040443 for episode: 45\n",
      "Average value: 2.2091924728884207 for episode: 46\n",
      "Average value: 2.2487328492439995 for episode: 47\n",
      "Average value: 2.2362962067817995 for episode: 48\n",
      "Average value: 2.2244813964427097 for episode: 49\n",
      "Average value: 2.213257326620574 for episode: 50\n",
      "Average value: 2.2525944602895454 for episode: 51\n",
      "Average value: 2.289964737275068 for episode: 52\n",
      "Average value: 2.2754665004113144 for episode: 53\n",
      "Average value: 2.2616931753907488 for episode: 54\n",
      "Average value: 2.2486085166212115 for episode: 55\n",
      "Average value: 2.2861780907901506 for episode: 56\n",
      "Average value: 2.271869186250643 for episode: 57\n",
      "Average value: 2.3082757269381107 for episode: 58\n",
      "Average value: 2.292861940591205 for episode: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:02,172] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000400.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 2.278218843561645 for episode: 60\n",
      "Average value: 2.2643079013835625 for episode: 61\n",
      "Average value: 2.2510925063143845 for episode: 62\n",
      "Average value: 2.288537880998665 for episode: 63\n",
      "Average value: 2.3241109869487313 for episode: 64\n",
      "Average value: 2.3579054376012945 for episode: 65\n",
      "Average value: 2.3900101657212294 for episode: 66\n",
      "Average value: 2.370509657435168 for episode: 67\n",
      "Average value: 2.401984174563409 for episode: 68\n",
      "Average value: 2.4318849658352386 for episode: 69\n",
      "Average value: 2.4102907175434765 for episode: 70\n",
      "Average value: 2.389776181666303 for episode: 71\n",
      "Average value: 2.4202873725829877 for episode: 72\n",
      "Average value: 2.449273003953838 for episode: 73\n",
      "Average value: 2.476809353756146 for episode: 74\n",
      "Average value: 2.5029688860683383 for episode: 75\n",
      "Average value: 2.5278204417649213 for episode: 76\n",
      "Average value: 2.551429419676675 for episode: 77\n",
      "Average value: 2.523857948692841 for episode: 78\n",
      "Average value: 2.547665051258199 for episode: 79\n",
      "Average value: 2.520281798695289 for episode: 80\n",
      "Average value: 2.4942677087605243 for episode: 81\n",
      "Average value: 2.519554323322498 for episode: 82\n",
      "Average value: 2.493576607156373 for episode: 83\n",
      "Average value: 2.5188977767985543 for episode: 84\n",
      "Average value: 2.4929528879586265 for episode: 85\n",
      "Average value: 2.468305243560695 for episode: 86\n",
      "Average value: 2.4448899813826603 for episode: 87\n",
      "Average value: 2.4726454823135273 for episode: 88\n",
      "Average value: 2.449013208197851 for episode: 89\n",
      "Average value: 2.476562547787958 for episode: 90\n",
      "Average value: 2.5027344203985598 for episode: 91\n",
      "Average value: 2.4775976993786317 for episode: 92\n",
      "Average value: 2.4537178144097003 for episode: 93\n",
      "Average value: 2.481031923689215 for episode: 94\n",
      "Average value: 2.506980327504754 for episode: 95\n",
      "Average value: 2.481631311129516 for episode: 96\n",
      "Average value: 2.5075497455730402 for episode: 97\n",
      "Average value: 2.532172258294388 for episode: 98\n",
      "Average value: 2.5555636453796686 for episode: 99\n",
      "Average value: 2.527785463110685 for episode: 100\n",
      "Average value: 2.5513961899551507 for episode: 101\n",
      "Average value: 2.573826380457393 for episode: 102\n",
      "Average value: 2.5451350614345234 for episode: 103\n",
      "Average value: 2.567878308362797 for episode: 104\n",
      "Average value: 2.539484392944657 for episode: 105\n",
      "Average value: 2.562510173297424 for episode: 106\n",
      "Average value: 2.5843846646325526 for episode: 107\n",
      "Average value: 2.555165431400925 for episode: 108\n",
      "Average value: 2.5274071598308785 for episode: 109\n",
      "Average value: 2.5510368018393343 for episode: 110\n",
      "Average value: 2.5734849617473676 for episode: 111\n",
      "Average value: 2.594810713659999 for episode: 112\n",
      "Average value: 2.565070177976999 for episode: 113\n",
      "Average value: 2.5868166690781487 for episode: 114\n",
      "Average value: 2.557475835624241 for episode: 115\n",
      "Average value: 2.529602043843029 for episode: 116\n",
      "Average value: 2.5031219416508774 for episode: 117\n",
      "Average value: 2.4779658445683337 for episode: 118\n",
      "Average value: 2.504067552339917 for episode: 119\n",
      "Average value: 2.478864174722921 for episode: 120\n",
      "Average value: 2.504920965986775 for episode: 121\n",
      "Average value: 2.479674917687436 for episode: 122\n",
      "Average value: 2.505691171803064 for episode: 123\n",
      "Average value: 2.5304066132129104 for episode: 124\n",
      "Average value: 2.5538862825522646 for episode: 125\n",
      "Average value: 2.5761919684246513 for episode: 126\n",
      "Average value: 2.547382370003419 for episode: 127\n",
      "Average value: 2.570013251503248 for episode: 128\n",
      "Average value: 2.5915125889280852 for episode: 129\n",
      "Average value: 2.611936959481681 for episode: 130\n",
      "Average value: 2.581340111507597 for episode: 131\n",
      "Average value: 2.602273105932217 for episode: 132\n",
      "Average value: 2.622159450635606 for episode: 133\n",
      "Average value: 2.6410514781038255 for episode: 134\n",
      "Average value: 2.6089989041986343 for episode: 135\n",
      "Average value: 2.5785489589887027 for episode: 136\n",
      "Average value: 2.5996215110392673 for episode: 137\n",
      "Average value: 2.6196404354873035 for episode: 138\n",
      "Average value: 2.5886584137129383 for episode: 139\n",
      "Average value: 2.5592254930272915 for episode: 140\n",
      "Average value: 2.5812642183759267 for episode: 141\n",
      "Average value: 2.60220100745713 for episode: 142\n",
      "Average value: 2.622090957084273 for episode: 143\n",
      "Average value: 2.5909864092300596 for episode: 144\n",
      "Average value: 2.5614370887685567 for episode: 145\n",
      "Average value: 2.5833652343301288 for episode: 146\n",
      "Average value: 2.604196972613622 for episode: 147\n",
      "Average value: 2.623987123982941 for episode: 148\n",
      "Average value: 2.6427877677837937 for episode: 149\n",
      "Average value: 2.610648379394604 for episode: 150\n",
      "Average value: 2.580115960424874 for episode: 151\n",
      "Average value: 2.60111016240363 for episode: 152\n",
      "Average value: 2.6210546542834483 for episode: 153\n",
      "Average value: 2.640001921569276 for episode: 154\n",
      "Average value: 2.608001825490812 for episode: 155\n",
      "Average value: 2.627601734216271 for episode: 156\n",
      "Average value: 2.6462216475054574 for episode: 157\n",
      "Average value: 2.6139105651301846 for episode: 158\n",
      "Average value: 2.5832150368736753 for episode: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:04,599] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000500.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 2.6040542850299913 for episode: 160\n",
      "Average value: 2.6238515707784913 for episode: 161\n",
      "Average value: 2.5926589922395666 for episode: 162\n",
      "Average value: 2.613026042627588 for episode: 163\n",
      "Average value: 2.632374740496209 for episode: 164\n",
      "Average value: 2.6007560034713983 for episode: 165\n",
      "Average value: 2.570718203297828 for episode: 166\n",
      "Average value: 2.5421822931329365 for episode: 167\n",
      "Average value: 2.51507317847629 for episode: 168\n",
      "Average value: 2.4893195195524753 for episode: 169\n",
      "Average value: 2.4648535435748515 for episode: 170\n",
      "Average value: 2.4916108663961087 for episode: 171\n",
      "Average value: 2.517030323076303 for episode: 172\n",
      "Average value: 2.491178806922488 for episode: 173\n",
      "Average value: 2.4666198665763637 for episode: 174\n",
      "Average value: 2.4432888732475453 for episode: 175\n",
      "Average value: 2.471124429585168 for episode: 176\n",
      "Average value: 2.497568208105909 for episode: 177\n",
      "Average value: 2.5226897977006137 for episode: 178\n",
      "Average value: 2.546555307815583 for episode: 179\n",
      "Average value: 2.5192275424248036 for episode: 180\n",
      "Average value: 2.4932661653035635 for episode: 181\n",
      "Average value: 2.4686028570383853 for episode: 182\n",
      "Average value: 2.4951727141864657 for episode: 183\n",
      "Average value: 2.5204140784771423 for episode: 184\n",
      "Average value: 2.5443933745532847 for episode: 185\n",
      "Average value: 2.56717370582562 for episode: 186\n",
      "Average value: 2.588815020534339 for episode: 187\n",
      "Average value: 2.609374269507622 for episode: 188\n",
      "Average value: 2.578905556032241 for episode: 189\n",
      "Average value: 2.5999602782306286 for episode: 190\n",
      "Average value: 2.569962264319097 for episode: 191\n",
      "Average value: 2.591464151103142 for episode: 192\n",
      "Average value: 2.611890943547985 for episode: 193\n",
      "Average value: 2.5812963963705857 for episode: 194\n",
      "Average value: 2.602231576552056 for episode: 195\n",
      "Average value: 2.572119997724453 for episode: 196\n",
      "Average value: 2.5935139978382304 for episode: 197\n",
      "Average value: 2.6138382979463186 for episode: 198\n",
      "Average value: 2.5831463830490025 for episode: 199\n",
      "Average value: 2.603989063896552 for episode: 200\n",
      "Average value: 2.6237896107017242 for episode: 201\n",
      "Average value: 2.592600130166638 for episode: 202\n",
      "Average value: 2.562970123658306 for episode: 203\n",
      "Average value: 2.58482161747539 for episode: 204\n",
      "Average value: 2.5555805366016204 for episode: 205\n",
      "Average value: 2.5278015097715394 for episode: 206\n",
      "Average value: 2.5514114342829624 for episode: 207\n",
      "Average value: 2.573840862568814 for episode: 208\n",
      "Average value: 2.545148819440373 for episode: 209\n",
      "Average value: 2.5678913784683544 for episode: 210\n",
      "Average value: 2.5394968095449366 for episode: 211\n",
      "Average value: 2.5625219690676895 for episode: 212\n",
      "Average value: 2.584395870614305 for episode: 213\n",
      "Average value: 2.5551760770835896 for episode: 214\n",
      "Average value: 2.57741727322941 for episode: 215\n",
      "Average value: 2.598546409567939 for episode: 216\n",
      "Average value: 2.6186190890895418 for episode: 217\n",
      "Average value: 2.587688134635065 for episode: 218\n",
      "Average value: 2.5583037279033114 for episode: 219\n",
      "Average value: 2.5803885415081456 for episode: 220\n",
      "Average value: 2.601369114432738 for episode: 221\n",
      "Average value: 2.571300658711101 for episode: 222\n",
      "Average value: 2.592735625775546 for episode: 223\n",
      "Average value: 2.5630988444867686 for episode: 224\n",
      "Average value: 2.5349439022624303 for episode: 225\n",
      "Average value: 2.5081967071493088 for episode: 226\n",
      "Average value: 2.532786871791843 for episode: 227\n",
      "Average value: 2.5061475282022507 for episode: 228\n",
      "Average value: 2.530840151792138 for episode: 229\n",
      "Average value: 2.504298144202531 for episode: 230\n",
      "Average value: 2.5290832369924043 for episode: 231\n",
      "Average value: 2.552629075142784 for episode: 232\n",
      "Average value: 2.5249976213856447 for episode: 233\n",
      "Average value: 2.548747740316362 for episode: 234\n",
      "Average value: 2.571310353300544 for episode: 235\n",
      "Average value: 2.5427448356355167 for episode: 236\n",
      "Average value: 2.565607593853741 for episode: 237\n",
      "Average value: 2.5873272141610535 for episode: 238\n",
      "Average value: 2.6079608534530005 for episode: 239\n",
      "Average value: 2.62756281078035 for episode: 240\n",
      "Average value: 2.6461846702413325 for episode: 241\n",
      "Average value: 2.613875436729266 for episode: 242\n",
      "Average value: 2.5831816648928023 for episode: 243\n",
      "Average value: 2.5540225816481623 for episode: 244\n",
      "Average value: 2.526321452565754 for episode: 245\n",
      "Average value: 2.500005379937466 for episode: 246\n",
      "Average value: 2.525005110940593 for episode: 247\n",
      "Average value: 2.548754855393563 for episode: 248\n",
      "Average value: 2.5213171126238847 for episode: 249\n",
      "Average value: 2.54525125699269 for episode: 250\n",
      "Average value: 2.5679886941430556 for episode: 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:06,897] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000600.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 2.5895892594359027 for episode: 252\n",
      "Average value: 2.5601097964641073 for episode: 253\n",
      "Average value: 2.532104306640902 for episode: 254\n",
      "Average value: 2.5554990913088567 for episode: 255\n",
      "Average value: 2.527724136743414 for episode: 256\n",
      "Average value: 2.5013379299062435 for episode: 257\n",
      "Average value: 2.5262710334109313 for episode: 258\n",
      "Average value: 2.4999574817403847 for episode: 259\n",
      "Average value: 2.4749596076533655 for episode: 260\n",
      "Average value: 2.5012116272706972 for episode: 261\n",
      "Average value: 2.526151045907162 for episode: 262\n",
      "Average value: 2.499843493611804 for episode: 263\n",
      "Average value: 2.524851318931214 for episode: 264\n",
      "Average value: 2.548608752984653 for episode: 265\n",
      "Average value: 2.57117831533542 for episode: 266\n",
      "Average value: 2.542619399568649 for episode: 267\n",
      "Average value: 2.5654884295902165 for episode: 268\n",
      "Average value: 2.5372140081107055 for episode: 269\n",
      "Average value: 2.51035330770517 for episode: 270\n",
      "Average value: 2.4848356423199114 for episode: 271\n",
      "Average value: 2.5105938602039157 for episode: 272\n",
      "Average value: 2.4850641671937197 for episode: 273\n",
      "Average value: 2.460810958834034 for episode: 274\n",
      "Average value: 2.487770410892332 for episode: 275\n",
      "Average value: 2.463381890347715 for episode: 276\n",
      "Average value: 2.4902127958303293 for episode: 277\n",
      "Average value: 2.5157021560388126 for episode: 278\n",
      "Average value: 2.489917048236872 for episode: 279\n",
      "Average value: 2.4654211958250283 for episode: 280\n",
      "Average value: 2.442150136033777 for episode: 281\n",
      "Average value: 2.470042629232088 for episode: 282\n",
      "Average value: 2.4965404977704835 for episode: 283\n",
      "Average value: 2.521713472881959 for episode: 284\n",
      "Average value: 2.495627799237861 for episode: 285\n",
      "Average value: 2.520846409275968 for episode: 286\n",
      "Average value: 2.5448040888121692 for episode: 287\n",
      "Average value: 2.5675638843715607 for episode: 288\n",
      "Average value: 2.5391856901529826 for episode: 289\n",
      "Average value: 2.562226405645333 for episode: 290\n",
      "Average value: 2.5341150853630667 for episode: 291\n",
      "Average value: 2.5074093310949133 for episode: 292\n",
      "Average value: 2.5320388645401675 for episode: 293\n",
      "Average value: 2.555436921313159 for episode: 294\n",
      "Average value: 2.5776650752475008 for episode: 295\n",
      "Average value: 2.5987818214851255 for episode: 296\n",
      "Average value: 2.568842730410869 for episode: 297\n",
      "Average value: 2.540400593890326 for episode: 298\n",
      "Average value: 2.5133805641958094 for episode: 299\n",
      "Average value: 2.4877115359860187 for episode: 300\n",
      "Average value: 2.5133259591867176 for episode: 301\n",
      "Average value: 2.4876596612273816 for episode: 302\n",
      "Average value: 2.4632766781660127 for episode: 303\n",
      "Average value: 2.490112844257712 for episode: 304\n",
      "Average value: 2.4656072020448265 for episode: 305\n",
      "Average value: 2.492326841942585 for episode: 306\n",
      "Average value: 2.4677104998454555 for episode: 307\n",
      "Average value: 2.4943249748531824 for episode: 308\n",
      "Average value: 2.519608726110523 for episode: 309\n",
      "Average value: 2.493628289804997 for episode: 310\n",
      "Average value: 2.518946875314747 for episode: 311\n",
      "Average value: 2.4929995315490094 for episode: 312\n",
      "Average value: 2.5183495549715587 for episode: 313\n",
      "Average value: 2.5424320772229807 for episode: 314\n",
      "Average value: 2.5653104733618313 for episode: 315\n",
      "Average value: 2.5870449496937393 for episode: 316\n",
      "Average value: 2.5576927022090525 for episode: 317\n",
      "Average value: 2.5298080670986 for episode: 318\n",
      "Average value: 2.50331766374367 for episode: 319\n",
      "Average value: 2.478151780556486 for episode: 320\n",
      "Average value: 2.5042441915286617 for episode: 321\n",
      "Average value: 2.4790319819522284 for episode: 322\n",
      "Average value: 2.505080382854617 for episode: 323\n",
      "Average value: 2.479826363711886 for episode: 324\n",
      "Average value: 2.5058350455262914 for episode: 325\n",
      "Average value: 2.5305432932499765 for episode: 326\n",
      "Average value: 2.5540161285874774 for episode: 327\n",
      "Average value: 2.5263153221581036 for episode: 328\n",
      "Average value: 2.549999556050198 for episode: 329\n",
      "Average value: 2.5224995782476882 for episode: 330\n",
      "Average value: 2.496374599335304 for episode: 331\n",
      "Average value: 2.5215558693685387 for episode: 332\n",
      "Average value: 2.495478075900112 for episode: 333\n",
      "Average value: 2.470704172105106 for episode: 334\n",
      "Average value: 2.447168963499851 for episode: 335\n",
      "Average value: 2.4248105153248583 for episode: 336\n",
      "Average value: 2.453569989558615 for episode: 337\n",
      "Average value: 2.480891490080684 for episode: 338\n",
      "Average value: 2.50684691557665 for episode: 339\n",
      "Average value: 2.5315045697978174 for episode: 340\n",
      "Average value: 2.5049293413079265 for episode: 341\n",
      "Average value: 2.4796828742425303 for episode: 342\n",
      "Average value: 2.5056987305304035 for episode: 343\n",
      "Average value: 2.480413794003883 for episode: 344\n",
      "Average value: 2.506393104303689 for episode: 345\n",
      "Average value: 2.5310734490885043 for episode: 346\n",
      "Average value: 2.504519776634079 for episode: 347\n",
      "Average value: 2.529293787802375 for episode: 348\n",
      "Average value: 2.552829098412256 for episode: 349\n",
      "Average value: 2.5251876434916434 for episode: 350\n",
      "Average value: 2.548928261317061 for episode: 351\n",
      "Average value: 2.571481848251208 for episode: 352\n",
      "Average value: 2.592907755838647 for episode: 353\n",
      "Average value: 2.6132623680467146 for episode: 354\n",
      "Average value: 2.632599249644379 for episode: 355\n",
      "Average value: 2.6509692871621597 for episode: 356\n",
      "Average value: 2.6184208228040515 for episode: 357\n",
      "Average value: 2.637499781663849 for episode: 358\n",
      "Average value: 2.655624792580656 for episode: 359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:09,295] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000700.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 2.672843552951623 for episode: 360\n",
      "Average value: 2.689201375304042 for episode: 361\n",
      "Average value: 2.7047413065388395 for episode: 362\n",
      "Average value: 2.7195042412118973 for episode: 363\n",
      "Average value: 2.6835290291513023 for episode: 364\n",
      "Average value: 2.649352577693737 for episode: 365\n",
      "Average value: 2.66688494880905 for episode: 366\n",
      "Average value: 2.6335407013685974 for episode: 367\n",
      "Average value: 2.651863666300167 for episode: 368\n",
      "Average value: 2.6692704829851586 for episode: 369\n",
      "Average value: 2.6358069588359005 for episode: 370\n",
      "Average value: 2.6540166108941055 for episode: 371\n",
      "Average value: 2.6213157803494003 for episode: 372\n",
      "Average value: 2.5902499913319303 for episode: 373\n",
      "Average value: 2.560737491765334 for episode: 374\n",
      "Average value: 2.532700617177067 for episode: 375\n",
      "Average value: 2.5560655863182133 for episode: 376\n",
      "Average value: 2.5782623070023023 for episode: 377\n",
      "Average value: 2.5493491916521873 for episode: 378\n",
      "Average value: 2.521881732069578 for episode: 379\n",
      "Average value: 2.4957876454660988 for episode: 380\n",
      "Average value: 2.470998263192794 for episode: 381\n",
      "Average value: 2.4974483500331544 for episode: 382\n",
      "Average value: 2.4725759325314964 for episode: 383\n",
      "Average value: 2.4489471359049215 for episode: 384\n",
      "Average value: 2.4264997791096756 for episode: 385\n",
      "Average value: 2.405174790154192 for episode: 386\n",
      "Average value: 2.4349160506464824 for episode: 387\n",
      "Average value: 2.463170248114158 for episode: 388\n",
      "Average value: 2.4900117357084497 for episode: 389\n",
      "Average value: 2.515511148923027 for episode: 390\n",
      "Average value: 2.4897355914768755 for episode: 391\n",
      "Average value: 2.5152488119030316 for episode: 392\n",
      "Average value: 2.5394863713078797 for episode: 393\n",
      "Average value: 2.5125120527424856 for episode: 394\n",
      "Average value: 2.536886450105361 for episode: 395\n",
      "Average value: 2.510042127600093 for episode: 396\n",
      "Average value: 2.534540021220088 for episode: 397\n",
      "Average value: 2.507813020159084 for episode: 398\n",
      "Average value: 2.5324223691511296 for episode: 399\n",
      "Average value: 2.555801250693573 for episode: 400\n",
      "Average value: 2.528011188158894 for episode: 401\n",
      "Average value: 2.5516106287509492 for episode: 402\n",
      "Average value: 2.5240300973134016 for episode: 403\n",
      "Average value: 2.4978285924477315 for episode: 404\n",
      "Average value: 2.5229371628253445 for episode: 405\n",
      "Average value: 2.546790304684077 for episode: 406\n",
      "Average value: 2.5194507894498734 for episode: 407\n",
      "Average value: 2.5434782499773796 for episode: 408\n",
      "Average value: 2.5663043374785106 for episode: 409\n",
      "Average value: 2.537989120604585 for episode: 410\n",
      "Average value: 2.5610896645743555 for episode: 411\n",
      "Average value: 2.5330351813456375 for episode: 412\n",
      "Average value: 2.5063834222783554 for episode: 413\n",
      "Average value: 2.4810642511644376 for episode: 414\n",
      "Average value: 2.5070110386062154 for episode: 415\n",
      "Average value: 2.5316604866759045 for episode: 416\n",
      "Average value: 2.505077462342109 for episode: 417\n",
      "Average value: 2.5298235892250034 for episode: 418\n",
      "Average value: 2.553332409763753 for episode: 419\n",
      "Average value: 2.575665789275565 for episode: 420\n",
      "Average value: 2.5468824998117867 for episode: 421\n",
      "Average value: 2.5695383748211973 for episode: 422\n",
      "Average value: 2.5410614560801372 for episode: 423\n",
      "Average value: 2.5640083832761302 for episode: 424\n",
      "Average value: 2.535807964112324 for episode: 425\n",
      "Average value: 2.5090175659067078 for episode: 426\n",
      "Average value: 2.5335666876113723 for episode: 427\n",
      "Average value: 2.5568883532308035 for episode: 428\n",
      "Average value: 2.5790439355692634 for episode: 429\n",
      "Average value: 2.6000917387908 for episode: 430\n",
      "Average value: 2.62008715185126 for episode: 431\n",
      "Average value: 2.6390827942586967 for episode: 432\n",
      "Average value: 2.607128654545762 for episode: 433\n",
      "Average value: 2.576772221818474 for episode: 434\n",
      "Average value: 2.59793361072755 for episode: 435\n",
      "Average value: 2.6180369301911726 for episode: 436\n",
      "Average value: 2.587135083681614 for episode: 437\n",
      "Average value: 2.557778329497533 for episode: 438\n",
      "Average value: 2.5298894130226564 for episode: 439\n",
      "Average value: 2.5033949423715236 for episode: 440\n",
      "Average value: 2.4782251952529473 for episode: 441\n",
      "Average value: 2.4543139354903 for episode: 442\n",
      "Average value: 2.431598238715785 for episode: 443\n",
      "Average value: 2.410018326779996 for episode: 444\n",
      "Average value: 2.439517410440996 for episode: 445\n",
      "Average value: 2.417541539918946 for episode: 446\n",
      "Average value: 2.4466644629229983 for episode: 447\n",
      "Average value: 2.4743312397768484 for episode: 448\n",
      "Average value: 2.450614677788006 for episode: 449\n",
      "Average value: 2.4780839438986053 for episode: 450\n",
      "Average value: 2.4541797467036752 for episode: 451\n",
      "Average value: 2.4814707593684915 for episode: 452\n",
      "Average value: 2.507397221400067 for episode: 453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:11,109] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000800.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 2.5320273603300634 for episode: 454\n",
      "Average value: 2.55542599231356 for episode: 455\n",
      "Average value: 2.527654692697882 for episode: 456\n",
      "Average value: 2.5012719580629876 for episode: 457\n",
      "Average value: 2.476208360159838 for episode: 458\n",
      "Average value: 2.4523979421518463 for episode: 459\n",
      "Average value: 2.429778045044254 for episode: 460\n",
      "Average value: 2.408289142792041 for episode: 461\n",
      "Average value: 2.3878746856524393 for episode: 462\n",
      "Average value: 2.3684809513698175 for episode: 463\n",
      "Average value: 2.3500569038013266 for episode: 464\n",
      "Average value: 2.38255405861126 for episode: 465\n",
      "Average value: 2.363426355680697 for episode: 466\n",
      "Average value: 2.345255037896662 for episode: 467\n",
      "Average value: 2.3779922860018288 for episode: 468\n",
      "Average value: 2.3590926717017373 for episode: 469\n",
      "Average value: 2.39113803811665 for episode: 470\n",
      "Average value: 2.4215811362108175 for episode: 471\n",
      "Average value: 2.4005020794002765 for episode: 472\n",
      "Average value: 2.4304769754302624 for episode: 473\n",
      "Average value: 2.408953126658749 for episode: 474\n",
      "Average value: 2.3885054703258115 for episode: 475\n",
      "Average value: 2.419080196809521 for episode: 476\n",
      "Average value: 2.4481261869690445 for episode: 477\n",
      "Average value: 2.425719877620592 for episode: 478\n",
      "Average value: 2.4544338837395623 for episode: 479\n",
      "Average value: 2.431712189552584 for episode: 480\n",
      "Average value: 2.4101265800749547 for episode: 481\n",
      "Average value: 2.389620251071207 for episode: 482\n",
      "Average value: 2.4201392385176463 for episode: 483\n",
      "Average value: 2.399132276591764 for episode: 484\n",
      "Average value: 2.4291756627621757 for episode: 485\n",
      "Average value: 2.4077168796240667 for episode: 486\n",
      "Average value: 2.3873310356428634 for episode: 487\n",
      "Average value: 2.36796448386072 for episode: 488\n",
      "Average value: 2.399566259667684 for episode: 489\n",
      "Average value: 2.3795879466842997 for episode: 490\n",
      "Average value: 2.3606085493500846 for episode: 491\n",
      "Average value: 2.3925781218825803 for episode: 492\n",
      "Average value: 2.422949215788451 for episode: 493\n",
      "Average value: 2.4018017549990285 for episode: 494\n",
      "Average value: 2.381711667249077 for episode: 495\n",
      "Average value: 2.362626083886623 for episode: 496\n",
      "Average value: 2.394494779692292 for episode: 497\n",
      "Average value: 2.4247700407076773 for episode: 498\n",
      "Average value: 2.4035315386722935 for episode: 499\n",
      "Average value: 2.4333549617386785 for episode: 500\n",
      "Average value: 2.4616872136517443 for episode: 501\n",
      "Average value: 2.488602852969157 for episode: 502\n",
      "Average value: 2.464172710320699 for episode: 503\n",
      "Average value: 2.490964074804664 for episode: 504\n",
      "Average value: 2.466415871064431 for episode: 505\n",
      "Average value: 2.4430950775112095 for episode: 506\n",
      "Average value: 2.4709403236356486 for episode: 507\n",
      "Average value: 2.497393307453866 for episode: 508\n",
      "Average value: 2.472523642081173 for episode: 509\n",
      "Average value: 2.498897459977114 for episode: 510\n",
      "Average value: 2.5239525869782584 for episode: 511\n",
      "Average value: 2.5477549576293455 for episode: 512\n",
      "Average value: 2.520367209747878 for episode: 513\n",
      "Average value: 2.544348849260484 for episode: 514\n",
      "Average value: 2.5671314067974595 for episode: 515\n",
      "Average value: 2.588774836457586 for episode: 516\n",
      "Average value: 2.6093360946347066 for episode: 517\n",
      "Average value: 2.628869289902971 for episode: 518\n",
      "Average value: 2.6474258254078222 for episode: 519\n",
      "Average value: 2.615054534137431 for episode: 520\n",
      "Average value: 2.634301807430559 for episode: 521\n",
      "Average value: 2.602586717059031 for episode: 522\n",
      "Average value: 2.622457381206079 for episode: 523\n",
      "Average value: 2.6413345121457747 for episode: 524\n",
      "Average value: 2.6592677865384857 for episode: 525\n",
      "Average value: 2.6263043972115616 for episode: 526\n",
      "Average value: 2.5949891773509837 for episode: 527\n",
      "Average value: 2.5652397184834346 for episode: 528\n",
      "Average value: 2.536977732559263 for episode: 529\n",
      "Average value: 2.5601288459312994 for episode: 530\n",
      "Average value: 2.5321224036347343 for episode: 531\n",
      "Average value: 2.5055162834529976 for episode: 532\n",
      "Average value: 2.5302404692803475 for episode: 533\n",
      "Average value: 2.5037284458163303 for episode: 534\n",
      "Average value: 2.4785420235255136 for episode: 535\n",
      "Average value: 2.4546149223492377 for episode: 536\n",
      "Average value: 2.481884176231776 for episode: 537\n",
      "Average value: 2.457789967420187 for episode: 538\n",
      "Average value: 2.4849004690491774 for episode: 539\n",
      "Average value: 2.5106554455967185 for episode: 540\n",
      "Average value: 2.4851226733168827 for episode: 541\n",
      "Average value: 2.4608665396510387 for episode: 542\n",
      "Average value: 2.4878232126684865 for episode: 543\n",
      "Average value: 2.463432052035062 for episode: 544\n",
      "Average value: 2.440260449433309 for episode: 545\n",
      "Average value: 2.418247426961644 for episode: 546\n",
      "Average value: 2.4473350556135616 for episode: 547\n",
      "Average value: 2.4249683028328834 for episode: 548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:13,098] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video000900.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 2.4037198876912393 for episode: 549\n",
      "Average value: 2.3835338933066774 for episode: 550\n",
      "Average value: 2.3643571986413434 for episode: 551\n",
      "Average value: 2.346139338709276 for episode: 552\n",
      "Average value: 2.3788323717738122 for episode: 553\n",
      "Average value: 2.3598907531851214 for episode: 554\n",
      "Average value: 2.3418962155258654 for episode: 555\n",
      "Average value: 2.374801404749572 for episode: 556\n",
      "Average value: 2.406061334512093 for episode: 557\n",
      "Average value: 2.435758267786488 for episode: 558\n",
      "Average value: 2.4139703543971636 for episode: 559\n",
      "Average value: 2.4432718366773054 for episode: 560\n",
      "Average value: 2.42110824484344 for episode: 561\n",
      "Average value: 2.450052832601268 for episode: 562\n",
      "Average value: 2.477550190971204 for episode: 563\n",
      "Average value: 2.4536726814226437 for episode: 564\n",
      "Average value: 2.5309890473515115 for episode: 565\n",
      "Average value: 2.504439594983936 for episode: 566\n",
      "Average value: 2.479217615234739 for episode: 567\n",
      "Average value: 2.455256734473002 for episode: 568\n",
      "Average value: 2.4824938977493516 for episode: 569\n",
      "Average value: 2.508369202861884 for episode: 570\n",
      "Average value: 2.5329507427187896 for episode: 571\n",
      "Average value: 2.55630320558285 for episode: 572\n",
      "Average value: 2.5784880453037076 for episode: 573\n",
      "Average value: 2.549563643038522 for episode: 574\n",
      "Average value: 2.5720854608865955 for episode: 575\n",
      "Average value: 2.5434811878422656 for episode: 576\n",
      "Average value: 2.516307128450152 for episode: 577\n",
      "Average value: 2.4904917720276445 for episode: 578\n",
      "Average value: 2.515967183426262 for episode: 579\n",
      "Average value: 2.5401688242549487 for episode: 580\n",
      "Average value: 2.513160383042201 for episode: 581\n",
      "Average value: 2.537502363890091 for episode: 582\n",
      "Average value: 2.5606272456955863 for episode: 583\n",
      "Average value: 2.532595883410807 for episode: 584\n",
      "Average value: 2.5059660892402666 for episode: 585\n",
      "Average value: 2.4806677847782534 for episode: 586\n",
      "Average value: 2.4566343955393406 for episode: 587\n",
      "Average value: 2.4838026757623735 for episode: 588\n",
      "Average value: 2.4596125419742547 for episode: 589\n",
      "Average value: 2.4866319148755416 for episode: 590\n",
      "Average value: 2.4623003191317645 for episode: 591\n",
      "Average value: 2.4391853031751762 for episode: 592\n",
      "Average value: 2.4172260380164174 for episode: 593\n",
      "Average value: 2.4463647361155965 for episode: 594\n",
      "Average value: 2.4740464993098166 for episode: 595\n",
      "Average value: 2.5003441743443258 for episode: 596\n",
      "Average value: 2.525326965627109 for episode: 597\n",
      "Average value: 2.4990606173457537 for episode: 598\n",
      "Average value: 2.5241075864784657 for episode: 599\n",
      "Average value: 2.4979022071545423 for episode: 600\n",
      "Average value: 2.523007096796815 for episode: 601\n",
      "Average value: 2.5468567419569736 for episode: 602\n",
      "Average value: 2.519513904859125 for episode: 603\n",
      "Average value: 2.4935382096161685 for episode: 604\n",
      "Average value: 2.46886129913536 for episode: 605\n",
      "Average value: 2.495418234178592 for episode: 606\n",
      "Average value: 2.4706473224696626 for episode: 607\n",
      "Average value: 2.4471149563461796 for episode: 608\n",
      "Average value: 2.4747592085288703 for episode: 609\n",
      "Average value: 2.451021248102427 for episode: 610\n",
      "Average value: 2.4284701856973054 for episode: 611\n",
      "Average value: 2.45704667641244 for episode: 612\n",
      "Average value: 2.4341943425918178 for episode: 613\n",
      "Average value: 2.412484625462227 for episode: 614\n",
      "Average value: 2.3918603941891154 for episode: 615\n",
      "Average value: 2.4222673744796595 for episode: 616\n",
      "Average value: 2.451154005755676 for episode: 617\n",
      "Average value: 2.4285963054678925 for episode: 618\n",
      "Average value: 2.407166490194498 for episode: 619\n",
      "Average value: 2.3868081656847733 for episode: 620\n",
      "Average value: 2.3674677574005347 for episode: 621\n",
      "Average value: 2.399094369530508 for episode: 622\n",
      "Average value: 2.4291396510539824 for episode: 623\n",
      "Average value: 2.407682668501283 for episode: 624\n",
      "Average value: 2.4372985350762186 for episode: 625\n",
      "Average value: 2.4654336083224075 for episode: 626\n",
      "Average value: 2.4921619279062868 for episode: 627\n",
      "Average value: 2.4675538315109726 for episode: 628\n",
      "Average value: 2.444176139935424 for episode: 629\n",
      "Average value: 2.4719673329386525 for episode: 630\n",
      "Average value: 2.4983689662917197 for episode: 631\n",
      "Average value: 2.4734505179771338 for episode: 632\n",
      "Average value: 2.449777992078277 for episode: 633\n",
      "Average value: 2.4272890924743633 for episode: 634\n",
      "Average value: 2.405924637850645 for episode: 635\n",
      "Average value: 2.3856284059581125 for episode: 636\n",
      "Average value: 2.366346985660207 for episode: 637\n",
      "Average value: 2.3480296363771966 for episode: 638\n",
      "Average value: 2.3806281545583365 for episode: 639\n",
      "Average value: 2.4115967468304196 for episode: 640\n",
      "Average value: 2.3910169094888984 for episode: 641\n",
      "Average value: 2.3714660640144536 for episode: 642\n",
      "Average value: 2.4028927608137307 for episode: 643\n",
      "Average value: 2.432748122773044 for episode: 644\n",
      "Average value: 2.4611107166343915 for episode: 645\n",
      "Average value: 2.438055180802672 for episode: 646\n",
      "Average value: 2.466152421762538 for episode: 647\n",
      "Average value: 2.442844800674411 for episode: 648\n",
      "Average value: 2.47070256064069 for episode: 649\n",
      "Average value: 2.4971674326086553 for episode: 650\n",
      "Average value: 2.4723090609782226 for episode: 651\n",
      "Average value: 2.498693607929311 for episode: 652\n",
      "Average value: 2.5237589275328456 for episode: 653\n",
      "Average value: 2.5475709811562033 for episode: 654\n",
      "Average value: 2.570192432098393 for episode: 655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:14,874] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 2.5416828104934734 for episode: 656\n",
      "Average value: 2.5145986699688 for episode: 657\n",
      "Average value: 2.48886873647036 for episode: 658\n",
      "Average value: 2.5144252996468417 for episode: 659\n",
      "Average value: 2.5387040346644993 for episode: 660\n",
      "Average value: 2.561768832931274 for episode: 661\n",
      "Average value: 2.5336803912847103 for episode: 662\n",
      "Average value: 2.5569963717204747 for episode: 663\n",
      "Average value: 2.529146553134451 for episode: 664\n",
      "Average value: 2.5526892254777285 for episode: 665\n",
      "Average value: 2.5750547642038417 for episode: 666\n",
      "Average value: 2.5463020259936497 for episode: 667\n",
      "Average value: 2.518986924693967 for episode: 668\n",
      "Average value: 2.4930375784592687 for episode: 669\n",
      "Average value: 2.4683856995363054 for episode: 670\n",
      "Average value: 2.4449664145594903 for episode: 671\n",
      "Average value: 2.4227180938315156 for episode: 672\n",
      "Average value: 2.4015821891399396 for episode: 673\n",
      "Average value: 2.431503079682942 for episode: 674\n",
      "Average value: 2.459927925698795 for episode: 675\n",
      "Average value: 2.4369315294138554 for episode: 676\n",
      "Average value: 2.4150849529431624 for episode: 677\n",
      "Average value: 2.3943307052960043 for episode: 678\n",
      "Average value: 2.374614170031204 for episode: 679\n",
      "Average value: 2.4058834615296436 for episode: 680\n",
      "Average value: 2.435589288453161 for episode: 681\n",
      "Average value: 2.463809824030503 for episode: 682\n",
      "Average value: 2.4406193328289776 for episode: 683\n",
      "Average value: 2.4685883661875283 for episode: 684\n",
      "Average value: 2.4951589478781515 for episode: 685\n",
      "Average value: 2.470401000484244 for episode: 686\n",
      "Average value: 2.4968809504600316 for episode: 687\n",
      "Average value: 2.52203690293703 for episode: 688\n",
      "Average value: 2.4959350577901787 for episode: 689\n",
      "Average value: 2.47113830490067 for episode: 690\n",
      "Average value: 2.497581389655636 for episode: 691\n",
      "Average value: 2.4727023201728544 for episode: 692\n",
      "Average value: 2.4490672041642116 for episode: 693\n",
      "Average value: 2.476613843956001 for episode: 694\n",
      "Average value: 2.5027831517582007 for episode: 695\n",
      "Average value: 2.5276439941702904 for episode: 696\n",
      "Average value: 2.5512617944617757 for episode: 697\n",
      "Average value: 2.573698704738687 for episode: 698\n",
      "Average value: 2.5450137695017525 for episode: 699\n",
      "Average value: 2.517763081026665 for episode: 700\n",
      "Average value: 2.5418749269753316 for episode: 701\n",
      "Average value: 2.564781180626565 for episode: 702\n",
      "Average value: 2.586542121595236 for episode: 703\n",
      "Average value: 2.5572150155154745 for episode: 704\n",
      "Average value: 2.529354264739701 for episode: 705\n",
      "Average value: 2.5028865515027157 for episode: 706\n",
      "Average value: 2.5277422239275795 for episode: 707\n",
      "Average value: 2.5013551127312006 for episode: 708\n",
      "Average value: 2.4762873570946407 for episode: 709\n",
      "Average value: 2.5024729892399082 for episode: 710\n",
      "Average value: 2.4773493397779127 for episode: 711\n",
      "Average value: 2.503481872789017 for episode: 712\n",
      "Average value: 2.528307779149566 for episode: 713\n",
      "Average value: 2.5518923901920876 for episode: 714\n",
      "Average value: 2.5742977706824828 for episode: 715\n",
      "Average value: 2.5955828821483586 for episode: 716\n",
      "Average value: 2.6158037380409405 for episode: 717\n",
      "Average value: 2.585013551138893 for episode: 718\n",
      "Average value: 2.5557628735819486 for episode: 719\n",
      "Average value: 2.577974729902851 for episode: 720\n",
      "Average value: 2.599075993407708 for episode: 721\n",
      "Average value: 2.6191221937373226 for episode: 722\n",
      "Average value: 2.6381660840504564 for episode: 723\n",
      "Average value: 2.6562577798479334 for episode: 724\n",
      "Average value: 2.6734448908555364 for episode: 725\n",
      "Average value: 2.6897726463127594 for episode: 726\n",
      "Average value: 2.705284013997121 for episode: 727\n",
      "Average value: 2.720019813297265 for episode: 728\n",
      "Average value: 2.7340188226324016 for episode: 729\n",
      "Average value: 2.7473178815007815 for episode: 730\n",
      "Average value: 2.759951987425742 for episode: 731\n",
      "Average value: 2.721954388054455 for episode: 732\n",
      "Average value: 2.685856668651732 for episode: 733\n",
      "Average value: 2.6515638352191453 for episode: 734\n",
      "Average value: 2.668985643458188 for episode: 735\n",
      "Average value: 2.6355363612852782 for episode: 736\n",
      "Average value: 2.603759543221014 for episode: 737\n",
      "Average value: 2.623571566059963 for episode: 738\n",
      "Average value: 2.592392987756965 for episode: 739\n",
      "Average value: 2.6127733383691165 for episode: 740\n",
      "Average value: 2.6321346714506606 for episode: 741\n",
      "Average value: 2.6505279378781275 for episode: 742\n",
      "Average value: 2.668001540984221 for episode: 743\n",
      "Average value: 2.6846014639350098 for episode: 744\n",
      "Average value: 2.700371390738259 for episode: 745\n",
      "Average value: 2.715352821201346 for episode: 746\n",
      "Average value: 2.6795851801412787 for episode: 747\n",
      "Average value: 2.645605921134215 for episode: 748\n",
      "Average value: 2.663325625077504 for episode: 749\n",
      "Average value: 2.6301593438236286 for episode: 750\n",
      "Average value: 2.5986513766324473 for episode: 751\n",
      "Average value: 2.568718807800825 for episode: 752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:16,975] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001100.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 2.5402828674107836 for episode: 753\n",
      "Average value: 2.5132687240402443 for episode: 754\n",
      "Average value: 2.537605287838232 for episode: 755\n",
      "Average value: 2.5107250234463203 for episode: 756\n",
      "Average value: 2.535188772274004 for episode: 757\n",
      "Average value: 2.508429333660304 for episode: 758\n",
      "Average value: 2.5330078669772886 for episode: 759\n",
      "Average value: 2.506357473628424 for episode: 760\n",
      "Average value: 2.5310395999470026 for episode: 761\n",
      "Average value: 2.504487619949652 for episode: 762\n",
      "Average value: 2.5292632389521694 for episode: 763\n",
      "Average value: 2.502800077004561 for episode: 764\n",
      "Average value: 2.527660073154333 for episode: 765\n",
      "Average value: 2.501277069496616 for episode: 766\n",
      "Average value: 2.4762132160217853 for episode: 767\n",
      "Average value: 2.4524025552206963 for episode: 768\n",
      "Average value: 2.479782427459661 for episode: 769\n",
      "Average value: 2.455793306086678 for episode: 770\n",
      "Average value: 2.483003640782344 for episode: 771\n",
      "Average value: 2.5088534587432267 for episode: 772\n",
      "Average value: 2.4834107858060652 for episode: 773\n",
      "Average value: 2.459240246515762 for episode: 774\n",
      "Average value: 2.436278234189974 for episode: 775\n",
      "Average value: 2.4644643224804748 for episode: 776\n",
      "Average value: 2.491241106356451 for episode: 777\n",
      "Average value: 2.516679051038628 for episode: 778\n",
      "Average value: 2.540845098486696 for episode: 779\n",
      "Average value: 2.5138028435623614 for episode: 780\n",
      "Average value: 2.4881127013842432 for episode: 781\n",
      "Average value: 2.463707066315031 for episode: 782\n",
      "Average value: 2.4405217129992796 for episode: 783\n",
      "Average value: 2.4184956273493157 for episode: 784\n",
      "Average value: 2.39757084598185 for episode: 785\n",
      "Average value: 2.827692303682757 for episode: 786\n",
      "Average value: 3.586307688498619 for episode: 787\n",
      "Average value: 4.306992304073688 for episode: 788\n",
      "Average value: 4.841642688870004 for episode: 789\n",
      "Average value: 5.449560554426503 for episode: 790\n",
      "Average value: 5.927082526705178 for episode: 791\n",
      "Average value: 6.930728400369919 for episode: 792\n",
      "Average value: 7.484191980351423 for episode: 793\n",
      "Average value: 8.059982381333851 for episode: 794\n",
      "Average value: 8.506983262267159 for episode: 795\n",
      "Average value: 8.381634099153802 for episode: 796\n",
      "Average value: 8.212552394196111 for episode: 797\n",
      "Average value: 8.101924774486305 for episode: 798\n",
      "Average value: 7.996828535761989 for episode: 799\n",
      "Average value: 8.69698710897389 for episode: 800\n",
      "Average value: 9.362137753525195 for episode: 801\n",
      "Average value: 9.194030865848935 for episode: 802\n",
      "Average value: 9.434329322556488 for episode: 803\n",
      "Average value: 9.612612856428663 for episode: 804\n",
      "Average value: 9.63198221360723 for episode: 805\n",
      "Average value: 9.800383102926869 for episode: 806\n",
      "Average value: 10.160363947780525 for episode: 807\n",
      "Average value: 10.302345750391499 for episode: 808\n",
      "Average value: 10.487228462871922 for episode: 809\n",
      "Average value: 10.962867039728325 for episode: 810\n",
      "Average value: 10.764723687741908 for episode: 811\n",
      "Average value: 11.326487503354812 for episode: 812\n",
      "Average value: 11.110163128187072 for episode: 813\n",
      "Average value: 11.504654971777716 for episode: 814\n",
      "Average value: 11.87942222318883 for episode: 815\n",
      "Average value: 12.535451112029387 for episode: 816\n",
      "Average value: 12.208678556427918 for episode: 817\n",
      "Average value: 13.298244628606522 for episode: 818\n",
      "Average value: 12.883332397176195 for episode: 819\n",
      "Average value: 13.789165777317386 for episode: 820\n",
      "Average value: 13.549707488451515 for episode: 821\n",
      "Average value: 14.07222211402894 for episode: 822\n",
      "Average value: 15.06861100832749 for episode: 823\n",
      "Average value: 14.915180457911115 for episode: 824\n",
      "Average value: 14.869421435015557 for episode: 825\n",
      "Average value: 14.575950363264777 for episode: 826\n",
      "Average value: 15.39715284510154 for episode: 827\n",
      "Average value: 15.127295202846462 for episode: 828\n",
      "Average value: 15.820930442704139 for episode: 829\n",
      "Average value: 15.679883920568932 for episode: 830\n",
      "Average value: 15.445889724540486 for episode: 831\n",
      "Average value: 15.37359523831346 for episode: 832\n",
      "Average value: 16.254915476397784 for episode: 833\n",
      "Average value: 16.342169702577895 for episode: 834\n",
      "Average value: 17.175061217449 for episode: 835\n",
      "Average value: 17.116308156576547 for episode: 836\n",
      "Average value: 17.61049274874772 for episode: 837\n",
      "Average value: 18.479968111310335 for episode: 838\n",
      "Average value: 18.80596970574482 for episode: 839\n",
      "Average value: 18.265671220457577 for episode: 840\n",
      "Average value: 18.552387659434697 for episode: 841\n",
      "Average value: 18.224768276462964 for episode: 842\n",
      "Average value: 18.513529862639814 for episode: 843\n",
      "Average value: 19.437853369507824 for episode: 844\n",
      "Average value: 19.015960701032434 for episode: 845\n",
      "Average value: 19.915162665980812 for episode: 846\n",
      "Average value: 19.269404532681772 for episode: 847\n",
      "Average value: 18.805934306047682 for episode: 848\n",
      "Average value: 19.365637590745298 for episode: 849\n",
      "Average value: 19.79735571120803 for episode: 850\n",
      "Average value: 19.457487925647627 for episode: 851\n",
      "Average value: 19.834613529365246 for episode: 852\n",
      "Average value: 20.242882852896983 for episode: 853\n",
      "Average value: 19.930738710252133 for episode: 854\n",
      "Average value: 20.334201774739523 for episode: 855\n",
      "Average value: 20.517491686002543 for episode: 856\n",
      "Average value: 20.991617101702417 for episode: 857\n",
      "Average value: 20.792036246617297 for episode: 858\n",
      "Average value: 19.95243443428643 for episode: 859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:27,863] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001200.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 20.85481271257211 for episode: 860\n",
      "Average value: 20.5620720769435 for episode: 861\n",
      "Average value: 20.933968473096325 for episode: 862\n",
      "Average value: 21.537270049441506 for episode: 863\n",
      "Average value: 22.060406546969432 for episode: 864\n",
      "Average value: 23.40738621962096 for episode: 865\n",
      "Average value: 24.337016908639914 for episode: 866\n",
      "Average value: 24.87016606320792 for episode: 867\n",
      "Average value: 25.676657760047522 for episode: 868\n",
      "Average value: 26.092824872045146 for episode: 869\n",
      "Average value: 25.538183628442887 for episode: 870\n",
      "Average value: 26.811274447020743 for episode: 871\n",
      "Average value: 27.370710724669703 for episode: 872\n",
      "Average value: 28.352175188436217 for episode: 873\n",
      "Average value: 28.884566429014402 for episode: 874\n",
      "Average value: 29.89033810756368 for episode: 875\n",
      "Average value: 30.795821202185493 for episode: 876\n",
      "Average value: 31.206030142076216 for episode: 877\n",
      "Average value: 31.995728634972405 for episode: 878\n",
      "Average value: 32.495942203223784 for episode: 879\n",
      "Average value: 31.121145093062594 for episode: 880\n",
      "Average value: 31.415087838409466 for episode: 881\n",
      "Average value: 33.04433344648899 for episode: 882\n",
      "Average value: 33.592116774164545 for episode: 883\n",
      "Average value: 34.16251093545631 for episode: 884\n",
      "Average value: 34.70438538868349 for episode: 885\n",
      "Average value: 35.26916611924931 for episode: 886\n",
      "Average value: 36.305707813286844 for episode: 887\n",
      "Average value: 36.940422422622504 for episode: 888\n",
      "Average value: 37.64340130149137 for episode: 889\n",
      "Average value: 37.8112312364168 for episode: 890\n",
      "Average value: 38.22066967459596 for episode: 891\n",
      "Average value: 38.859636190866155 for episode: 892\n",
      "Average value: 38.91665438132284 for episode: 893\n",
      "Average value: 38.9208216622567 for episode: 894\n",
      "Average value: 40.27478057914386 for episode: 895\n",
      "Average value: 41.211041550186664 for episode: 896\n",
      "Average value: 41.35048947267733 for episode: 897\n",
      "Average value: 42.08296499904346 for episode: 898\n",
      "Average value: 42.528816749091284 for episode: 899\n",
      "Average value: 42.452375911636715 for episode: 900\n",
      "Average value: 42.629757116054876 for episode: 901\n",
      "Average value: 42.64826926025213 for episode: 902\n",
      "Average value: 42.86585579723952 for episode: 903\n",
      "Average value: 42.92256300737754 for episode: 904\n",
      "Average value: 44.07643485700866 for episode: 905\n",
      "Average value: 44.72261311415822 for episode: 906\n",
      "Average value: 44.686482458450314 for episode: 907\n",
      "Average value: 45.5521583355278 for episode: 908\n",
      "Average value: 45.62455041875141 for episode: 909\n",
      "Average value: 45.543322897813844 for episode: 910\n",
      "Average value: 45.36615675292315 for episode: 911\n",
      "Average value: 45.59784891527699 for episode: 912\n",
      "Average value: 46.41795646951314 for episode: 913\n",
      "Average value: 46.297058646037485 for episode: 914\n",
      "Average value: 46.43220571373561 for episode: 915\n",
      "Average value: 46.71059542804883 for episode: 916\n",
      "Average value: 46.47506565664639 for episode: 917\n",
      "Average value: 46.30131237381407 for episode: 918\n",
      "Average value: 46.48624675512337 for episode: 919\n",
      "Average value: 46.5119344173672 for episode: 920\n",
      "Average value: 46.486337696498836 for episode: 921\n",
      "Average value: 46.76202081167389 for episode: 922\n",
      "Average value: 46.723919771090195 for episode: 923\n",
      "Average value: 46.73772378253568 for episode: 924\n",
      "Average value: 46.8508375934089 for episode: 925\n",
      "Average value: 46.95829571373845 for episode: 926\n",
      "Average value: 46.610380928051526 for episode: 927\n",
      "Average value: 46.47986188164895 for episode: 928\n",
      "Average value: 46.655868787566504 for episode: 929\n",
      "Average value: 46.573075348188176 for episode: 930\n",
      "Average value: 46.29442158077876 for episode: 931\n",
      "Average value: 46.22970050173982 for episode: 932\n",
      "Average value: 44.71821547665282 for episode: 933\n",
      "Average value: 44.98230470282018 for episode: 934\n",
      "Average value: 45.18318946767917 for episode: 935\n",
      "Average value: 45.12402999429521 for episode: 936\n",
      "Average value: 45.26782849458045 for episode: 937\n",
      "Average value: 45.45443706985143 for episode: 938\n",
      "Average value: 45.631715216358856 for episode: 939\n",
      "Average value: 45.55012945554091 for episode: 940\n",
      "Average value: 45.622622982763865 for episode: 941\n",
      "Average value: 45.59149183362567 for episode: 942\n",
      "Average value: 45.51191724194439 for episode: 943\n",
      "Average value: 45.38632137984717 for episode: 944\n",
      "Average value: 45.267005310854806 for episode: 945\n",
      "Average value: 45.153655045312064 for episode: 946\n",
      "Average value: 45.04597229304646 for episode: 947\n",
      "Average value: 44.943673678394134 for episode: 948\n",
      "Average value: 45.096489994474425 for episode: 949\n",
      "Average value: 45.2416654947507 for episode: 950\n",
      "Average value: 45.37958222001316 for episode: 951\n",
      "Average value: 45.310603109012504 for episode: 952\n",
      "Average value: 45.64507295356188 for episode: 953\n",
      "Average value: 45.51281930588378 for episode: 954\n",
      "Average value: 45.637178340589585 for episode: 955\n",
      "Average value: 45.5053194235601 for episode: 956\n",
      "Average value: 45.730053452382094 for episode: 957\n",
      "Average value: 46.39355077976299 for episode: 958\n",
      "Average value: 46.27387324077484 for episode: 959\n",
      "Average value: 46.3101795787361 for episode: 960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:26:58,536] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001300.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 46.29467059979929 for episode: 961\n",
      "Average value: 46.12993706980932 for episode: 962\n",
      "Average value: 46.223440216318856 for episode: 963\n",
      "Average value: 46.01226820550291 for episode: 964\n",
      "Average value: 45.961654795227766 for episode: 965\n",
      "Average value: 46.06357205546637 for episode: 966\n",
      "Average value: 45.81039345269305 for episode: 967\n",
      "Average value: 45.8698737800584 for episode: 968\n",
      "Average value: 45.77638009105548 for episode: 969\n",
      "Average value: 45.8875610865027 for episode: 970\n",
      "Average value: 45.64318303217756 for episode: 971\n",
      "Average value: 45.61102388056868 for episode: 972\n",
      "Average value: 45.33047268654025 for episode: 973\n",
      "Average value: 43.86394905221323 for episode: 974\n",
      "Average value: 43.67075159960257 for episode: 975\n",
      "Average value: 43.737214019622435 for episode: 976\n",
      "Average value: 43.85035331864131 for episode: 977\n",
      "Average value: 43.807835652709244 for episode: 978\n",
      "Average value: 42.167443870073775 for episode: 979\n",
      "Average value: 42.20907167657008 for episode: 980\n",
      "Average value: 42.198618092741576 for episode: 981\n",
      "Average value: 42.338687188104494 for episode: 982\n",
      "Average value: 40.621752828699265 for episode: 983\n",
      "Average value: 40.7906651872643 for episode: 984\n",
      "Average value: 40.751131927901085 for episode: 985\n",
      "Average value: 40.81357533150603 for episode: 986\n",
      "Average value: 40.87289656493073 for episode: 987\n",
      "Average value: 40.92925173668419 for episode: 988\n",
      "Average value: 40.982789149849985 for episode: 989\n",
      "Average value: 40.83364969235748 for episode: 990\n",
      "Average value: 40.99196720773961 for episode: 991\n",
      "Average value: 41.09236884735263 for episode: 992\n",
      "Average value: 41.037750404985 for episode: 993\n",
      "Average value: 41.08586288473575 for episode: 994\n",
      "Average value: 40.93156974049896 for episode: 995\n",
      "Average value: 40.934991253474 for episode: 996\n",
      "Average value: 41.0382416908003 for episode: 997\n",
      "Average value: 41.28632960626028 for episode: 998\n",
      "Average value: 41.27201312594726 for episode: 999\n",
      "Average value: 41.45841246964989 for episode: 1000\n",
      "Average value: 41.535491846167396 for episode: 1001\n",
      "Average value: 41.55871725385902 for episode: 1002\n",
      "Average value: 41.58078139116607 for episode: 1003\n",
      "Average value: 41.601742321607766 for episode: 1004\n",
      "Average value: 41.521655205527374 for episode: 1005\n",
      "Average value: 41.695572445251 for episode: 1006\n",
      "Average value: 41.61079382298845 for episode: 1007\n",
      "Average value: 41.730254131839025 for episode: 1008\n",
      "Average value: 41.94374142524707 for episode: 1009\n",
      "Average value: 42.24655435398471 for episode: 1010\n",
      "Average value: 42.384226636285476 for episode: 1011\n",
      "Average value: 42.4650153044712 for episode: 1012\n",
      "Average value: 42.59176453924764 for episode: 1013\n",
      "Average value: 42.91217631228526 for episode: 1014\n",
      "Average value: 43.466567496671 for episode: 1015\n",
      "Average value: 43.44323912183744 for episode: 1016\n",
      "Average value: 43.671077165745565 for episode: 1017\n",
      "Average value: 43.63752330745828 for episode: 1018\n",
      "Average value: 44.005647142085365 for episode: 1019\n",
      "Average value: 43.95536478498109 for episode: 1020\n",
      "Average value: 44.157596545732034 for episode: 1021\n",
      "Average value: 44.14971671844543 for episode: 1022\n",
      "Average value: 44.142230882523165 for episode: 1023\n",
      "Average value: 44.08511933839701 for episode: 1024\n",
      "Average value: 43.83086337147716 for episode: 1025\n",
      "Average value: 44.3893202029033 for episode: 1026\n",
      "Average value: 45.01985419275813 for episode: 1027\n",
      "Average value: 45.81886148312022 for episode: 1028\n",
      "Average value: 45.727918408964214 for episode: 1029\n",
      "Average value: 45.791522488516 for episode: 1030\n",
      "Average value: 45.8019463640902 for episode: 1031\n",
      "Average value: 45.61184904588569 for episode: 1032\n",
      "Average value: 45.68125659359141 for episode: 1033\n",
      "Average value: 45.447193763911834 for episode: 1034\n",
      "Average value: 45.07483407571624 for episode: 1035\n",
      "Average value: 45.12109237193042 for episode: 1036\n",
      "Average value: 44.8650377533339 for episode: 1037\n",
      "Average value: 44.6717858656672 for episode: 1038\n",
      "Average value: 44.58819657238384 for episode: 1039\n",
      "Average value: 43.60878674376465 for episode: 1040\n",
      "Average value: 43.428347406576414 for episode: 1041\n",
      "Average value: 43.306930036247586 for episode: 1042\n",
      "Average value: 43.09158353443521 for episode: 1043\n",
      "Average value: 43.237004357713445 for episode: 1044\n",
      "Average value: 43.175154139827775 for episode: 1045\n",
      "Average value: 43.61639643283639 for episode: 1046\n",
      "Average value: 43.63557661119457 for episode: 1047\n",
      "Average value: 43.453797780634844 for episode: 1048\n",
      "Average value: 43.5311078916031 for episode: 1049\n",
      "Average value: 43.354552497022944 for episode: 1050\n",
      "Average value: 43.086824872171796 for episode: 1051\n",
      "Average value: 42.8324836285632 for episode: 1052\n",
      "Average value: 42.59085944713504 for episode: 1053\n",
      "Average value: 42.41131647477829 for episode: 1054\n",
      "Average value: 42.34075065103937 for episode: 1055\n",
      "Average value: 42.323713118487404 for episode: 1056\n",
      "Average value: 41.30752746256304 for episode: 1057\n",
      "Average value: 41.492151089434884 for episode: 1058\n",
      "Average value: 39.817543534963136 for episode: 1059\n",
      "Average value: 40.07666635821498 for episode: 1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:27:30,187] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001400.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 40.072833040304225 for episode: 1061\n",
      "Average value: 40.019191388289016 for episode: 1062\n",
      "Average value: 39.86823181887456 for episode: 1063\n",
      "Average value: 39.77482022793083 for episode: 1064\n",
      "Average value: 38.48607921653429 for episode: 1065\n",
      "Average value: 38.71177525570757 for episode: 1066\n",
      "Average value: 38.726186492922196 for episode: 1067\n",
      "Average value: 38.689877168276084 for episode: 1068\n",
      "Average value: 38.70538330986228 for episode: 1069\n",
      "Average value: 38.77011414436916 for episode: 1070\n",
      "Average value: 38.8316084371507 for episode: 1071\n",
      "Average value: 38.74002801529316 for episode: 1072\n",
      "Average value: 38.803026614528505 for episode: 1073\n",
      "Average value: 38.96287528380208 for episode: 1074\n",
      "Average value: 38.96473151961197 for episode: 1075\n",
      "Average value: 38.91649494363137 for episode: 1076\n",
      "Average value: 39.070670196449804 for episode: 1077\n",
      "Average value: 39.16713668662731 for episode: 1078\n",
      "Average value: 39.40877985229594 for episode: 1079\n",
      "Average value: 39.488340859681145 for episode: 1080\n",
      "Average value: 39.563923816697084 for episode: 1081\n",
      "Average value: 39.63572762586222 for episode: 1082\n",
      "Average value: 39.65394124456911 for episode: 1083\n",
      "Average value: 39.621244182340654 for episode: 1084\n",
      "Average value: 39.690181973223616 for episode: 1085\n",
      "Average value: 39.80567287456243 for episode: 1086\n",
      "Average value: 39.715389230834305 for episode: 1087\n",
      "Average value: 39.72961976929259 for episode: 1088\n",
      "Average value: 39.693138780827965 for episode: 1089\n",
      "Average value: 39.808481841786566 for episode: 1090\n",
      "Average value: 39.91805774969724 for episode: 1091\n",
      "Average value: 38.522154862212375 for episode: 1092\n",
      "Average value: 38.54604711910176 for episode: 1093\n",
      "Average value: 38.71874476314667 for episode: 1094\n",
      "Average value: 38.68280752498933 for episode: 1095\n",
      "Average value: 38.64866714873986 for episode: 1096\n",
      "Average value: 38.61623379130287 for episode: 1097\n",
      "Average value: 38.78542210173772 for episode: 1098\n",
      "Average value: 38.99615099665083 for episode: 1099\n",
      "Average value: 38.996343446818294 for episode: 1100\n",
      "Average value: 39.04652627447738 for episode: 1101\n",
      "Average value: 39.19419996075351 for episode: 1102\n",
      "Average value: 39.234489962715834 for episode: 1103\n",
      "Average value: 39.17276546458004 for episode: 1104\n",
      "Average value: 39.11412719135103 for episode: 1105\n",
      "Average value: 37.358420831783484 for episode: 1106\n",
      "Average value: 37.490499790194306 for episode: 1107\n",
      "Average value: 37.56597480068459 for episode: 1108\n",
      "Average value: 37.68767606065036 for episode: 1109\n",
      "Average value: 37.85329225761784 for episode: 1110\n",
      "Average value: 38.01062764473694 for episode: 1111\n",
      "Average value: 38.11009626250009 for episode: 1112\n",
      "Average value: 38.354591449375086 for episode: 1113\n",
      "Average value: 38.486861876906325 for episode: 1114\n",
      "Average value: 38.612518783061006 for episode: 1115\n",
      "Average value: 38.531892843907954 for episode: 1116\n",
      "Average value: 38.505298201712556 for episode: 1117\n",
      "Average value: 38.68003329162693 for episode: 1118\n",
      "Average value: 38.84603162704558 for episode: 1119\n",
      "Average value: 38.8037300456933 for episode: 1120\n",
      "Average value: 38.76354354340863 for episode: 1121\n",
      "Average value: 38.7753663662382 for episode: 1122\n",
      "Average value: 38.68659804792629 for episode: 1123\n",
      "Average value: 38.65226814552997 for episode: 1124\n",
      "Average value: 38.61965473825347 for episode: 1125\n",
      "Average value: 38.5886720013408 for episode: 1126\n",
      "Average value: 38.60923840127376 for episode: 1127\n",
      "Average value: 38.678776481210065 for episode: 1128\n",
      "Average value: 38.54483765714956 for episode: 1129\n",
      "Average value: 38.56759577429208 for episode: 1130\n",
      "Average value: 36.989215985577474 for episode: 1131\n",
      "Average value: 36.9397551862986 for episode: 1132\n",
      "Average value: 37.042767426983666 for episode: 1133\n",
      "Average value: 37.19062905563448 for episode: 1134\n",
      "Average value: 37.28109760285276 for episode: 1135\n",
      "Average value: 37.31704272271012 for episode: 1136\n",
      "Average value: 37.55119058657461 for episode: 1137\n",
      "Average value: 36.573631057245876 for episode: 1138\n",
      "Average value: 36.594949504383585 for episode: 1139\n",
      "Average value: 36.765202029164406 for episode: 1140\n",
      "Average value: 36.876941927706184 for episode: 1141\n",
      "Average value: 36.93309483132087 for episode: 1142\n",
      "Average value: 37.03644008975483 for episode: 1143\n",
      "Average value: 37.08461808526708 for episode: 1144\n",
      "Average value: 37.08038718100373 for episode: 1145\n",
      "Average value: 37.12636782195354 for episode: 1146\n",
      "Average value: 37.170049430855855 for episode: 1147\n",
      "Average value: 37.261546959313065 for episode: 1148\n",
      "Average value: 37.198469611347406 for episode: 1149\n",
      "Average value: 37.288546130780034 for episode: 1150\n",
      "Average value: 37.27411882424103 for episode: 1151\n",
      "Average value: 37.26041288302898 for episode: 1152\n",
      "Average value: 35.89739223887753 for episode: 1153\n",
      "Average value: 35.95252262693365 for episode: 1154\n",
      "Average value: 36.25489649558697 for episode: 1155\n",
      "Average value: 36.39215167080762 for episode: 1156\n",
      "Average value: 36.62254408726724 for episode: 1157\n",
      "Average value: 36.69141688290387 for episode: 1158\n",
      "Average value: 36.756846038758674 for episode: 1159\n",
      "Average value: 36.869003736820744 for episode: 1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:27:58,193] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001500.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 37.025553549979705 for episode: 1161\n",
      "Average value: 37.07427587248072 for episode: 1162\n",
      "Average value: 37.22056207885668 for episode: 1163\n",
      "Average value: 37.30953397491385 for episode: 1164\n",
      "Average value: 37.49405727616815 for episode: 1165\n",
      "Average value: 37.66935441235974 for episode: 1166\n",
      "Average value: 37.68588669174175 for episode: 1167\n",
      "Average value: 37.70159235715466 for episode: 1168\n",
      "Average value: 37.716512739296924 for episode: 1169\n",
      "Average value: 37.78068710233208 for episode: 1170\n",
      "Average value: 37.84165274721548 for episode: 1171\n",
      "Average value: 37.949570109854704 for episode: 1172\n",
      "Average value: 38.05209160436197 for episode: 1173\n",
      "Average value: 38.149487024143866 for episode: 1174\n",
      "Average value: 38.19201267293668 for episode: 1175\n",
      "Average value: 38.33241203928984 for episode: 1176\n",
      "Average value: 38.46579143732534 for episode: 1177\n",
      "Average value: 38.492501865459076 for episode: 1178\n",
      "Average value: 38.41787677218612 for episode: 1179\n",
      "Average value: 38.54698293357681 for episode: 1180\n",
      "Average value: 38.619633786897964 for episode: 1181\n",
      "Average value: 38.63865209755307 for episode: 1182\n",
      "Average value: 38.55671949267542 for episode: 1183\n",
      "Average value: 38.57888351804165 for episode: 1184\n",
      "Average value: 37.04993934213957 for episode: 1185\n",
      "Average value: 37.04744237503259 for episode: 1186\n",
      "Average value: 37.19507025628096 for episode: 1187\n",
      "Average value: 37.28531674346691 for episode: 1188\n",
      "Average value: 37.471050906293556 for episode: 1189\n",
      "Average value: 37.597498360978875 for episode: 1190\n",
      "Average value: 37.617623442929926 for episode: 1191\n",
      "Average value: 37.686742270783434 for episode: 1192\n",
      "Average value: 37.70240515724426 for episode: 1193\n",
      "Average value: 37.76728489938205 for episode: 1194\n",
      "Average value: 37.728920654412946 for episode: 1195\n",
      "Average value: 37.642474621692294 for episode: 1196\n",
      "Average value: 37.510350890607675 for episode: 1197\n",
      "Average value: 36.38483334607729 for episode: 1198\n",
      "Average value: 36.36559167877343 for episode: 1199\n",
      "Average value: 36.497312094834754 for episode: 1200\n",
      "Average value: 36.47244649009301 for episode: 1201\n",
      "Average value: 34.89882416558836 for episode: 1202\n",
      "Average value: 35.00388295730894 for episode: 1203\n",
      "Average value: 35.20368880944349 for episode: 1204\n",
      "Average value: 35.29350436897132 for episode: 1205\n",
      "Average value: 35.478829150522756 for episode: 1206\n",
      "Average value: 35.75488769299661 for episode: 1207\n",
      "Average value: 36.017143308346775 for episode: 1208\n",
      "Average value: 36.01628614292943 for episode: 1209\n",
      "Average value: 36.115471835782955 for episode: 1210\n",
      "Average value: 36.209698243993806 for episode: 1211\n",
      "Average value: 36.24921333179412 for episode: 1212\n",
      "Average value: 36.33675266520441 for episode: 1213\n",
      "Average value: 36.46991503194419 for episode: 1214\n",
      "Average value: 36.646419280346976 for episode: 1215\n",
      "Average value: 36.76409831632963 for episode: 1216\n",
      "Average value: 36.825893400513145 for episode: 1217\n",
      "Average value: 36.88459873048748 for episode: 1218\n",
      "Average value: 36.99036879396311 for episode: 1219\n",
      "Average value: 36.89085035426495 for episode: 1220\n",
      "Average value: 36.8963078365517 for episode: 1221\n",
      "Average value: 37.15149244472411 for episode: 1222\n",
      "Average value: 37.14391782248791 for episode: 1223\n",
      "Average value: 37.336721931363506 for episode: 1224\n",
      "Average value: 37.31988583479533 for episode: 1225\n",
      "Average value: 37.30389154305556 for episode: 1226\n",
      "Average value: 37.38869696590279 for episode: 1227\n",
      "Average value: 37.419262117607644 for episode: 1228\n",
      "Average value: 37.49829901172726 for episode: 1229\n",
      "Average value: 37.5733840611409 for episode: 1230\n",
      "Average value: 37.89471485808386 for episode: 1231\n",
      "Average value: 37.89997911517966 for episode: 1232\n",
      "Average value: 36.80498015942067 for episode: 1233\n",
      "Average value: 36.914731151449644 for episode: 1234\n",
      "Average value: 36.91899459387716 for episode: 1235\n",
      "Average value: 37.023044864183305 for episode: 1236\n",
      "Average value: 37.22189262097414 for episode: 1237\n",
      "Average value: 37.26079798992543 for episode: 1238\n",
      "Average value: 37.24775809042916 for episode: 1239\n",
      "Average value: 36.135370185907696 for episode: 1240\n",
      "Average value: 36.27860167661231 for episode: 1241\n",
      "Average value: 36.414671592781694 for episode: 1242\n",
      "Average value: 36.593938013142605 for episode: 1243\n",
      "Average value: 36.714241112485475 for episode: 1244\n",
      "Average value: 35.0785290568612 for episode: 1245\n",
      "Average value: 35.27460260401814 for episode: 1246\n",
      "Average value: 35.66087247381723 for episode: 1247\n",
      "Average value: 35.92782885012636 for episode: 1248\n",
      "Average value: 35.53143740762004 for episode: 1249\n",
      "Average value: 35.754865537239034 for episode: 1250\n",
      "Average value: 34.76712226037708 for episode: 1251\n",
      "Average value: 34.928766147358225 for episode: 1252\n",
      "Average value: 35.182327839990315 for episode: 1253\n",
      "Average value: 35.523211447990796 for episode: 1254\n",
      "Average value: 35.69705087559126 for episode: 1255\n",
      "Average value: 35.8621983318117 for episode: 1256\n",
      "Average value: 36.06908841522111 for episode: 1257\n",
      "Average value: 36.16563399446005 for episode: 1258\n",
      "Average value: 36.407352294737045 for episode: 1259\n",
      "Average value: 36.536984680000195 for episode: 1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:28:23,306] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001600.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 36.56013544600019 for episode: 1261\n",
      "Average value: 36.732128673700174 for episode: 1262\n",
      "Average value: 36.845522240015164 for episode: 1263\n",
      "Average value: 36.9032461280144 for episode: 1264\n",
      "Average value: 36.90808382161368 for episode: 1265\n",
      "Average value: 37.112679630532995 for episode: 1266\n",
      "Average value: 37.207045649006346 for episode: 1267\n",
      "Average value: 37.29669336655603 for episode: 1268\n",
      "Average value: 37.38185869822823 for episode: 1269\n",
      "Average value: 37.41276576331681 for episode: 1270\n",
      "Average value: 37.44212747515097 for episode: 1271\n",
      "Average value: 37.570021101393415 for episode: 1272\n",
      "Average value: 36.64152004632375 for episode: 1273\n",
      "Average value: 36.65944404400756 for episode: 1274\n",
      "Average value: 36.82647184180718 for episode: 1275\n",
      "Average value: 37.13514824971681 for episode: 1276\n",
      "Average value: 37.17839083723097 for episode: 1277\n",
      "Average value: 37.21947129536942 for episode: 1278\n",
      "Average value: 37.25849773060094 for episode: 1279\n",
      "Average value: 36.7455728440709 for episode: 1280\n",
      "Average value: 36.95829420186735 for episode: 1281\n",
      "Average value: 37.11037949177398 for episode: 1282\n",
      "Average value: 37.254860517185286 for episode: 1283\n",
      "Average value: 37.29211749132602 for episode: 1284\n",
      "Average value: 37.27751161675972 for episode: 1285\n",
      "Average value: 37.363636035921736 for episode: 1286\n",
      "Average value: 37.645454234125644 for episode: 1287\n",
      "Average value: 37.76318152241936 for episode: 1288\n",
      "Average value: 37.925022446298385 for episode: 1289\n",
      "Average value: 38.02877132398346 for episode: 1290\n",
      "Average value: 38.427332757784285 for episode: 1291\n",
      "Average value: 38.40596611989507 for episode: 1292\n",
      "Average value: 38.38566781390031 for episode: 1293\n",
      "Average value: 37.0663844232053 for episode: 1294\n",
      "Average value: 37.01306520204503 for episode: 1295\n",
      "Average value: 37.362411941942774 for episode: 1296\n",
      "Average value: 37.44429134484564 for episode: 1297\n",
      "Average value: 37.57207677760336 for episode: 1298\n",
      "Average value: 37.74347293872319 for episode: 1299\n",
      "Average value: 37.90629929178702 for episode: 1300\n",
      "Average value: 37.96098432719767 for episode: 1301\n",
      "Average value: 38.062935110837785 for episode: 1302\n",
      "Average value: 38.1097883552959 for episode: 1303\n",
      "Average value: 38.1542989375311 for episode: 1304\n",
      "Average value: 38.196583990654545 for episode: 1305\n",
      "Average value: 38.186754791121814 for episode: 1306\n",
      "Average value: 36.67741705156572 for episode: 1307\n",
      "Average value: 36.84354619898743 for episode: 1308\n",
      "Average value: 36.951368889038065 for episode: 1309\n",
      "Average value: 37.25380044458616 for episode: 1310\n",
      "Average value: 37.341110422356856 for episode: 1311\n",
      "Average value: 37.52405490123901 for episode: 1312\n",
      "Average value: 37.64785215617706 for episode: 1313\n",
      "Average value: 37.8154595483682 for episode: 1314\n",
      "Average value: 37.87468657094979 for episode: 1315\n",
      "Average value: 38.0309522424023 for episode: 1316\n",
      "Average value: 38.17940463028218 for episode: 1317\n",
      "Average value: 38.47043439876807 for episode: 1318\n",
      "Average value: 38.746912678829666 for episode: 1319\n",
      "Average value: 39.05956704488818 for episode: 1320\n",
      "Average value: 37.30658869264378 for episode: 1321\n",
      "Average value: 37.49125925801159 for episode: 1322\n",
      "Average value: 37.666696295111 for episode: 1323\n",
      "Average value: 37.83336148035545 for episode: 1324\n",
      "Average value: 37.991693406337674 for episode: 1325\n",
      "Average value: 37.94210873602079 for episode: 1326\n",
      "Average value: 37.99500329921975 for episode: 1327\n",
      "Average value: 37.99525313425876 for episode: 1328\n",
      "Average value: 38.09549047754582 for episode: 1329\n",
      "Average value: 37.29071595366853 for episode: 1330\n",
      "Average value: 36.4761801559851 for episode: 1331\n",
      "Average value: 36.95237114818584 for episode: 1332\n",
      "Average value: 37.154752590776546 for episode: 1333\n",
      "Average value: 36.147014961237716 for episode: 1334\n",
      "Average value: 36.28966421317583 for episode: 1335\n",
      "Average value: 36.37518100251704 for episode: 1336\n",
      "Average value: 36.70642195239118 for episode: 1337\n",
      "Average value: 36.87110085477162 for episode: 1338\n",
      "Average value: 35.22754581203304 for episode: 1339\n",
      "Average value: 35.61616852143139 for episode: 1340\n",
      "Average value: 35.735360095359816 for episode: 1341\n",
      "Average value: 36.048592090591825 for episode: 1342\n",
      "Average value: 36.54616248606223 for episode: 1343\n",
      "Average value: 36.86885436175911 for episode: 1344\n",
      "Average value: 36.57541164367115 for episode: 1345\n",
      "Average value: 36.746641061487594 for episode: 1346\n",
      "Average value: 37.05930900841321 for episode: 1347\n",
      "Average value: 37.20634355799255 for episode: 1348\n",
      "Average value: 37.346026380092916 for episode: 1349\n",
      "Average value: 37.12872506108827 for episode: 1350\n",
      "Average value: 37.37228880803386 for episode: 1351\n",
      "Average value: 37.70367436763217 for episode: 1352\n",
      "Average value: 36.318490649250556 for episode: 1353\n",
      "Average value: 35.252566116788024 for episode: 1354\n",
      "Average value: 35.68993781094863 for episode: 1355\n",
      "Average value: 35.805440920401196 for episode: 1356\n",
      "Average value: 36.11516887438113 for episode: 1357\n",
      "Average value: 36.359410430662074 for episode: 1358\n",
      "Average value: 36.74143990912897 for episode: 1359\n",
      "Average value: 36.65436791367252 for episode: 1360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:28:49,349] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001700.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 36.87164951798889 for episode: 1361\n",
      "Average value: 35.678067042089445 for episode: 1362\n",
      "Average value: 36.14416368998497 for episode: 1363\n",
      "Average value: 36.386955505485716 for episode: 1364\n",
      "Average value: 36.81760773021143 for episode: 1365\n",
      "Average value: 37.276727343700855 for episode: 1366\n",
      "Average value: 37.56289097651581 for episode: 1367\n",
      "Average value: 37.834746427690014 for episode: 1368\n",
      "Average value: 37.99300910630551 for episode: 1369\n",
      "Average value: 37.89335865099023 for episode: 1370\n",
      "Average value: 37.89869071844072 for episode: 1371\n",
      "Average value: 38.10375618251868 for episode: 1372\n",
      "Average value: 37.948568373392746 for episode: 1373\n",
      "Average value: 38.00113995472311 for episode: 1374\n",
      "Average value: 38.30108295698695 for episode: 1375\n",
      "Average value: 36.83602880913761 for episode: 1376\n",
      "Average value: 37.09422736868073 for episode: 1377\n",
      "Average value: 35.539516000246685 for episode: 1378\n",
      "Average value: 34.46254020023435 for episode: 1379\n",
      "Average value: 35.03941319022263 for episode: 1380\n",
      "Average value: 35.3374425307115 for episode: 1381\n",
      "Average value: 35.72057040417592 for episode: 1382\n",
      "Average value: 35.734541883967125 for episode: 1383\n",
      "Average value: 36.29781478976877 for episode: 1384\n",
      "Average value: 36.48292405028033 for episode: 1385\n",
      "Average value: 36.408777847766316 for episode: 1386\n",
      "Average value: 36.688338955378 for episode: 1387\n",
      "Average value: 36.8539220076091 for episode: 1388\n",
      "Average value: 37.36122590722864 for episode: 1389\n",
      "Average value: 37.24316461186721 for episode: 1390\n",
      "Average value: 37.83100638127385 for episode: 1391\n",
      "Average value: 37.98945606221015 for episode: 1392\n",
      "Average value: 36.83998325909964 for episode: 1393\n",
      "Average value: 36.79798409614465 for episode: 1394\n",
      "Average value: 35.70808489133742 for episode: 1395\n",
      "Average value: 36.02268064677055 for episode: 1396\n",
      "Average value: 36.32154661443202 for episode: 1397\n",
      "Average value: 36.45546928371042 for episode: 1398\n",
      "Average value: 36.7326958195249 for episode: 1399\n",
      "Average value: 36.596061028548654 for episode: 1400\n",
      "Average value: 36.466257977121224 for episode: 1401\n",
      "Average value: 35.44294507826516 for episode: 1402\n",
      "Average value: 35.7207978243519 for episode: 1403\n",
      "Average value: 36.0847579331343 for episode: 1404\n",
      "Average value: 36.68052003647758 for episode: 1405\n",
      "Average value: 37.1964940346537 for episode: 1406\n",
      "Average value: 37.43666933292102 for episode: 1407\n",
      "Average value: 37.864835866274966 for episode: 1408\n",
      "Average value: 37.92159407296122 for episode: 1409\n",
      "Average value: 38.425514369313156 for episode: 1410\n",
      "Average value: 38.554238650847495 for episode: 1411\n",
      "Average value: 38.72652671830512 for episode: 1412\n",
      "Average value: 38.44020038238986 for episode: 1413\n",
      "Average value: 38.81819036327036 for episode: 1414\n",
      "Average value: 39.427280845106836 for episode: 1415\n",
      "Average value: 39.40591680285149 for episode: 1416\n",
      "Average value: 40.235620962708914 for episode: 1417\n",
      "Average value: 40.123839914573466 for episode: 1418\n",
      "Average value: 40.06764791884479 for episode: 1419\n",
      "Average value: 40.31426552290255 for episode: 1420\n",
      "Average value: 40.49855224675743 for episode: 1421\n",
      "Average value: 39.173624634419554 for episode: 1422\n",
      "Average value: 39.46494340269857 for episode: 1423\n",
      "Average value: 38.29169623256364 for episode: 1424\n",
      "Average value: 36.777111420935455 for episode: 1425\n",
      "Average value: 37.038255849888685 for episode: 1426\n",
      "Average value: 37.43634305739425 for episode: 1427\n",
      "Average value: 37.56452590452453 for episode: 1428\n",
      "Average value: 38.136299609298305 for episode: 1429\n",
      "Average value: 38.42948462883339 for episode: 1430\n",
      "Average value: 38.90801039739172 for episode: 1431\n",
      "Average value: 39.01260987752213 for episode: 1432\n",
      "Average value: 39.11197938364602 for episode: 1433\n",
      "Average value: 37.75638041446372 for episode: 1434\n",
      "Average value: 37.968561393740536 for episode: 1435\n",
      "Average value: 38.370133324053505 for episode: 1436\n",
      "Average value: 38.651626657850834 for episode: 1437\n",
      "Average value: 38.769045324958284 for episode: 1438\n",
      "Average value: 38.83059305871037 for episode: 1439\n",
      "Average value: 38.689063405774846 for episode: 1440\n",
      "Average value: 38.854610235486106 for episode: 1441\n",
      "Average value: 39.0118797237118 for episode: 1442\n",
      "Average value: 39.1112857375262 for episode: 1443\n",
      "Average value: 39.45572145064989 for episode: 1444\n",
      "Average value: 39.632935378117395 for episode: 1445\n",
      "Average value: 40.051288609211525 for episode: 1446\n",
      "Average value: 40.09872417875094 for episode: 1447\n",
      "Average value: 39.84378796981339 for episode: 1448\n",
      "Average value: 40.00159857132272 for episode: 1449\n",
      "Average value: 40.05151864275658 for episode: 1450\n",
      "Average value: 38.34894271061874 for episode: 1451\n",
      "Average value: 39.0814955750878 for episode: 1452\n",
      "Average value: 39.17742079633341 for episode: 1453\n",
      "Average value: 38.96854975651674 for episode: 1454\n",
      "Average value: 37.7201222686909 for episode: 1455\n",
      "Average value: 37.38411615525635 for episode: 1456\n",
      "Average value: 37.91491034749353 for episode: 1457\n",
      "Average value: 39.06916483011885 for episode: 1458\n",
      "Average value: 39.56570658861291 for episode: 1459\n",
      "Average value: 39.88742125918226 for episode: 1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:29:16,288] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.0.39825.video001800.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 39.94305019622314 for episode: 1461\n",
      "Average value: 40.145897686411985 for episode: 1462\n",
      "Average value: 40.53860280209138 for episode: 1463\n",
      "Average value: 38.861672661986816 for episode: 1464\n",
      "Average value: 37.31858902888747 for episode: 1465\n",
      "Average value: 37.3526595774431 for episode: 1466\n",
      "Average value: 41.48502659857094 for episode: 1467\n",
      "Average value: 41.66077526864239 for episode: 1468\n",
      "Average value: 41.92773650521027 for episode: 1469\n",
      "Average value: 42.031349679949756 for episode: 1470\n",
      "Average value: 42.179782195952264 for episode: 1471\n",
      "Average value: 42.32079308615465 for episode: 1472\n",
      "Average value: 42.55475343184692 for episode: 1473\n",
      "Average value: 42.37701576025457 for episode: 1474\n",
      "Average value: 42.50816497224184 for episode: 1475\n",
      "Average value: 42.532756723629745 for episode: 1476\n",
      "Average value: 42.60611888744826 for episode: 1477\n",
      "Average value: 42.575812943075846 for episode: 1478\n",
      "Average value: 42.74702229592205 for episode: 1479\n",
      "Average value: 42.60967118112595 for episode: 1480\n",
      "Average value: 42.479187622069645 for episode: 1481\n",
      "Average value: 42.65522824096616 for episode: 1482\n",
      "Average value: 42.92246682891785 for episode: 1483\n",
      "Average value: 42.72634348747196 for episode: 1484\n",
      "Average value: 43.04002631309836 for episode: 1485\n",
      "Average value: 41.13802499744344 for episode: 1486\n",
      "Average value: 40.98112374757126 for episode: 1487\n",
      "Average value: 41.18206756019269 for episode: 1488\n",
      "Average value: 41.77296418218305 for episode: 1489\n",
      "Average value: 41.9343159730739 for episode: 1490\n",
      "Average value: 42.2376001744202 for episode: 1491\n",
      "Average value: 40.575720165699195 for episode: 1492\n",
      "Average value: 41.09693415741423 for episode: 1493\n",
      "Average value: 41.39208744954352 for episode: 1494\n",
      "Average value: 55.872483077066335 for episode: 1495\n",
      "Average value: 53.428858923213014 for episode: 1496\n",
      "Average value: 100.75741597705236 for episode: 1497\n",
      "Average value: 98.31954517819973 for episode: 1498\n",
      "Average value: 95.30356791928975 for episode: 1499\n",
      "Average value: 91.03838952332525 for episode: 1500\n",
      "Average value: 136.486470047159 for episode: 1501\n",
      "Average value: 179.66214654480103 for episode: 1502\n",
      "Average value: 173.87903921756097 for episode: 1503\n",
      "Average value: 215.18508725668292 for episode: 1504\n",
      "Average value: 254.42583289384876 for episode: 1505\n",
      "Average value: 291.7045412491563 for episode: 1506\n",
      "Average value: 327.1193141866985 for episode: 1507\n",
      "Average value: 314.61334847736356 for episode: 1508\n",
      "Average value: 348.8826810534954 for episode: 1509\n",
      "Average value: 381.4385470008206 for episode: 1510\n",
      "Average value: 412.36661965077957 for episode: 1511\n",
      "Average value: 441.74828866824055 for episode: 1512\n",
      "Average value: 469.6608742348285 for episode: 1513\n",
      "Average value: 496.17783052308704 for episode: 1514\n",
      "Average value: 521.3689389969327 for episode: 1515\n"
     ]
    }
   ],
   "source": [
    "env = NormalizeAction(env) # remap action values for the environment\n",
    "avg_val = 0\n",
    "\n",
    "#for plotting\n",
    "running_rewards_ddpg = []\n",
    "step_list_ddpg = []\n",
    "step_counter = 0\n",
    "\n",
    "# set term_condition for early stopping according to environment being used\n",
    "# term_condition = -150 # Pendulum\n",
    "term_condition = 500 # inverted pendulum\n",
    "# term_condition = 1500 # halfcheetah \n",
    "ddpg.replayBuffer.initialize(1000, env)\n",
    "\n",
    "for itr in range(NUM_EPISODES):\n",
    "    state = env.reset() # get initial state\n",
    "    animate_this_episode = (itr % animate_interval == 0) and VISUALIZE\n",
    "    total_reward = 0\n",
    "    while True: # for each episode, we loop each step in this episode\n",
    "        ddpg.noise.reset()\n",
    "        if animate_this_episode:\n",
    "            env.render()\n",
    "            time.sleep(0.05)\n",
    "        # use actor to get action, add ddpg.noise.step() to action\n",
    "        # remember to put NN in eval mode while testing (to deal with BatchNorm layers) and put it back \n",
    "        # to train mode after you're done getting the action\n",
    "        var_state = Variable(torch.unsqueeze(FloatTensor(state),0), requires_grad=False)\n",
    "        ddpg.actor.eval()\n",
    "        cuda_tensor_action = ddpg.actor(var_state)\n",
    "        ddpg.actor.train()\n",
    "        \n",
    "        action = cuda_tensor_action.data[0].cpu().numpy()\n",
    "        \n",
    "        action = action + ddpg.noise.step()\n",
    "        \n",
    "        # below already include [-1,1] => [action_space.low, action_space.high]\n",
    "        new_state, reward, done, _ = env.step(action) \n",
    "        total_reward += reward\n",
    "        \n",
    "        ddpg.replayBuffer.push((state, action, new_state, reward, done))\n",
    "        # step action, get next state, reward, done (keep track of total_reward)\n",
    "        # populate ddpg.replayBuffer\n",
    "        ddpg.train() ###################### update network (per step) in one episode\n",
    "        step_counter += 1\n",
    "        state = new_state\n",
    "        if done: break\n",
    "\n",
    "    if avg_val > term_condition and itr >100 : break\n",
    "\n",
    "    running_rewards_ddpg.append(total_reward) # return of this episode\n",
    "    step_list_ddpg.append(step_counter)\n",
    "\n",
    "    avg_val = avg_val * 0.95 + 0.05*running_rewards_ddpg[-1]\n",
    "    print(\"Average value: {} for episode: {}\".format(avg_val,itr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rewards over multiple training runs \n",
    "This is provided to generate and plot results for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Inverted-Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 15:31:57,897] Starting new video recorder writing to /datasets/home/85/185/chs140/ECE276C/PA3/DDPG/openaigym.video.1.39825.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg)\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "env = NormalizeAction(env) # remap action values for the environment\n",
    "state = env.reset() # get initial state\n",
    "while True: # for each episode, we loop each step in this episode\n",
    "    ddpg.noise.reset()\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    # use actor to get action, add ddpg.noise.step() to action\n",
    "    # remember to put NN in eval mode while testing (to deal with BatchNorm layers) and put it back \n",
    "    # to train mode after you're done getting the action\n",
    "    var_state = Variable(torch.unsqueeze(FloatTensor(state),0), requires_grad=False)\n",
    "    ddpg.actor.eval()\n",
    "    cuda_tensor_action = ddpg.actor(var_state)\n",
    "    action = cuda_tensor_action.data[0].cpu().numpy()\n",
    "    action = action + ddpg.noise.step()\n",
    "    # below already include [-1,1] => [action_space.low, action_space.high]\n",
    "    new_state, reward, done, _ = env.step(action) \n",
    "    # step action, get next state, reward, done (keep track of total_reward)\n",
    "    # populate ddpg.replayBuffer\n",
    "    state = new_state\n",
    "    if done: break\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG HalfCheetah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "In this section you will implement REINFORCE, with modifications for batch training. It will be for use on both discrete and continous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Parametrization\n",
    "\n",
    "Define a MLP which outputs a distribution over the action preferences given input state. For the discrete case, the MLP outputs the likelihood of each action (softmax) while for the continuous case, the output is the mean and standard deviation parametrizing the normal distribution from which the action is sampled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Policy parametrizing model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 1 or 2 hidden layers with a small number of units per layer (similar to DQN)\n",
    "# use ReLU for hidden layer activations\n",
    "# softmax as activation for output if discrete actions, linear for continuous control\n",
    "# for the continuous case, output_dim=2*act_dim (each act_dim gets a mean and std_dev)\n",
    "\n",
    "class mlp(nn.Module):\n",
    "    # For discrete, it is the number of actions for outputs\n",
    "    # For continuous, it is the dimension of action\n",
    "    def __init__(self, Dim_state, num_outputs, disct):\n",
    "        super(mlp, self).__init__()\n",
    "        self.disct = disct\n",
    "        if self.disct == True:\n",
    "            self.fc1 = nn.Linear(Dim_state, 50)\n",
    "            self.fc2 = nn.Linear(50, 50)\n",
    "            self.fc3 = nn.Linear(50, num_outputs)\n",
    "            # parameters initialization\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "            nn.init.xavier_normal_(self.fc2.weight)\n",
    "            nn.init.xavier_normal_(self.fc3.weight)\n",
    "            nn.init.normal_(self.fc1.bias)\n",
    "            nn.init.normal_(self.fc2.bias)\n",
    "            nn.init.normal_(self.fc3.bias)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(Dim_state, 50)\n",
    "            self.fc2 = nn.Linear(50, 50)\n",
    "            self.fcmu_sigma = nn.Linear(50, 2*num_outputs)\n",
    "\n",
    "            \n",
    "            # parameters initialization\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "            nn.init.xavier_normal_(self.fc2.weight)\n",
    "            nn.init.xavier_normal_(self.fcmu_sigma.weight)\n",
    "            nn.init.normal_(self.fc1.bias)\n",
    "            nn.init.normal_(self.fc2.bias)\n",
    "            nn.init.normal_(self.fcmu_sigma.weight)       \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.disct == True:\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            actions_prob = F.softmax(x, dim=1)\n",
    "            return actions_prob\n",
    "        else: \n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            distribution_parameters = self.fcmu_sigma(x)\n",
    "            return distribution_parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that samples an action from the policy distribtion parameters obtained as output of the MLP. The function should return the action and the log-probability (log_odds) of taking that action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(logit, disct, num_outputs):\n",
    "    # logit is the output of the softmax/linear layer\n",
    "    # discrete is a flag for the environment type\n",
    "    # Hint: use Categorical and Normal from torch.distributions to sample action and get the log-probability\n",
    "    # Note that log_probability in this case translates to ln(\\pi(a|s)) \n",
    "    if disct == True:\n",
    "        action_distribution=torch.distributions.Categorical(logit)\n",
    "        action = action_distribution.sample()\n",
    "        log_odds = action_distribution.log_prob(action)\n",
    "        \n",
    "    else : # continuous \n",
    "        mean = distribution_parameters[0, 0:num_ouputs]\n",
    "        sigma = distribution_parameters[0, num_outputs:]\n",
    "        cov = torch.diag(sigma)\n",
    "        \n",
    "        action_distribution = torch.distributions.MultivariateNormal(mean, cov)\n",
    "        action = action_distribution.sample()\n",
    "        log_odds = action_distribution.log_prob(action)\n",
    "    return action, log_odds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function update_policy that defines the loss function and updates the MLP according to the REINFORCE update rule (ref. slide 24 of Lec 7 or page 330 of Sutton and Barto (2018)). The update algorithm to be used below is slightly different: instead of updating the network at every time-step, we take the gradient of the loss averaged over a batch of timesteps (this is to make SGD more stable). We also use a baseline to reduce variance. \n",
    "\n",
    "The discount factor is set as 1 here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(paths, net):\n",
    "    # paths: a list of paths (complete episodes, used to calculate return at each time step)\n",
    "    # net: MLP object\n",
    "    \n",
    "    num_paths = len(paths)\n",
    "    print(num_paths)\n",
    "    rew_cums = []\n",
    "    log_odds = []\n",
    "    # calculated as \"reward to go\"\n",
    "    def reward2go(rewards, gamma =1):\n",
    "        r2g = []\n",
    "        acc_r = 0\n",
    "        for r in reversed(rewards):\n",
    "            acc_r = acc_r * gamma + r\n",
    "            r2g.append(acc_r)\n",
    "        return r2g[::-1]\n",
    "    \n",
    "    for path in paths:\n",
    "        # rew_cums should record return at each time step for each path\n",
    "        rew_cums += reward2go(path['reward'])\n",
    "        # log_odds should record log_odds obtained at each timestep of path\n",
    "        log_odds += path['log_odds']\n",
    "        # calculated as \"reward to go\" \n",
    "\n",
    "    # make log_odds, rew_cums each a vector\n",
    "    rew_cums = np.array(rew_cums)\n",
    "    log_odds = np.array(log_odds)\n",
    "    rew_cums = (rew_cums - rew_cums.mean()) / (rew_cums.std() + 1e-5) # create baseline\n",
    "    # calculate policy loss and average over paths\n",
    "    policy_loss = -rew_cums.dot(log_odds)/ num_paths\n",
    "    \n",
    "    # take optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment and instantiate objects. Your algorithm is to be tested on one discrete and two continuous environments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-10 18:22:25,913] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp(\n",
      "  (fc1): Linear(in_features=4, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Select Environment\n",
    "\n",
    "#discrete environment:\n",
    "env_name='CartPole-v0'\n",
    "\n",
    "#continous environments:\n",
    "#env_name='InvertedPendulum-v2'\n",
    "#env_name = 'HalfCheetah-v2'\n",
    "\n",
    "# env_name='InvertedPendulum-v1'\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make(env_name)\n",
    "visualize = False\n",
    "animate=visualize\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "max_path_length=None\n",
    "min_timesteps_per_batch = 2000  # sets the batch size for updating network\n",
    "\n",
    "# Set random seeds\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Saving parameters\n",
    "logdir='./REINFORCE/'\n",
    "\n",
    "if visualize:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%animate_interval==0)\n",
    "\n",
    "env._max_episode_steps = min_timesteps_per_batch\n",
    "\n",
    "\n",
    "# Is this env continuous, or discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# Get observation and action space dimensions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "# Make network object (remember to pass in appropriate flags for the type of action space in use)\n",
    "# net = mlp(*args)\n",
    "net = mlp(Dim_state = obs_dim, num_outputs = act_dim, disct = discrete).type(FloatTensor)\n",
    "\n",
    "# Make optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run REINFORCE\n",
    "\n",
    "Run REINFORCE for CartPole, InvertedPendulum, and HalfCheetah. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "Average reward: 10.0\n",
      "206\n",
      "207\n",
      "205\n",
      "204\n",
      "200\n",
      "201\n",
      "201\n",
      "198\n",
      "196\n",
      "192\n",
      "190\n",
      "185\n",
      "180\n",
      "173\n",
      "174\n",
      "171\n",
      "169\n",
      "164\n",
      "148\n",
      "148\n",
      "152\n",
      "138\n",
      "126\n",
      "127\n",
      "110\n",
      "99\n",
      "90\n",
      "94\n",
      "88\n",
      "83\n",
      "81\n",
      "89\n",
      "89\n",
      "95\n",
      "95\n",
      "85\n",
      "91\n",
      "96\n",
      "81\n",
      "94\n",
      "76\n",
      "64\n",
      "68\n",
      "70\n",
      "68\n",
      "79\n",
      "73\n",
      "67\n",
      "69\n",
      "70\n",
      "73\n",
      "60\n",
      "62\n",
      "61\n",
      "59\n",
      "62\n",
      "52\n",
      "55\n",
      "60\n",
      "60\n",
      "55\n",
      "54\n",
      "54\n",
      "49\n",
      "50\n",
      "58\n",
      "50\n",
      "51\n",
      "48\n",
      "48\n",
      "54\n",
      "49\n",
      "50\n",
      "52\n",
      "48\n",
      "46\n",
      "47\n",
      "49\n",
      "44\n",
      "42\n",
      "43\n",
      "41\n",
      "37\n",
      "42\n",
      "41\n",
      "41\n",
      "41\n",
      "41\n",
      "37\n",
      "40\n",
      "47\n",
      "39\n",
      "38\n",
      "35\n",
      "40\n",
      "31\n",
      "38\n",
      "37\n",
      "35\n",
      "30\n",
      "Average reward: 56.85572898982444\n",
      "33\n",
      "35\n",
      "39\n",
      "29\n",
      "32\n",
      "32\n",
      "33\n",
      "29\n",
      "32\n",
      "30\n",
      "26\n",
      "23\n",
      "26\n",
      "27\n",
      "28\n",
      "26\n",
      "20\n",
      "27\n",
      "23\n",
      "19\n",
      "23\n",
      "17\n",
      "22\n",
      "23\n",
      "19\n",
      "21\n",
      "16\n",
      "19\n",
      "15\n",
      "23\n",
      "17\n",
      "17\n",
      "14\n",
      "20\n",
      "17\n",
      "18\n",
      "15\n",
      "15\n",
      "14\n",
      "15\n",
      "13\n",
      "16\n",
      "15\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "14\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "11\n",
      "11\n",
      "12\n",
      "13\n",
      "12\n",
      "13\n",
      "12\n",
      "14\n",
      "14\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "Average reward: 189.1320869297605\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "Average reward: 193.12183447158304\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "Average reward: 199.36961517791931\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "13\n",
      "11\n",
      "12\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-61b0ef0ae2a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mvar_ob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mdistribution_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_ob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# sample action and get log-probability (log_odds) from distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-763623e342a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisct\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 1000 \n",
    "min_timesteps_per_batch = 2000  # sets the batch size for updating network\n",
    "avg_reward = 0\n",
    "avg_rewards = []\n",
    "step_list_reinforce = []\n",
    "total_steps = 0\n",
    "episodes = 0\n",
    "\n",
    "for itr in range(n_iter): # loop for number of optimization steps\n",
    "    paths = []\n",
    "    steps = 0\n",
    "    while True: # loop to get enough timesteps in this batch --> if episode ends this loop will restart till steps reaches limit\n",
    "        ob = env.reset()   \n",
    "        animate_this_episode = (itr % animate_interval == 0) and VISUALIZE\n",
    "        obs, acs, rews, log_odds = [], [], [], [] \n",
    "        obs.append(ob)\n",
    "\n",
    "        while True: # loop for episode inside batch\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            # get parametrized policy distribution from net using current state ob\n",
    "            net.eval()\n",
    "            var_ob = Variable(torch.unsqueeze(FloatTensor(ob),0), requires_grad=False)\n",
    "            distribution_parameters = net(var_ob)\n",
    "            net.train()\n",
    "            # sample action and get log-probability (log_odds) from distribution\n",
    "            cuda_tensor_ac, log_odd= sample_action\n",
    "            (logit = distribution_parameters , disct = discrete, num_outputs = act_dim)\n",
    "            ac = cuda_tensor_ac.data[0].cpu().numpy()\n",
    "            # step environment, record reward, next state\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            # append to obs, acs, rewards, log_odds\n",
    "            obs.append(ob)\n",
    "            acs.append(ac)\n",
    "            rews.append(rew)\n",
    "            log_odds.append(log_odd)\n",
    "            \n",
    "            # if done, restart episode till min_timesteps_per_batch is reached     \n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                episodes = episodes + 1\n",
    "                break\n",
    "                \n",
    "        path = {\"observation\" : obs, \n",
    "                \"reward\" : np.array(rews), \n",
    "                \"action\" : (acs),\n",
    "                \"log_odds\" : log_odds}\n",
    "        \n",
    "        paths.append(path)\n",
    "        if steps > min_timesteps_per_batch: break \n",
    "\n",
    "    update_policy(paths, net)  # use all complete episodes (a batch of timesteps) recorded in this itr to update net\n",
    "    if itr == 0: avg_reward = path['reward'].sum()\n",
    "        \n",
    "    else: avg_reward = avg_reward * 0.95 + 0.05 * path['reward'].sum()\n",
    "        \n",
    "    if avg_reward > 300: break\n",
    "    total_steps += steps\n",
    "    avg_rewards.append(avg_reward)\n",
    "    step_list_reinforce.append(total_steps)\n",
    "    if itr % logging_interval == 0: print('Average reward: {}'.format(avg_reward))   \n",
    "        \n",
    "env.close()\n",
    "\n",
    "plt.plot(avg_rewards)\n",
    "plt.title('Training reward for <env> over multiple runs ')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average reward')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_rewards)\n",
    "plt.title('Training reward for <env> over multiple runs ')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS (15% extra)\n",
    "\n",
    "Compare average returns for CartPole (discrete action space) when using REINFORCE and DQN. Since in REINFORCE we update the network after a set number of steps instead of after every episode, plot the average rewards as a function of steps rather than episodes for both DQN and REINFORCE. You will need to make minor edits to your DQN code from the previous assignment to record average returns as a function of time_steps.\n",
    "\n",
    "Similarly, compare REINFORCE with DDPG on InvertedPendulum and HalfCheetah using steps for the x-axis.\n",
    "\n",
    "You may use the example code provided below as a reference for the graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 DQN environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-10 20:13:03,308] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    " # import your DQN and format your average returns as defined above\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create the CartPole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "\n",
    "class Net(nn.Module):\n",
    "# Define your network here\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.tanh(x)\n",
    "        Qs_actions = self.out(x) # Q value for one state, at different actions\n",
    "        return Qs_actions\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate, state_size, action_size, hidden_size, alpha_decay):\n",
    "        self.LR = learning_rate\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.model = Net(self.state_size, self.action_size, self.hidden_size)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.LR)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def learn(self, batch_Q_behavior, batch_Q_target):\n",
    "        loss = self.criterion(batch_Q_behavior, batch_Q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DQN replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay():\n",
    "    def __init__(self, max_size):\n",
    "        self.capacity = max_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def initialize(self, init_length, envir):\n",
    "        st = env.reset()\n",
    "        for _ in range(init_length):\n",
    "            a = np.random.randint(2, size=1)\n",
    "            st1, r, done, info = env.step(int(a))\n",
    "            self.push((st, a, st1, r, done))\n",
    "            if done: st = env.reset()\n",
    "            else : st = st1\n",
    "            \n",
    "    def push(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def generate_minibatch(self, DQN, targetDQN, batch_size):\n",
    "        \n",
    "        batch_memory = random.sample(self.memory, batch_size) #return a list\n",
    "        batch_memory = list(zip(*batch_memory))\n",
    "        \n",
    "        batch_st = Variable(FloatTensor(batch_memory[0]))\n",
    "        batch_at = Variable(LongTensor(batch_memory[1]))\n",
    "        batch_st1 = Variable(FloatTensor(batch_memory[2]))\n",
    "        batch_r = Variable(torch.unsqueeze(FloatTensor(batch_memory[3]),1))\n",
    "        batch_done = FloatTensor(batch_memory[4])\n",
    "\n",
    "        batch_Q_behavior = DQN.model(batch_st).gather(1, batch_at)\n",
    "        mask = 1. - batch_done\n",
    "        batch_Q_next = targetDQN.model(batch_st1).detach()\n",
    "        \n",
    "        QQ_next = Variable((batch_Q_next.max(1)[0].data * mask).view(batch_size, 1))\n",
    "        batch_Q_target = batch_r + self.gamma*(QQ_next)\n",
    "        return batch_Q_behavior, batch_Q_target\n",
    "         \n",
    "    def __len__(self):            \n",
    "        return len(self.memory)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 DQN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run gpu !\n",
      "Average reward: 37.252340002014066\n",
      "Average reward: 90.21662392735004\n",
      "Average reward: 136.0911559073801\n",
      "Average reward: 114.03852854346641\n",
      "Average reward: 80.83668077454597\n",
      "Average reward: 191.47600351186938\n",
      "Average reward: 199.9253710251429\n",
      "Average reward: 199.41618203324447\n",
      "Average reward: 199.93978452885656\n",
      "Average reward: 197.51373584803193\n",
      "Average reward: 199.9852800004386\n",
      "Average reward: 199.9999128498123\n",
      "Average reward: 199.99999948402458\n",
      "Average reward: 199.99999999694498\n",
      "Average reward: 199.38039046222713\n",
      "Average reward: 199.99633158362627\n",
      "Average reward: 199.91444392629492\n",
      "Average reward: 199.99949346276549\n",
      "Average reward: 199.5507878929079\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01 \n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "hidden_size = 64\n",
    "alpha_decay = 0.1\n",
    "batch_size = 500\n",
    "\n",
    "DQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "targetDQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "# set targetDQN weights to DQN weights\n",
    "# for ex. targetDQN.model.weights = DQN.model.weights (syntax given here is for representation purpose only)\n",
    "targetDQN.model.load_state_dict(DQN.model.state_dict())\n",
    "replay = Replay(max_size=10000) ## Initialize Replay Buffer\n",
    "replay.initialize(init_length=1000, envir=env) ## Populate the initial experience buffer\n",
    "if use_cuda:\n",
    "    print('run gpu !')\n",
    "    targetDQN.model.cuda()\n",
    "    DQN.model.cuda()\n",
    "else: \n",
    "    print('gpu not activited !')\n",
    "    \n",
    "# Runtime parameters\n",
    "num_episodes = 2000            # max number of episodes to learn from\n",
    "gamma = 0.99                   # future reward discount\n",
    "max_steps = 500                # cut off simulation after this many steps\n",
    "# Exploration parameters\n",
    "min_epsilon = 0.01             # minimum exploration probability\n",
    "decay_rate = 5/num_episodes    # exponential decay rate for exploration prob\n",
    "returns = np.zeros(num_episodes)\n",
    "step_list_DQN = []\n",
    "total_steps = 0\n",
    "avg_reward = 0\n",
    "avg_rewards = []\n",
    "logging_interval = 100\n",
    "\n",
    "\n",
    "for ep in range(1, num_episodes): # ep now is for one iteration\n",
    "    paths = []\n",
    "    steps = 0\n",
    "    while True: #  paths = a number of episode, but restricted by step> 2000 break\n",
    "        total_reward = 0\n",
    "        epsilon = min_epsilon + (1.0 - min_epsilon)*np.exp(-decay_rate*ep)\n",
    "    # --> start episode\n",
    "        state = env.reset()\n",
    "        rews = []\n",
    "        for step in range(max_steps): # path = one episode\n",
    "            # generate the steps in each episode \n",
    "            # explore/exploit and get action using DQN\n",
    "            if random.random()<= epsilon:\n",
    "                action = np.random.randint(2, size=1)\n",
    "            else:\n",
    "                var_state = Variable(torch.unsqueeze(FloatTensor(state),0))# here change the (4,) to (1,4) in variable \n",
    "                DQN.model.eval()\n",
    "                Qs_actions = DQN.model.forward(var_state) # shape of (1, 2) variable\n",
    "                DQN.model.train()\n",
    "                cuda_tensor_action = torch.max(Qs_actions,1)[1].data\n",
    "                action = cuda_tensor_action.cpu().numpy()\n",
    "\n",
    "            new_state, reward, done, _ = env.step(int(action))\n",
    "            rews.append(reward)\n",
    "            replay.push((state, action, new_state, reward, done))\n",
    "            steps += 1\n",
    "        # perform action and record new_state, action, reward\n",
    "        # populate Replay experience buffer \n",
    "            if done:  break\n",
    "            else: state = new_state \n",
    "        # <-- end episode\n",
    "        path={'reward':np.array(rews)}\n",
    "        paths.append(path)\n",
    "        if steps > 2000: break\n",
    "    \n",
    "    batch_Q_behavior, batch_Q_target = replay.generate_minibatch(DQN, targetDQN, batch_size) #outputs and targets\n",
    "    DQN.learn(batch_Q_behavior, batch_Q_target) \n",
    "    targetDQN.model.load_state_dict(DQN.model.state_dict())\n",
    " \n",
    "    avg_reward = avg_reward * 0.95 + 0.05 * path['reward'].sum()\n",
    "    total_steps += steps\n",
    "    avg_rewards.append(avg_reward)\n",
    "    step_list_DQN.append(total_steps)\n",
    "    if ep % logging_interval == 0: print('Average reward: {}'.format(avg_reward)) \n",
    "print('finished training')\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average reward')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_rewards)\n",
    "plt.title('Training reward for <env> over multiple runs ')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 DQN post possing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "plt.plot(step_list_ddpg, out) # or plt.plot(step_list_DQN, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.legend(['DDPG', 'REINFORCE']) #or plt.legend(['DQN', 'REINFORCE'])\n",
    "plt.plot(step_list_reinforce, avg_rewards)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
