{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-04 22:22:27,933] Making new env: InvertedPendulum-v1\n",
      "[2018-06-04 22:22:28,482] Making new env: InvertedPendulum-v1\n",
      "[2018-06-04 22:22:28,500] Making new env: InvertedPendulum-v1\n",
      "[2018-06-04 22:22:28,514] Making new env: InvertedPendulum-v1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "#environment\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "#pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "import copy\n",
    "\n",
    "\n",
    "logging_interval = 40\n",
    "animate_interval = logging_interval * 5\n",
    "logdir='./DDPG/'\n",
    "VISUALIZE = False\n",
    "SEED = 0\n",
    "MAX_PATH_LENGTH = 500\n",
    "NUM_EPISODES = 12000\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Environments to be tested on\n",
    "env_name = 'InvertedPendulum-v1'\n",
    "#env_name = 'Pendulum-v0'\n",
    "#env_name = 'HalfCheetah-v1' \n",
    "#env_name = 'MountainCarContinuous-v0'\n",
    "#env_name = 'Humanoid-v1'\n",
    "#env_name = 'Ant-v1'\n",
    "\n",
    "\n",
    "\n",
    "# wrap gym to save videos\n",
    "env = gym.make(env_name)\n",
    "env1 = gym.make(env_name)\n",
    "env2 = gym.make(env_name)\n",
    "env3 = gym.make(env_name)\n",
    "\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env1 = gym.wrappers.Monitor(env1, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "env1._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "# check observation and action space\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    print(\"This is a discrete action space, probably not the right algorithm to use\")\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAction(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        action = (action + 1) / 2  \n",
    "        action *= (self.action_space.high - self.action_space.low)\n",
    "        action += self.action_space.low\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        action -= self.action_space.low\n",
    "        action /= (self.action_space.high - self.action_space.low)\n",
    "        a\n",
    "        ction = action * 2 - 1\n",
    "        return actions\n",
    "\n",
    "def weightSync(target_model, source_model, tau = 0.001):\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)\n",
    "        \n",
    "class OrnsteinUhlenbeckProcess:\n",
    "    def __init__(self, mu=np.zeros(act_dim), sigma=0.05, theta=.25, dimension=1e-2, x0=None,num_steps=12000):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dimension\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def step(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveParamNoiseSpec(object):\n",
    "    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.2, adaptation_coefficient=1.01):\n",
    "        \"\"\"\n",
    "        Note that initial_stddev and current_stddev refer to std of parameter noise, \n",
    "        but desired_action_stddev refers to (as name notes) desired std in action space\n",
    "        \"\"\"\n",
    "        self.initial_stddev = initial_stddev\n",
    "        self.desired_action_stddev = desired_action_stddev\n",
    "        self.adaptation_coefficient = adaptation_coefficient\n",
    "\n",
    "        self.current_stddev = initial_stddev\n",
    "\n",
    "    def adapt(self, distance):\n",
    "        if distance > self.desired_action_stddev:\n",
    "            # Decrease stddev.\n",
    "            self.current_stddev /= self.adaptation_coefficient\n",
    "        else:\n",
    "            # Increase stddev.\n",
    "            self.current_stddev *= self.adaptation_coefficient\n",
    "\n",
    "    def get_stats(self):\n",
    "        stats = {\n",
    "            'param_noise_stddev': self.current_stddev,\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt = 'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adaptation_coefficient={})'\n",
    "        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adaptation_coefficient)\n",
    "\n",
    "def ddpg_distance_metric(actions1, actions2):\n",
    "    \"\"\"\n",
    "    Compute \"distance\" between actions taken by two policies at the same states\n",
    "    Expects numpy arrays\n",
    "    \"\"\"\n",
    "    diff = actions1-actions2\n",
    "    mean_diff = np.mean(np.square(diff), axis=0)\n",
    "    dist = sqrt(np.mean(mean_diff))\n",
    "    return dist\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "           target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Replay(object):\n",
    "    def __init__(self, maxlen = 60000):\n",
    "        self.maxlen = maxlen\n",
    "        #self.data = deque(maxlen = self.maxlen)\n",
    "        self.data=[]\n",
    "        self.position=0\n",
    "#         self.initialize(init_length=1000, envir=env)\n",
    "        \n",
    "    def initialize(self,init_length=1000, envir=env):\n",
    "        s = envir.reset()\n",
    "        for i in range (init_length):\n",
    "            #a = np.random.random(1)-np.random.random(1)\n",
    "            a = env.action_space.sample()\n",
    "            s1, r, done, _ = env.step(a)            \n",
    "            self.add((s,a,r,s1,done))\n",
    "                        \n",
    "            if done:\n",
    "                s=envir.reset()\n",
    "            else: \n",
    "                s = s1  \n",
    "              \n",
    "    def add(self, ep):\n",
    "        self.data.append(ep)\n",
    "        self.position = (self.position + 1) % self.maxlen       \n",
    "        #self.data[self.position] = tuple(ep)\n",
    "        \n",
    "    def sample(self, batch_size):     \n",
    "        return random.sample(self.data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, output_size = 1):\n",
    "        super(critic, self).__init__()\n",
    "\n",
    "        self.state_dim = state_size\n",
    "        self.action_dim = action_size\n",
    "        h1_dim = 300\n",
    "        h2_dim = 300\n",
    "        self.fc1 = nn.Linear(self.state_dim, h1_dim)\n",
    "        #self.bn1 = nn.BatchNorm1d(h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim + self.action_dim, h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim, output_size)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        #s1 = F.relu(self.bn1(self.fc1(state)))\n",
    "        s1 = F.relu(self.fc1(state))\n",
    "        x = torch.cat((s1,action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class actor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(actor, self).__init__()\n",
    "\n",
    "        self.state_dim = input_size\n",
    "        self.action_dim = output_size\n",
    "        h1_dim = 400\n",
    "        h2_dim = 400\n",
    "        self.fc1 = nn.Linear(self.state_dim, h1_dim)\n",
    "        #self.bn1 = nn.BatchNorm1d(h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim, h2_dim)\n",
    "        #self.bn2 = nn.BatchNorm1d(h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim, self.action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        #x = F.relu(self.bn1(self.fc1(state)))\n",
    "        #x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action = F.tanh(self.fc3(x))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, obs_dim, act_dim, critic_lr = 1e-3, actor_lr = 1e-4, gamma = GAMMA, batch_size = BATCH_SIZE):\n",
    "        \n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        \n",
    "        # actor\n",
    "        self.actor = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_perturbed = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # critic\n",
    "        self.critic = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.actor.cuda()\n",
    "            self.actor_target.cuda()\n",
    "            self.critic.cuda()\n",
    "            self.critic_target.cuda()\n",
    "            \n",
    "        # optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr = critic_lr, weight_decay=1e-2)\n",
    "        \n",
    "        # critic loss\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        # noise\n",
    "        self.noise = OrnsteinUhlenbeckProcess(dimension = act_dim, num_steps = NUM_EPISODES)\n",
    "    \n",
    "    def action(self, s, noise, para):\n",
    "        obs = torch.from_numpy(s).unsqueeze(0)\n",
    "        inp = Variable(obs,requires_grad=False).type(FloatTensor)\n",
    "\n",
    "        self.actor.eval()\n",
    "        self.actor_perturbed.eval()\n",
    "\n",
    "        if para is not None:\n",
    "            a = self.actor_perturbed(inp).data[0].cpu().numpy() \n",
    "        else:\n",
    "            a = self.actor(inp).data[0].cpu().numpy() \n",
    "        self.actor.train()\n",
    "\n",
    "        if noise is not None:\n",
    "            a = a + noise\n",
    "        return a\n",
    "    \n",
    "    def perturb_actor_parameters(self, param_noise):\n",
    "        \"\"\"Apply parameter noise to actor model, for exploration\"\"\"\n",
    "        hard_update(self.actor_perturbed, self.actor)\n",
    "        params = self.actor_perturbed.state_dict()\n",
    "        for name in params:\n",
    "            if 'ln' in name: \n",
    "                pass \n",
    "            param = params[name]\n",
    "            random = torch.randn(param.shape)\n",
    "            if use_cuda:\n",
    "                random = random.cuda()\n",
    "            param += random * param_noise.current_stddev\n",
    "            \n",
    "    def train(self, training_data):\n",
    "        batch_s,batch_a,batch_r,batch_s1,batch_done=zip(*training_data)\n",
    "        s1 = Variable(FloatTensor(batch_s))\n",
    "        a1 = Variable(FloatTensor(batch_a))\n",
    "        r1 = Variable(FloatTensor(np.array(batch_r).reshape(-1,1)))\n",
    "        s2 = Variable(FloatTensor(batch_s1))\n",
    "        d  = Variable(FloatTensor(1.0*np.array(batch_done).reshape(-1,1)))\n",
    "        \n",
    "        a2 = self.actor_target(s2)\n",
    "        # ---------------------- optimize critic ----------------------\n",
    "        next_val = self.critic_target(s2, a2).detach()\n",
    "        q_expected = r1 + self.gamma*next_val*(1.0-d)\n",
    "        q_predicted = self.critic(s1, a1)\n",
    "        \n",
    "        # compute critic loss, and update the critic\n",
    "        loss_critic = self.critic_loss(q_predicted, q_expected)\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        # ---------------------- optimize actor ----------------------\n",
    "        pred_a1 = self.actor.forward(s1)\n",
    "        loss_actor = -1*self.critic(s1, pred_a1)\n",
    "        loss_actor = loss_actor.mean()\n",
    "        \n",
    "        self.optimizer_actor.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        # sychronize target network with fast moving one\n",
    "        weightSync(self.critic_target, self.critic)\n",
    "        weightSync(self.actor_target, self.actor)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg1 = DDPG(obs_dim = obs_dim, act_dim = act_dim)\n",
    "ddpg2 = DDPG(obs_dim = obs_dim, act_dim = act_dim)\n",
    "ddpg3 = DDPG(obs_dim = obs_dim, act_dim = act_dim)\n",
    "memory = Replay(60000)\n",
    "memory.initialize(init_length=150, envir=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 0.30000000000000004 for episode: 0\n",
      "Average value: 0.935 for episode: 1\n",
      "Average value: 1.43825 for episode: 2\n",
      "Average value: 1.5663375 for episode: 3\n",
      "Average value: 1.6380206249999998 for episode: 4\n",
      "Average value: 1.7061195937499996 for episode: 5\n",
      "Average value: 1.7708136140624995 for episode: 6\n",
      "Average value: 1.8322729333593744 for episode: 7\n",
      "Average value: 1.8906592866914056 for episode: 8\n",
      "Average value: 1.9461263223568355 for episode: 9\n",
      "Average value: 1.9988200062389936 for episode: 10\n",
      "Average value: 2.0488790059270436 for episode: 11\n",
      "Average value: 2.096435055630691 for episode: 12\n",
      "Average value: 2.1416133028491564 for episode: 13\n",
      "Average value: 2.1845326377066985 for episode: 14\n",
      "Average value: 2.225306005821363 for episode: 15\n",
      "Average value: 2.2640407055302947 for episode: 16\n",
      "Average value: 2.3008386702537797 for episode: 17\n",
      "Average value: 2.3357967367410906 for episode: 18\n",
      "Average value: 2.369006899904036 for episode: 19\n",
      "Average value: 2.400556554908834 for episode: 20\n",
      "Average value: 2.530528727163392 for episode: 21\n",
      "Average value: 2.5540022908052222 for episode: 22\n",
      "Average value: 2.576302176264961 for episode: 23\n",
      "Average value: 2.5974870674517128 for episode: 24\n",
      "Average value: 2.767612714079127 for episode: 25\n",
      "Average value: 2.7792320783751707 for episode: 26\n",
      "Average value: 2.8402704744564122 for episode: 27\n",
      "Average value: 3.0482569507335917 for episode: 28\n",
      "Average value: 3.095844103196912 for episode: 29\n",
      "Average value: 3.0910518980370663 for episode: 30\n",
      "Average value: 3.086499303135213 for episode: 31\n",
      "Average value: 3.082174337978452 for episode: 32\n",
      "Average value: 3.1280656210795295 for episode: 33\n",
      "Average value: 3.1216623400255528 for episode: 34\n",
      "Average value: 3.115579223024275 for episode: 35\n",
      "Average value: 3.109800261873061 for episode: 36\n",
      "Average value: 3.1043102487794076 for episode: 37\n",
      "Average value: 3.099094736340437 for episode: 38\n",
      "Average value: 3.094139999523415 for episode: 39\n",
      "Average value: 3.089432999547244 for episode: 40\n",
      "Average value: 3.5849613495698818 for episode: 41\n",
      "Average value: 3.5557132820913875 for episode: 42\n",
      "Average value: 3.5279276179868178 for episode: 43\n",
      "Average value: 3.601531237087477 for episode: 44\n",
      "Average value: 3.571454675233103 for episode: 45\n",
      "Average value: 3.5428819414714474 for episode: 46\n",
      "Average value: 3.515737844397875 for episode: 47\n",
      "Average value: 3.489950952177981 for episode: 48\n",
      "Average value: 3.4654534045690815 for episode: 49\n",
      "Average value: 3.442180734340627 for episode: 50\n",
      "Average value: 3.4200716976235954 for episode: 51\n",
      "Average value: 3.3990681127424156 for episode: 52\n",
      "Average value: 3.3791147071052947 for episode: 53\n",
      "Average value: 3.3601589717500295 for episode: 54\n",
      "Average value: 3.3421510231625278 for episode: 55\n",
      "Average value: 3.5750434720044013 for episode: 56\n",
      "Average value: 3.5962912984041813 for episode: 57\n",
      "Average value: 3.566476733483972 for episode: 58\n",
      "Average value: 3.8381528968097736 for episode: 59\n",
      "Average value: 3.7962452519692844 for episode: 60\n",
      "Average value: 3.8064329893708204 for episode: 61\n",
      "Average value: 3.766111339902279 for episode: 62\n",
      "Average value: 3.7778057729071652 for episode: 63\n",
      "Average value: 3.688915484261807 for episode: 64\n",
      "Average value: 3.654469710048716 for episode: 65\n",
      "Average value: 3.62174622454628 for episode: 66\n",
      "Average value: 3.5906589133189657 for episode: 67\n",
      "Average value: 3.5611259676530174 for episode: 68\n",
      "Average value: 3.983069669270366 for episode: 69\n",
      "Average value: 3.9339161858068477 for episode: 70\n",
      "Average value: 3.9372203765165055 for episode: 71\n",
      "Average value: 3.89035935769068 for episode: 72\n",
      "Average value: 3.8458413898061456 for episode: 73\n",
      "Average value: 3.803549320315838 for episode: 74\n",
      "Average value: 3.7133718543000462 for episode: 75\n",
      "Average value: 3.9277032615850436 for episode: 76\n",
      "Average value: 3.8813180985057913 for episode: 77\n",
      "Average value: 3.8372521935805017 for episode: 78\n",
      "Average value: 3.8953895839014763 for episode: 79\n",
      "Average value: 3.8506201047064024 for episode: 80\n",
      "Average value: 3.808089099471082 for episode: 81\n",
      "Average value: 3.7676846444975274 for episode: 82\n",
      "Average value: 3.729300412272651 for episode: 83\n",
      "Average value: 4.042835391659018 for episode: 84\n",
      "Average value: 4.090693622076067 for episode: 85\n",
      "Average value: 4.036158940972264 for episode: 86\n",
      "Average value: 3.9843509939236506 for episode: 87\n",
      "Average value: 3.885133444227468 for episode: 88\n",
      "Average value: 3.8408767720160943 for episode: 89\n",
      "Average value: 3.7988329334152895 for episode: 90\n",
      "Average value: 3.7588912867445248 for episode: 91\n",
      "Average value: 3.720946722407298 for episode: 92\n",
      "Average value: 3.734899386286933 for episode: 93\n",
      "Average value: 3.798154416972586 for episode: 94\n",
      "Average value: 3.7582466961239565 for episode: 95\n",
      "Average value: 3.8203343613177583 for episode: 96\n",
      "Average value: 4.27931764325187 for episode: 97\n",
      "Average value: 4.265351761089277 for episode: 98\n",
      "Average value: 4.352084173034813 for episode: 99\n",
      "Average value: 4.284479964383072 for episode: 100\n",
      "Average value: 4.2202559661639185 for episode: 101\n",
      "Average value: 4.709243167855723 for episode: 102\n",
      "Average value: 4.623781009462937 for episode: 103\n",
      "Average value: 4.49259195898979 for episode: 104\n",
      "Average value: 4.617962361040299 for episode: 105\n",
      "Average value: 4.537064242988285 for episode: 106\n",
      "Average value: 4.46021103083887 for episode: 107\n",
      "Average value: 4.387200479296927 for episode: 108\n",
      "Average value: 4.567840455332081 for episode: 109\n",
      "Average value: 4.489448432565477 for episode: 110\n",
      "Average value: 4.364976010937203 for episode: 111\n",
      "Average value: 4.296727210390343 for episode: 112\n",
      "Average value: 4.231890849870826 for episode: 113\n",
      "Average value: 4.320296307377284 for episode: 114\n",
      "Average value: 4.25428149200842 for episode: 115\n",
      "Average value: 4.191567417408 for episode: 116\n",
      "Average value: 4.3819890465375995 for episode: 117\n",
      "Average value: 4.31288959421072 for episode: 118\n",
      "Average value: 4.447245114500183 for episode: 119\n",
      "Average value: 4.374882858775174 for episode: 120\n",
      "Average value: 4.906138715836415 for episode: 121\n",
      "Average value: 4.760831780044594 for episode: 122\n",
      "Average value: 4.672790191042364 for episode: 123\n",
      "Average value: 4.639150681490246 for episode: 124\n",
      "Average value: 5.607193147415734 for episode: 125\n",
      "Average value: 5.476833490044948 for episode: 126\n",
      "Average value: 5.3529918155427 for episode: 127\n",
      "Average value: 5.285342224765565 for episode: 128\n",
      "Average value: 5.171075113527287 for episode: 129\n",
      "Average value: 5.062521357850923 for episode: 130\n",
      "Average value: 5.059395289958377 for episode: 131\n",
      "Average value: 4.956425525460458 for episode: 132\n",
      "Average value: 5.4586042491874345 for episode: 133\n",
      "Average value: 5.335674036728063 for episode: 134\n",
      "Average value: 5.26889033489166 for episode: 135\n",
      "Average value: 5.4054458181470775 for episode: 136\n",
      "Average value: 5.385173527239723 for episode: 137\n",
      "Average value: 13.515914850877737 for episode: 138\n",
      "Average value: 12.99011910833385 for episode: 139\n",
      "Average value: 12.490613152917158 for episode: 140\n",
      "Average value: 12.0160824952713 for episode: 141\n",
      "Average value: 11.565278370507736 for episode: 142\n",
      "Average value: 11.237014451982349 for episode: 143\n",
      "Average value: 10.825163729383231 for episode: 144\n",
      "Average value: 10.433905542914069 for episode: 145\n",
      "Average value: 10.012210265768365 for episode: 146\n",
      "Average value: 9.711599752479946 for episode: 147\n",
      "Average value: 9.626019764855949 for episode: 148\n",
      "Average value: 9.294718776613152 for episode: 149\n",
      "Average value: 8.979982837782494 for episode: 150\n",
      "Average value: 8.730983695893368 for episode: 151\n",
      "Average value: 8.444434511098699 for episode: 152\n",
      "Average value: 8.172212785543763 for episode: 153\n",
      "Average value: 8.213602146266574 for episode: 154\n",
      "Average value: 7.952922038953245 for episode: 155\n",
      "Average value: 7.805275937005582 for episode: 156\n",
      "Average value: 7.715012140155303 for episode: 157\n",
      "Average value: 7.479261533147538 for episode: 158\n",
      "Average value: 7.305298456490161 for episode: 159\n",
      "Average value: 7.2400335336656525 for episode: 160\n",
      "Average value: 7.02803185698237 for episode: 161\n",
      "Average value: 7.076630264133251 for episode: 162\n",
      "Average value: 7.572798750926589 for episode: 163\n",
      "Average value: 7.344158813380259 for episode: 164\n",
      "Average value: 7.276950872711246 for episode: 165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 7.063103329075683 for episode: 166\n",
      "Average value: 6.909948162621899 for episode: 167\n",
      "Average value: 7.864450754490804 for episode: 168\n",
      "Average value: 7.671228216766264 for episode: 169\n",
      "Average value: 8.03766680592795 for episode: 170\n",
      "Average value: 8.985783465631553 for episode: 171\n",
      "Average value: 9.986494292349974 for episode: 172\n",
      "Average value: 10.687169577732476 for episode: 173\n",
      "Average value: 16.002811098845854 for episode: 174\n",
      "Average value: 15.65267054390356 for episode: 175\n",
      "Average value: 16.42003701670838 for episode: 176\n",
      "Average value: 19.099035165872962 for episode: 177\n",
      "Average value: 18.54408340757931 for episode: 178\n",
      "Average value: 18.166879237200344 for episode: 179\n",
      "Average value: 18.558535275340326 for episode: 180\n",
      "Average value: 18.380608511573307 for episode: 181\n",
      "Average value: 17.86157808599464 for episode: 182\n",
      "Average value: 17.41849918169491 for episode: 183\n",
      "Average value: 17.397574222610164 for episode: 184\n",
      "Average value: 17.377695511479658 for episode: 185\n",
      "Average value: 17.708810735905672 for episode: 186\n",
      "Average value: 17.62337019911039 for episode: 187\n",
      "Average value: 22.79220168915487 for episode: 188\n",
      "Average value: 25.152591604697125 for episode: 189\n",
      "Average value: 26.594962024462266 for episode: 190\n",
      "Average value: 26.21521392323915 for episode: 191\n",
      "Average value: 26.504453227077192 for episode: 192\n",
      "Average value: 25.67923056572333 for episode: 193\n",
      "Average value: 26.145269037437163 for episode: 194\n",
      "Average value: 27.888005585565306 for episode: 195\n",
      "Average value: 26.843605306287042 for episode: 196\n",
      "Average value: 31.10142504097269 for episode: 197\n",
      "Average value: 30.246353788924054 for episode: 198\n",
      "Average value: 33.48403609947785 for episode: 199\n",
      "Average value: 32.359834294503955 for episode: 200\n",
      "Average value: 32.44184257977876 for episode: 201\n",
      "Average value: 33.76975045078982 for episode: 202\n",
      "Average value: 32.38126292825033 for episode: 203\n",
      "Average value: 32.66219978183781 for episode: 204\n",
      "Average value: 33.32908979274592 for episode: 205\n",
      "Average value: 33.512635303108624 for episode: 206\n",
      "Average value: 34.337003537953194 for episode: 207\n",
      "Average value: 36.07015336105554 for episode: 208\n",
      "Average value: 35.816645693002755 for episode: 209\n",
      "Average value: 37.67581340835262 for episode: 210\n",
      "Average value: 37.942022737934984 for episode: 211\n",
      "Average value: 38.34492160103823 for episode: 212\n",
      "Average value: 46.227675520986324 for episode: 213\n",
      "Average value: 45.816291744937004 for episode: 214\n",
      "Average value: 47.125477157690156 for episode: 215\n",
      "Average value: 48.019203299805646 for episode: 216\n",
      "Average value: 48.168243134815356 for episode: 217\n",
      "Average value: 49.15983097807458 for episode: 218\n",
      "Average value: 54.951839429170846 for episode: 219\n",
      "Average value: 54.754247457712296 for episode: 220\n",
      "Average value: 55.46653508482668 for episode: 221\n",
      "Average value: 57.143208330585345 for episode: 222\n",
      "Average value: 57.53604791405608 for episode: 223\n",
      "Average value: 59.95924551835327 for episode: 224\n",
      "Average value: 61.81128324243561 for episode: 225\n",
      "Average value: 61.870719080313826 for episode: 226\n",
      "Average value: 73.42718312629813 for episode: 227\n",
      "Average value: 75.15582396998323 for episode: 228\n",
      "Average value: 83.54803277148407 for episode: 229\n",
      "Average value: 87.37063113290986 for episode: 230\n",
      "Average value: 86.85209957626437 for episode: 231\n",
      "Average value: 93.15949459745114 for episode: 232\n",
      "Average value: 93.85151986757857 for episode: 233\n",
      "Average value: 92.95894387419963 for episode: 234\n",
      "Average value: 94.01099668048965 for episode: 235\n",
      "Average value: 139.31044684646517 for episode: 236\n",
      "Average value: 182.34492450414191 for episode: 237\n",
      "Average value: 223.2276782789348 for episode: 238\n",
      "Average value: 217.56629436498807 for episode: 239\n",
      "Average value: 208.63797964673864 for episode: 240\n",
      "Average value: 200.5060806644017 for episode: 241\n",
      "Average value: 203.03077663118162 for episode: 242\n",
      "Average value: 242.87923779962253 for episode: 243\n",
      "Average value: 280.73527590964136 for episode: 244\n",
      "Average value: 316.69851211415926 for episode: 245\n",
      "Average value: 346.71358650845133 for episode: 246\n",
      "Average value: 379.37790718302875 for episode: 247\n",
      "Average value: 410.4090118238773 for episode: 248\n",
      "Average value: 400.7885612326834 for episode: 249\n",
      "Average value: 387.34913317104923 for episode: 250\n",
      "Average value: 375.83167651249676 for episode: 251\n",
      "Average value: 365.4400926868719 for episode: 252\n",
      "Average value: 356.5180880525283 for episode: 253\n",
      "Average value: 388.6921836499019 for episode: 254\n",
      "Average value: 387.40757446740673 for episode: 255\n",
      "Average value: 378.43719574403633 for episode: 256\n",
      "Average value: 362.2653359568345 for episode: 257\n",
      "Average value: 363.2020691589928 for episode: 258\n",
      "Average value: 359.84196570104314 for episode: 259\n",
      "Average value: 344.99986741599093 for episode: 260\n",
      "Average value: 337.69987404519134 for episode: 261\n",
      "Average value: 336.81488034293176 for episode: 262\n",
      "Average value: 334.1241363257851 for episode: 263\n",
      "Average value: 335.8179295094958 for episode: 264\n",
      "Average value: 322.577033034021 for episode: 265\n",
      "Average value: 317.79818138231997 for episode: 266\n",
      "Average value: 309.008272313204 for episode: 267\n",
      "Average value: 302.80785869754374 for episode: 268\n",
      "Average value: 291.2674657626666 for episode: 269\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3bb5a8a469b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mddpg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddpg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0ms1_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms1_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-31e35e8c8dad>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_prev\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m \u001b[0;34m+\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#env = NormalizeAction(env) # remap action values for the environment\n",
    "avg_val = 0\n",
    "\n",
    "\n",
    "env1 = NormalizeAction(env1)\n",
    "env2 = NormalizeAction(env2)\n",
    "env3 = NormalizeAction(env3)\n",
    "\n",
    "#for plotting\n",
    "running_rewards_ddpg = []\n",
    "step_list_ddpg = []\n",
    "step_count = 0\n",
    "total_rewards=[]\n",
    "# set term_condition for early stopping according to environment being used\n",
    "#term_condition = -150 # Pendulum\n",
    "term_condition = 700 # inverted Pendulum\n",
    "#term_condition = 1500 # half cheetah\n",
    "\n",
    "param_noise = AdaptiveParamNoiseSpec(initial_stddev=0.05,desired_action_stddev=0.3, adaptation_coefficient=1.05)\n",
    "\n",
    "for itr in range(NUM_EPISODES):\n",
    "    s1 = env1.reset() # get initial state\n",
    "    s2 = env2.reset()\n",
    "    s3 = env3.reset()\n",
    "    total_reward = [0, 0, 0]\n",
    "    step_counter = [0, 0, 0]\n",
    "    \n",
    "    noise_counter=0\n",
    "    ddpg1.perturb_actor_parameters(param_noise)\n",
    "    ddpg2.perturb_actor_parameters(param_noise)\n",
    "    ddpg3.perturb_actor_parameters(param_noise)\n",
    "    \n",
    "    done1 = 0\n",
    "    done2 = 0\n",
    "    done3 = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if not done1:\n",
    "            ddpg1.noise.reset()\n",
    "            a1 = ddpg1.action(s1, ddpg1.noise.step(), param_noise)\n",
    "            s1_1, r1, done1, _ = env1.step(a1)\n",
    "            memory.add((s1,a1,r1,s1_1,done1))\n",
    "            total_reward[0] += r1\n",
    "            step_counter[0] += 1\n",
    "            training_data1 = np.array(memory.sample(BATCH_SIZE))\n",
    "            ddpg1.train(training_data1)\n",
    "            noise_counter += 1\n",
    "            s1 = s1_1\n",
    "            \n",
    "        if not done2:\n",
    "            ddpg2.noise.reset()\n",
    "            a2 = ddpg2.action(s2, ddpg2.noise.step(), param_noise)\n",
    "            \n",
    "            s1_2, r2, done2, _ = env2.step(a2)\n",
    "            memory.add((s2,a2,r2,s1_2,done2))\n",
    "            total_reward[1] += r2\n",
    "            step_counter[1] += 1\n",
    "            training_data2 = np.array(memory.sample(BATCH_SIZE))\n",
    "            ddpg2.train(training_data2)\n",
    "            noise_counter += 1\n",
    "            s2 = s1_2\n",
    "            \n",
    "        if not done3:\n",
    "            ddpg3.noise.reset()\n",
    "            a3 = ddpg3.action(s3, ddpg3.noise.step(), param_noise)\n",
    "            \n",
    "            s1_3, r3, done3, _ = env3.step(a3)\n",
    "            memory.add((s3,a3,r3,s1_3,done3))\n",
    "            total_reward[2] += r3\n",
    "            \n",
    "            step_counter[2] += 1\n",
    "            training_data3 = np.array(memory.sample(BATCH_SIZE))\n",
    "            ddpg3.train(training_data3)\n",
    "            noise_counter += 1\n",
    "            s3 = s1_3\n",
    "            \n",
    "        if done1 and done2 and done3:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    ddpgs = [ddpg1, ddpg2, ddpg3]\n",
    "    total_rewards.append(total_reward)\n",
    "    idx = np.array(total_reward).argmax()\n",
    "    myddpg = ddpgs[idx]\n",
    "\n",
    "    ddpg1 = copy.deepcopy(myddpg)\n",
    "    ddpg2 = copy.deepcopy(myddpg)\n",
    "    ddpg3 = copy.deepcopy(myddpg)\n",
    "\n",
    "    \n",
    "\n",
    "    if memory.position-noise_counter > 0:\n",
    "        noise_data=memory.data[memory.position-noise_counter:memory.position]\n",
    "    else:\n",
    "        noise_data=memory.data[memory.position-noise_counter+60000:60000] \\\n",
    "        + memory.data[0:memory.position]\n",
    "        \n",
    "    \n",
    "    noise_data=np.array(noise_data)\n",
    "    noise_s, noise_a, _,_ , _= zip(*noise_data)\n",
    "    \n",
    "    perturbed_actions = noise_a\n",
    "    unperturbed_actions = myddpg.action(np.array(noise_s), None, None)\n",
    "    ddpg_dist = ddpg_distance_metric(perturbed_actions, unperturbed_actions)\n",
    "    param_noise.adapt(ddpg_dist)\n",
    "    \n",
    "    \n",
    "    if itr > 3 and avg_val > term_condition:\n",
    "        break\n",
    "    running_rewards_ddpg.append(total_reward[idx]) # return of this episode\n",
    "    step_count = step_count + step_counter[idx]\n",
    "    step_list_ddpg.append(step_count)\n",
    "\n",
    "    avg_val = avg_val * 0.95 + 0.05*running_rewards_ddpg[-1]\n",
    "    print(\"Average value: {} for episode: {}\".format(avg_val,itr))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('out.npy',out)\n",
    "np.save('step_list_ddpg.npy',step_list_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(myddpg.actor, 'actor.pt')\n",
    "torch.save(myddpg.actor_perturbed, 'actor_perturbed.pt')\n",
    "np.save('param_noise.npy',[param_noise.desired_action_stddev,\n",
    "                           param_noise.adaptation_coefficient,\n",
    "                           param_noise.current_stddev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl = np.load('param_noise.npy')\n",
    "myddpg.actor = torch.load('actor.pt')\n",
    "myddpg.actor_perturbed = torch.load('actor_perturbed.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'InvertedPendulum-v1'\n",
    "env4 = gym.make(env_name)\n",
    "\n",
    "# check whether the environment is discrete or continuous\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env4.observation_space.shape[0]\n",
    "act_dim = env4.action_space.n if discrete else env.action_space.shape[0]\n",
    "if discrete: print(\"This is a discrete action space, probably not the right algorithm to use\")\n",
    "\n",
    "\n",
    "logdir='./DDPG_final/'\n",
    "logging_interval = 20\n",
    "animate_interval = logging_interval * 5\n",
    "VISUALIZE = True\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env4 = gym.wrappers.Monitor(env4, logdir, force=True, video_callable=lambda \\\n",
    "                               episode_id: episode_id%logging_interval==0)\n",
    "env4._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "env4 = NormalizeAction(env4) # remap action values for the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    SEED = 0   \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    s1 = env4.reset() # get initial state\n",
    "    # env1.render()\n",
    "    # time.sleep(0.05)\n",
    "    count=0\n",
    "    param_noise = AdaptiveParamNoiseSpec(pnl[2],pnl[0],pnl[1])\n",
    "    while True: # for each episode, we loop each step in this episode\n",
    "        myddpg.noise.reset()\n",
    "        env4.render()\n",
    "        time.sleep(0.05)\n",
    "        # use actor to get action, add ddpg.noise.step() to action\n",
    "        # remember to put NN in eval mode while testing (to deal with BatchNorm layers) and put it back \n",
    "        # to train mode after you're done getting the action\n",
    "        a1 = myddpg.action(s1, myddpg.noise.step(), param_noise)\n",
    "        s1_1, r1, done1, _ = env4.step(a1)\n",
    "        s1 = s1_1\n",
    "        if done1: break\n",
    "        count+=1\n",
    "print('Finish Visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out=np.load('out.npy')\n",
    "step_list_ddpg=np.load('step_list_ddpg.npy')\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
